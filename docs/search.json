[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "Introduction to Linear Algebra\nThese are the companion notes to the Winter 2025 Linear Algebra class. The course is designed to bridge theoretical understanding with practical applications, recognizing two fundamental aspects of linear algebra:\n\nThe geometric/visual interpretation of linear algebra concepts, helping students move beyond purely algorithmic approaches to understand systems as relationships between vectors and transformations that can be visualized.\nThe integration of computational tools to handle routine calculations, allowing focus on interpretation and real-world applications rather than manual computation.\n\nLinear algebra serves as the mathematical foundation for numerous real-world applications that shape our modern world. You’ll encounter it in:\n\nSciences & Engineering: Including quantum computing, computational biology, signal processing, robotics and autonomous systems\nComputing, Data & Statistics: Including machine learning, computer graphics, statistical analysis, and network modeling\nBusiness & Economics: Including market modeling, optimization, and equilibrium analysis\n\nYou’ll learn to view linear algebra through three lenses: theoretical foundations, geometric intuition, and practical applications - preparing you to apply these concepts across mathematics, sciences, engineering, and beyond.",
    "crumbs": [
      "Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "parts/vectors.html",
    "href": "parts/vectors.html",
    "title": "Vectors",
    "section": "",
    "text": "This section covers vectors.\n\nVectors in \\(\\mathbb{R}^n\\)\nAbstract Vector Spaces\nProblems: Vector Operations and Dot Products",
    "crumbs": [
      "Vectors"
    ]
  },
  {
    "objectID": "chapters/vectors.html",
    "href": "chapters/vectors.html",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "1.1 Addition of vectors and scalar product of vectors\nThe following videos from the Essence of Linear Algebra, from 3Blue1Brown, are exceptionally good. Watch them carefully\nLet \\(\\mathbf{v}=(v_1,\\dots,v_n)\\) and \\(\\mathbf{w}=(w_1,\\dots,w_n)\\) be two vectors in \\(\\mathbb{R}^n\\) and let \\(c\\in\\mathbb{R}\\) be a scalar. The sum of \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) is defined by: \\[\\mathbf{v}+\\mathbf{w}=\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots \\\\ v_n\\end{bmatrix}+\\begin{bmatrix}w_1\\\\w_2\\\\\\vdots\\\\ w_n\\end{bmatrix}=\\begin{bmatrix}v_1+w_1\\\\v_2+w_2\\\\\\vdots\\\\ v_n+w_n\\end{bmatrix}.\\]\nThe scalar product of \\(c\\) and \\(\\mathbf{v}\\) is defined by: \\[c\\mathbf{v}=c\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots\\\\ v_n\\end{bmatrix}=\\begin{bmatrix}cv_1\\\\cv_2\\\\\\vdots\\\\ cv_n\\end{bmatrix}.\\]\nIn \\(\\mathbb{R}^n\\), we represent vectors as column vectors. For convenience in inline text, we sometimes write \\(\\mathbf{v}=(v_1,\\dots,v_n)\\), but this notation should be understood to represent a column vector. This is distinct from a row vector, which we explicitly denote as \\(\\mathbf{v}=\\begin{bmatrix}v_1& v_2 &\\cdots &v_n\\end{bmatrix}\\). The distinction is important because row and column vectors behave differently under matrix operations.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vectors.html#addition-of-vectors-and-scalar-product-of-vectors",
    "href": "chapters/vectors.html#addition-of-vectors-and-scalar-product-of-vectors",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "Properties of addition and scalar product\nLet \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{w}\\) be vectors in \\(\\mathbb{R}^n\\), and let \\(c\\) and \\(d\\) be scalars.\n\n\\(\\mathbf{u} + \\mathbf{v} \\in \\mathbb{R}^n\\) (Closed under addition)\n\\(c\\mathbf{u} \\in \\mathbb{R}^n\\) (Closed under scalar multiplication)\n\\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\) (Commutative property of addition)\n\\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\) (Associative property of addition)\n\\(\\exists \\, \\mathbf{0} \\in \\mathbb{R}^n\\) such that \\(\\mathbf{u} + \\mathbf{0} = \\mathbf{u}\\) (Existence of an additive identity)\n\\(\\forall \\, \\mathbf{u} \\in \\mathbb{R}^n, \\, \\exists \\, -\\mathbf{u}\\) such that \\(\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}\\) (Existence of additive inverses)\n\\(1\\mathbf{u} = \\mathbf{u}\\) (Identity element of scalar multiplication)\n\\((cd)\\mathbf{u} = c(d\\mathbf{u})\\) (Associative property of scalar multiplication)\n\\(c(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}\\) (Distributive property)\n\\((c + d)\\mathbf{u} = c\\mathbf{u} + d\\mathbf{u}\\) (Distributive property)\n\nThese properties characterize sets equipped with addition and scalar multiplication that satisfy the axioms of a vector space. In particular, they define the structure not only of \\(\\mathbb{R}^n\\) but also of more abstract vector spaces, where elements need not be geometric vectors, and scalars may belong to fields other than \\(\\mathbb{R}\\), such as \\(\\mathbb{C}\\) or finite fields.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vectors.html#linear-combinations",
    "href": "chapters/vectors.html#linear-combinations",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "1.2 Linear Combinations",
    "text": "1.2 Linear Combinations\nA linear combination of vectors \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\) in \\(\\mathbb{R}^n\\) is a sum of scalar multiples of these vectors:\n\\[a_1\\mathbf{v}_1 + a_2\\mathbf{v}_2 + \\cdots + a_k\\mathbf{v}_k\\]\nwhere \\(a_1, a_2, \\ldots, a_k\\) are scalars (real numbers).\nLet’s illustrate this with two vectors in \\(\\mathbb{R}^2\\): \\(\\mathbf{v}_1=\\begin{bmatrix}1\\\\1\\end{bmatrix}\\) and \\(\\mathbf{v}_2=\\begin{bmatrix}-1\\\\1\\end{bmatrix}\\). We can create different vectors through linear combinations of \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\):\n\nCombining with positive coefficients: \\[2\\mathbf{v}_1 + \\mathbf{v}_2 = 2\\begin{bmatrix}1\\\\1\\end{bmatrix} + \\begin{bmatrix}-1\\\\1\\end{bmatrix} = \\begin{bmatrix}1\\\\3\\end{bmatrix}\\]\nUsing a negative coefficient: \\[\\mathbf{v}_1 - 3\\mathbf{v}_2 = \\begin{bmatrix}1\\\\1\\end{bmatrix} - 3\\begin{bmatrix}-1\\\\1\\end{bmatrix} = \\begin{bmatrix}4\\\\-2\\end{bmatrix}\\]\nWorking with fractions: \\[\\frac{1}{2}\\mathbf{v}_1 + \\frac{1}{2}\\mathbf{v}_2 = \\frac{1}{2}\\begin{bmatrix}1\\\\1\\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix}-1\\\\1\\end{bmatrix} = \\begin{bmatrix}0\\\\1\\end{bmatrix}\\]\n\nMany questions in linear algebra reduce to solving systems of linear equations. Questions about linear combinations are a prime example, as we’ll see in the following exercise:\n\nExercise: Can we write \\(\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}\\) as a linear combination of the vectors \\(\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}\\), \\(\\begin{bmatrix}4\\\\5\\\\6\\end{bmatrix}\\), \\(\\begin{bmatrix}7\\\\8\\\\9\\end{bmatrix}\\)?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nWe want scalars \\(c_1\\), \\(c_2\\), \\(c_3\\) such that: \\[c_1\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix} + c_2\\begin{bmatrix}4\\\\5\\\\6\\end{bmatrix} + c_3\\begin{bmatrix}7\\\\8\\\\9\\end{bmatrix} = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}\\] This gives the system of equations: \\[\\begin{align*}\nc_1 + 4c_2 + 7c_3 &= 0\\\\\n2c_1 + 5c_2 + 8c_3 &= 1\\\\\n3c_1 + 6c_2 + 9c_3 &= 0.\n\\end{align*}\\] We can use substitution, or elimination, to show that this system has no solution, so \\(\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}\\) is not a linear combination of the given vectors. We’ll cover solutions to such systems extensively later in the course.\n\n\n\n\nLinear combinations are fundamental in linear algebra and have numerous applications, such as:\n\nExpressing a vector in terms of other vectors\nSolving systems of linear equations\nDescribing lines, planes, and hyperplanes in \\(\\mathbb{R}^n\\)\nAnalyzing linear transformations and matrices",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vectors.html#sec-span",
    "href": "chapters/vectors.html#sec-span",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "1.3 Span",
    "text": "1.3 Span\nThe set of all possible linear combinations of a given set of vectors is known as the span of those vectors, and it has important properties.\nLet’s start with a precise definition. If \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\) are vectors in \\(\\mathbb{R}^n\\), then \\[\\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right) =\\{ c_1\\mathbf{v}_1+\\cdots+c_k\\mathbf{v}_k:c_1,\\dots,c_k\\in\\mathbb{R} \\}\\]\nNotice that \\(\\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\) is a subset of \\(\\mathbb{R}^n\\). When working with sets, we typically focus on two key questions:\n\nHow do we verify if an element belongs to the set?\nWhat properties can we deduce when we know an element belongs to the set?\n\nFor the span of vectors \\(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\):\nVerification: To check if \\(\\mathbf{v}\\) is in the span, we solve a system of equations. Consider:\nIf \\(\\mathbf{v}_1 = \\begin{bmatrix}1\\\\2\\\\1\\end{bmatrix}\\), \\(\\mathbf{v}_2 = \\begin{bmatrix}0\\\\1\\\\1\\end{bmatrix}\\), and \\(\\mathbf{v} = \\begin{bmatrix}2\\\\5\\\\3\\end{bmatrix}\\)\nTo check if \\(\\mathbf{v}\\) is in span\\((\\{\\mathbf{v}_1,\\mathbf{v}_2\\})\\), we ask: do there exist \\(c_1,c_2\\) such that \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 = \\mathbf{v}\\)?\nThis gives us: \\[\\begin{align*}\nc_1(1) + c_2(0) &= 2\\\\\nc_1(2) + c_2(1) &= 5\\\\\nc_1(1) + c_2(1) &= 3\n\\end{align*}\\]\nIf we find values for \\(c_1,c_2\\) satisfying all equations, then \\(\\mathbf{v}\\) is in the span. If no such values exist, \\(\\mathbf{v}\\) is not in the span.\nProperties: If a vector \\(\\mathbf{w}\\) is in \\(\\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\), we know that there exist constants \\(c_1,\\dots,c_n\\in\\mathbb{R}\\) such that \\(\\mathbf{w}=c_1\\mathbf{v}_1+\\cdots+c_k\\mathbf{v}_k\\).\nWe use these properties to deduce important results:\n\nTheorem 1.1 Suppose that \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\) are vectors in \\(\\mathbb{R}^n\\). Then\n\nIf \\(\\mathbf{w}_1\\) and \\(\\mathbf{w}_2\\) are in \\(\\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\), then \\(\\mathbf{w}_1+\\mathbf{w}_2\\) is also in \\(\\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\).\nIf \\(\\mathbf{w}\\in \\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\), and \\(c\\in\\mathbb{R}\\), then \\(c\\mathbf{w}\\in \\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\).\n\n\n\nProof. We only check (1). The proof of (2) is similar. Since \\(\\mathbf{w}_1\\) is in the span, there exist \\(a_1,\\dots,a_k\\) with \\(\\mathbf{w}_1 = a_1\\mathbf{v}_1+\\cdots+a_k\\mathbf{v}_k\\). Similarly, there exist \\(b_1,\\dots,b_k\\) with \\(\\mathbf{w}_2 = b_1\\mathbf{v}_1+\\cdots+b_k\\mathbf{v}_k\\). Then: \\[\\mathbf{w}_1 + \\mathbf{w}_2 = (a_1+b_1)\\mathbf{v}_1+\\cdots+(a_k+b_k)\\mathbf{v}_k\\] showing \\(\\mathbf{w}_1 + \\mathbf{w}_2\\) is in the span. \\(\\square\\)\n\nVisualizing Vector Spans in \\(\\mathbb{R}^3\\)\n\nSpan of a Single Vector Given \\(\\mathbf{v} \\in \\mathbb{R}^3\\), the \\(\\text{span}({\\mathbf{v}})\\) is:\n\n\nA point at origin if \\(\\mathbf{v} = \\mathbf{0}\\)\nA line through the origin if \\(\\mathbf{v} \\neq \\mathbf{0}\\), containing all scalar multiples of \\(\\mathbf{v}\\)\n\n\nSpan of Two Vectors For nonzero vectors \\(\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^3\\), the \\(\\text{span}({\\mathbf{v}, \\mathbf{w}})\\) is:\n\n\nA line through origin if the vectors are parallel (one is a scalar multiple of the other)\nA plane through origin otherwise, containing all linear combinations \\(s\\mathbf{v} + t\\mathbf{w}\\) where \\(s,t \\in \\mathbb{R}\\)\n\n\nSpan of Multiple Vectors Consider the set of vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3, \\mathbf{v}_4\\}\\): \\[\n\\mathbf{v}_1 = \\begin{bmatrix}1\\\\1\\\\3\\end{bmatrix}, \\;\n\\mathbf{v}_2 = \\begin{bmatrix}-2\\\\-2\\\\-6\\end{bmatrix}, \\;\n\\mathbf{v}_3 = \\begin{bmatrix}1\\\\-2\\\\5\\end{bmatrix}, \\;\n\\mathbf{v}_4 = \\begin{bmatrix}0\\\\3\\\\-2\\end{bmatrix}\n\\]\n\nThe span of these vectors is the set of all possible linear combinations: \\[\\text{span}(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3, \\mathbf{v}_4\\}) = \\{t_1\\mathbf{v}_1 + t_2\\mathbf{v}_2 + t_3\\mathbf{v}_3 + t_4\\mathbf{v}_4 : t_1,t_2,t_3,t_4 \\in \\mathbb{R}\\}.\\]\nTheir span is visualized by the following graph and we see that all the vectors are in one plane.\n\n\n                                                \n\n\nThe span of a set of vectors in \\(\\mathbb{R}^3\\) must be one of exactly four geometric objects:\n\nA single point (specifically, the origin \\((0,0,0)\\))\nA line passing through the origin\nA plane containing the origin\nAll of \\(\\mathbb{R}^3\\) (the entire three-dimensional space)",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vectors.html#dot-product-in-mathbbrn",
    "href": "chapters/vectors.html#dot-product-in-mathbbrn",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "1.4 Dot Product in \\(\\mathbb{R}^n\\)",
    "text": "1.4 Dot Product in \\(\\mathbb{R}^n\\)\nLet \\(\\mathbf{v}=(v_1,\\dots,v_n)\\) and \\(\\mathbf{w}=(w_1,\\dots,w_n)\\) be two vectors in \\(\\mathbb{R}^n\\). The dot product of \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) is defined by: \\[\\mathbf{v}\\cdot\\mathbf{w}=\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots\\\\ v_n\\end{bmatrix}\\cdot \\begin{bmatrix}w_1\\\\w_2\\\\\\vdots\\\\ w_n\\end{bmatrix}= \\sum_{i=1}^nv_iw_i.\\]\n\n1.4.1 Properties of the Dot Product in \\(\\mathbb{R}^n\\)\nThe dot product has three fundamental properties that can be verified directly from its definition. These properties form the foundation for many calculations and proofs in linear algebra.\nLet \\(\\mathbf{u}\\), \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) be vectors in \\(\\mathbb{R}^n\\), and let \\(a\\in\\mathbb{R}\\) and \\(b\\in\\mathbb{R}\\) be scalars.\n\n\\(\\mathbf{v} \\cdot \\mathbf{v} \\geq 0\\) and \\(\\mathbf{v} \\cdot \\mathbf{v} = 0\\) if and only if \\(\\mathbf{v} = \\mathbf{0}\\) (Positive Definite)\n\\(\\mathbf{v} \\cdot \\mathbf{w} = \\mathbf{w} \\cdot \\mathbf{v}\\) (Symmetric)\n\\(\\mathbf{u}\\cdot(a\\mathbf{v} + b\\mathbf{w}) = a(\\mathbf{u} \\cdot \\mathbf{v}) + b(\\mathbf{u} \\cdot \\mathbf{w})\\) and \\((a\\mathbf{u} + b\\mathbf{v}) \\cdot \\mathbf{w} = a(\\mathbf{u} \\cdot \\mathbf{w}) + b(\\mathbf{v} \\cdot \\mathbf{w})\\) (Linear in each Variable)\n\nExercise: Suppose that \\(\\mathbf{v}_1,\\mathbf{v}_2,\\mathbf{w}_1,\\mathbf{w}_2\\in\\mathbb{R}^n\\) and that we know the values of \\(\\mathbf{v}_i\\cdot\\mathbf{w}_j\\). Find \\[(2\\mathbf{v}_1+3\\mathbf{v}_2)\\cdot(\\mathbf{w}_1-2\\mathbf{w}_2)\\] in terms of the \\(\\mathbf{v}_i\\cdot\\mathbf{w}_j\\)’s.\n\n\n\n\n\n\nClick to see the answer\n\n\n\n\n\n\nFirst, recall the key linearity properties:\n\nLinear in first variable: \\((c_1\\mathbf{a}_1 + c_2\\mathbf{a}_2)\\cdot\\mathbf{b} = c_1(\\mathbf{a}_1\\cdot\\mathbf{b}) + c_2(\\mathbf{a}_2\\cdot\\mathbf{b})\\)\nLinear in second variable: \\(\\mathbf{a}\\cdot(c_1\\mathbf{b}_1 + c_2\\mathbf{b}_2) = c_1(\\mathbf{a}\\cdot\\mathbf{b}_1) + c_2(\\mathbf{a}\\cdot\\mathbf{b}_2)\\)\n\nLet’s start with the first variable using linearity:\n\n\\((2\\mathbf{v}_1+3\\mathbf{v}_2)\\cdot(\\mathbf{w}_1-2\\mathbf{w}_2)\\)\n\\(= 2\\mathbf{v}_1\\cdot(\\mathbf{w}_1-2\\mathbf{w}_2) + 3\\mathbf{v}_2\\cdot(\\mathbf{w}_1-2\\mathbf{w}_2)\\)\n\nNow apply linearity in the second variable for each term:\n\n\\(= 2(\\mathbf{v}_1\\cdot\\mathbf{w}_1 - 2\\mathbf{v}_1\\cdot\\mathbf{w}_2) + 3(\\mathbf{v}_2\\cdot\\mathbf{w}_1 - 2\\mathbf{v}_2\\cdot\\mathbf{w}_2)\\)\n\nExpand this:\n\n\\(= 2\\mathbf{v}_1\\cdot\\mathbf{w}_1 - 4\\mathbf{v}_1\\cdot\\mathbf{w}_2 + 3\\mathbf{v}_2\\cdot\\mathbf{w}_1 - 6\\mathbf{v}_2\\cdot\\mathbf{w}_2\\)\n\n\nNow we can use the given values of \\(\\mathbf{v}_i\\cdot\\mathbf{w}_j\\) to compute the final result by substituting those values into this expression.\nThe final formula in terms of the known dot products is: \\[2(\\mathbf{v}_1\\cdot\\mathbf{w}_1) - 4(\\mathbf{v}_1\\cdot\\mathbf{w}_2) + 3(\\mathbf{v}_2\\cdot\\mathbf{w}_1) - 6(\\mathbf{v}_2\\cdot\\mathbf{w}_2).\\]\n\n\n\n\n\n1.4.2 Norm\nThe dot product induces a norm on \\(\\mathbb{R}^n\\). The norm of a vector \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\in\\mathbb{R}^n\\) is given by:\n\\[\\|\\mathbf{v}\\| = \\sqrt{\\mathbf{v}\\cdot\\mathbf{v}} = \\sqrt{\\sum_{i=1}^n |v_i|^2}\\]\nThe norm is also known as the magnitude or length of a vector. When we compute the norm or when we check properties, we often look at \\(\\|\\mathbf{v}\\|^2=\\mathbf{v}\\cdot\\mathbf{v}\\) to avoid the square root.\nThe norm satisfies the following properties:\n\nNon-negativity: \\(\\|\\mathbf{v}\\| \\geq 0\\) for all \\(\\mathbf{v} \\in \\mathbb{R}^n\\)\nDefiniteness: \\(\\|\\mathbf{v}\\| = 0\\) if and only if \\(\\mathbf{v} = \\mathbf{0}\\)\nHomogeneity: \\(\\|c\\mathbf{v}\\| = |c| \\|\\mathbf{v}\\|\\) for all \\(c \\in \\mathbb{R}\\) and \\(\\mathbf{v} \\in \\mathbb{R}^n\\)\nTriangle inequality: \\(\\|\\mathbf{v} + \\mathbf{w}\\| \\leq \\|\\mathbf{v}\\| + \\|\\mathbf{w}\\|\\) for all \\(\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^n\\)\n\nThe first three properties are easy to verify. The Triangle Inequality can be proved using Cauchy-Schwarz inequality that says that \\(|\\mathbf{v} \\cdot \\mathbf{w}| \\leq \\|\\mathbf{v}\\| \\|\\mathbf{w}\\|\\)\n\nExercise: Use Cauchy-Schwarz Inequality to prove the Triangle Inequality of the norm.\n\n\n\n\n\n\nClick to see the proof\n\n\n\n\n\nWe start with the squared norm of \\(\\mathbf{v} + \\mathbf{w}\\):\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 = (\\mathbf{v} + \\mathbf{w}) \\cdot (\\mathbf{v} + \\mathbf{w})\\]\nExpanding the right-hand side using the properties of the dot product, we get:\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 = \\mathbf{v} \\cdot \\mathbf{v} + 2(\\mathbf{v} \\cdot \\mathbf{w}) + \\mathbf{w} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\|^2 + 2(\\mathbf{v} \\cdot \\mathbf{w}) + \\|\\mathbf{w}\\|^2\\]\nNow, we apply the Cauchy-Schwarz Inequality to the term \\(2(\\mathbf{v} \\cdot \\mathbf{w})\\):\n\\[2(\\mathbf{v}\\cdot\\mathbf{w})\\leq2|\\mathbf{v} \\cdot \\mathbf{w}| \\leq 2\\|\\mathbf{v}\\| \\|\\mathbf{w}\\|\\]\nSubstituting this into the previous equation, we obtain:\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 \\leq \\|\\mathbf{v}\\|^2 + 2\\|\\mathbf{v}\\| \\|\\mathbf{w}\\| + \\|\\mathbf{w}\\|^2 = (\\|\\mathbf{v}\\| + \\|\\mathbf{w}\\|)^2\\]\nTaking the square root of both sides (which is valid since both sides are non-negative) yields:\n\\[\\|\\mathbf{v} + \\mathbf{w}\\| \\leq \\|\\mathbf{v}\\| + \\|\\mathbf{w}\\|\\]\nwhich is the Triangle Inequality for the norm in \\(\\mathbb{R}^n\\). \\(\\square\\)\n\n\n\n\n\n\n1.4.3 Orthogonality and Cauchy-Schwarz Inequality\nTwo vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) in \\(\\mathbb{R}^n\\) are orthogonal if \\(\\mathbf{v}\\cdot\\mathbf{w}=0\\). Orthogonality is a central topic in linear algebra and has numerous applications in various fields, such as:\n\nCoordinate systems and basis vectors\nLeast squares approximation and regression analysis\nFourier series and signal processing\nQuantum mechanics and Hilbert spaces\n\n\nTheorem 1.2 (Pythagorean Theorem) If \\(\\mathbf{v}, \\mathbf{w}\\in \\mathbb{R}^n\\) are orthogonal vectors, then \\(\\|\\mathbf{v} + \\mathbf{w}\\|^2= \\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2\\)\n\n\nProof. Let \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\) and \\(\\mathbf{w} = (w_1, w_2, \\ldots, w_n)\\) be orthogonal vectors in \\(\\mathbb{R}^n\\). We start with the squared norm of \\(\\mathbf{v} + \\mathbf{w}\\):\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 = (\\mathbf{v} + \\mathbf{w}) \\cdot (\\mathbf{v} + \\mathbf{w})\\]\nExpanding the right-hand side using the properties of the dot product, we get:\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 = \\mathbf{v} \\cdot \\mathbf{v} + 2(\\mathbf{v} \\cdot \\mathbf{w}) + \\mathbf{w} \\cdot \\mathbf{w}\\]\nSince \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) are orthogonal, \\(\\mathbf{v} \\cdot \\mathbf{w} = 0\\). Substituting this into the equation above, we obtain:\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 = \\mathbf{v} \\cdot \\mathbf{v} + \\mathbf{w} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2\\]\nwhich is the Pythagorean Theorem for orthogonal vectors in \\(\\mathbb{R}^n\\). \\(\\square\\)\n\nA similar result is the Parallelogram Law, that says that for any two vectors \\(\\mathbf{v}, \\mathbf{w}\\in \\mathbb{R}^n\\):\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 + \\|\\mathbf{v} - \\mathbf{w}\\|^2 = 2(\\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2) \\tag{1.1}\\]\nWhen \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) are orthogonal, \\(\\|\\mathbf{v} + \\mathbf{w}\\|^2 = \\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2\\) and \\(\\|\\mathbf{v} - \\mathbf{w}\\|^2 = \\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2\\). Then the Parallelogram Law reduces to the Pythagorean Theorem.\n\nExercise: Prove the Parallelogram Law\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nExpand both squared norms using the dot product definition: \\(|\\mathbf{u}|^2 = \\mathbf{u} \\cdot \\mathbf{u}\\)\nFor the left side, you’ll get terms with \\(\\mathbf{v} \\cdot \\mathbf{v}\\), \\(\\mathbf{w} \\cdot \\mathbf{w}\\), and \\(\\mathbf{v} \\cdot \\mathbf{w}\\)\nPay attention to the signs of the cross terms \\(\\mathbf{v} \\cdot \\mathbf{w}\\) in both expansions\n\n\n\n\n\n\nTheorem 1.3 (Theorem: Cauchy-Schwarz Inequality) Let \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\) and \\(\\mathbf{w} = (w_1, w_2, \\ldots, w_n)\\) be two vectors in \\(\\mathbb{R}^n\\). Then \\(|\\mathbf{v} \\cdot \\mathbf{w}| \\leq \\|\\mathbf{v}\\| \\|\\mathbf{w}\\|\\)\n\n\nProof. Let \\(t \\in \\mathbb{R}\\) be a scalar. Consider the non-negative quantity \\(\\|\\mathbf{v} - t\\mathbf{w}\\|^2\\):\n\\[\\|\\mathbf{v} - t\\mathbf{w}\\|^2 \\geq 0\\]\nExpanding the left-hand side using the properties of the dot product, we get:\n\\[(\\mathbf{v} - t\\mathbf{w}) \\cdot (\\mathbf{v} - t\\mathbf{w}) = \\mathbf{v} \\cdot \\mathbf{v} - 2t(\\mathbf{v} \\cdot \\mathbf{w}) + t^2(\\mathbf{w} \\cdot \\mathbf{w}) \\geq 0\\]\nThis inequality holds for all values of \\(t\\). Let’s choose \\(t\\) to be the value that minimizes the left-hand side:\n\\[t = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{\\|\\mathbf{w}\\|^2}\\]\nSubstituting this value of \\(t\\) into the inequality, we obtain:\n\\[\\|\\mathbf{v}\\|^2 - 2\\frac{(\\mathbf{v} \\cdot \\mathbf{w})^2}{\\|\\mathbf{w}\\|^2} + \\frac{(\\mathbf{v} \\cdot \\mathbf{w})^2}{\\|\\mathbf{w}\\|^2} \\geq 0\\]\nSimplifying the left-hand side:\n\\[\\|\\mathbf{v}\\|^2 - \\frac{(\\mathbf{v} \\cdot \\mathbf{w})^2}{\\|\\mathbf{w}\\|^2} \\geq 0\\]\nMultiplying both sides by \\(\\|\\mathbf{w}\\|^2\\) yields:\n\\[\\|\\mathbf{v}\\|^2\\|\\mathbf{w}\\|^2 \\geq (\\mathbf{v} \\cdot \\mathbf{w})^2\\]\nTaking the square root of both sides (which is valid since both sides are non-negative) gives:\n\\[\\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\geq |\\mathbf{v} \\cdot \\mathbf{w}|\\]\nwhich is the Cauchy-Schwarz Inequality. \\(\\square\\)\n\n\n\n\n\n\n\nClick to see an alternative proof of Cauchy-Schawrz\n\n\n\n\n\nSuppose that neither \\(\\mathbf{v}\\) nor \\(\\mathbf{w}\\) are zero and that one is not a multiple of the other.\nFor any scalar \\(t\\in\\mathbb{R}\\), we can write \\(\\mathbf{w}\\) as the sum of two vectors: \\(\\mathbf{w} = t\\mathbf{v}+(\\mathbf{w}-t\\mathbf{v})\\). Our goal is to find \\(t\\in\\mathbb{R}\\) such that \\(t\\mathbf{v}\\) and \\(\\mathbf{w}-t\\mathbf{v}\\) are orthogonal. For such a \\(t\\), \\[\\|\\mathbf{w}\\|^2 = t^2\\|\\mathbf{v}\\|^2+\\|\\mathbf{w}-t\\mathbf{v}\\|^2.\\] In particular, \\[t^2\\|\\mathbf{v}\\|^2\\leq\\|\\mathbf{w}\\|^2.\\] From the equation \\(\\mathbf{v}\\cdot (\\mathbf{w}-t\\mathbf{v})=0\\), we find that \\(t=\\frac{\\mathbf{v}\\cdot \\mathbf{w}}{\\mathbf{v}\\cdot \\mathbf{v}}\\). When we substitute this into \\[t^2\\|\\mathbf{v}\\|^2\\leq\\|\\mathbf{w}\\|^2,\\] and simplify we get the Cauchy-Schwarz inequality.\n\n\n\n\n\n1.4.4 Geometric Interpretation of the Dot Product\nThe dot product of two vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) in \\(\\mathbb{R}^n\\) can be expressed in terms of their norms and the angle between them: \\[\\mathbf{v} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\cos(\\theta)\\] where \\(\\theta\\) is the angle between \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\). This relationship highlights the geometric interpretation of the dot product. When \\(\\theta = 0°\\), the vectors are parallel, and the dot product equals the product of their norms. When \\(\\theta = 90°\\), the vectors are orthogonal, and the dot product is zero. The Cauchy-Schwarz Inequality follows directly from this relationship, as \\(|\\cos(\\theta)| \\leq 1\\).\nWe can verify this easily in \\(\\mathbb{R}^2\\). Consider two vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) in \\(\\mathbb{R}^2\\). Let the angle that vector \\(\\mathbf{v}\\) makes with the positive x-axis be \\(\\alpha\\), and the angle that vector \\(\\mathbf{w}\\) makes with the positive x-axis be \\(\\alpha + \\beta\\), where \\(\\beta\\) is the angle between vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\).\nThe vectors can be expressed in terms of their magnitudes and angles: \\(\\mathbf{v} = (\\|\\mathbf{v}\\| \\cos(\\alpha), \\|\\mathbf{v}\\| \\sin(\\alpha))\\) and \\(\\mathbf{w} = (\\|\\mathbf{w}\\| \\cos(\\alpha + \\beta), \\|\\mathbf{w}\\| \\sin(\\alpha + \\beta))\\). The dot product of these vectors is:\n\\[\\mathbf{v} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| (\\cos(\\alpha) \\cos(\\alpha + \\beta) + \\sin(\\alpha) \\sin(\\alpha + \\beta))\\]\nUsing the angle addition formulas:\n\\[\\cos(\\alpha + \\beta) = \\cos(\\alpha) \\cos(\\beta) - \\sin(\\alpha) \\sin(\\beta)\\] \\[\\sin(\\alpha + \\beta) = \\sin(\\alpha) \\cos(\\beta) + \\cos(\\alpha) \\sin(\\beta)\\]\nWe show that \\(\\cos(\\alpha) \\cos(\\alpha + \\beta) + \\sin(\\alpha) \\sin(\\alpha + \\beta) = \\cos(\\beta)\\) and we conclude that \\[\\mathbf{v} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\cos(\\beta).\\]\n\nExercise: Prove that \\(\\cos(\\alpha) \\cos(\\alpha + \\beta) + \\sin(\\alpha) \\sin(\\alpha + \\beta) = \\cos(\\beta)\\)\n\n\n\n\n\n\nClick to see the proof\n\n\n\n\n\n\\[\\begin{aligned}\n\\cos(\\alpha) \\cos(\\alpha + \\beta) + \\sin(\\alpha) \\sin(\\alpha + \\beta) &=\n\\cos(\\alpha) (\\cos(\\alpha) \\cos(\\beta) - \\sin(\\alpha) \\sin(\\beta)) \\\\\n&\\quad + \\sin(\\alpha) (\\sin(\\alpha) \\cos(\\beta) + \\cos(\\alpha) \\sin(\\beta))) \\\\\n&= (\\cos^2(\\alpha) \\cos(\\beta) - \\cos(\\alpha) \\sin(\\alpha) \\sin(\\beta) \\\\\n&\\quad + \\sin^2(\\alpha) \\cos(\\beta) + \\cos(\\alpha) \\sin(\\alpha) \\sin(\\beta)) \\\\\n&= (\\cos^2(\\alpha) \\cos(\\beta) + \\sin^2(\\alpha) \\cos(\\beta)) \\\\\n&= (\\cos^2(\\alpha) + \\sin^2(\\alpha)) \\cos(\\beta) \\\\\n&= \\cos(\\beta)\n\\end{aligned}\\]\n\n\n\n\n\n\n1.4.5 Distance\nThe norm induces a distance (or metric) on \\(\\mathbb{R}^n\\), the distance between two vectors \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\) and \\(\\mathbf{w} = (w_1, w_2, \\ldots, w_n)\\) is given by:\n\\[d(\\mathbf{v}, \\mathbf{w}) =\\|\\mathbf{v}-\\mathbf{w}\\| = \\sqrt{\\sum_{i=1}^n |v_i - w_i|^2}\\]\nThis distance is known as the Euclidean distance. It satisfies the following properties:\n\nNon-negativity: \\(d(\\mathbf{v}, \\mathbf{w}) \\geq 0\\) for all \\(\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^n\\)\nDefiniteness: \\(d(\\mathbf{v}, \\mathbf{w}) = 0\\) if and only if \\(\\mathbf{v} = \\mathbf{w}\\)\nSymmetry: \\(d(\\mathbf{v}, \\mathbf{w}) = d(\\mathbf{w}, \\mathbf{v})\\) for all \\(\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^n\\)\nTriangle inequality: \\(d(\\mathbf{v}, \\mathbf{z}) \\leq d(\\mathbf{v}, \\mathbf{w}) + d(\\mathbf{w}, \\mathbf{z})\\) for all \\(\\mathbf{v}, \\mathbf{w}, \\mathbf{z} \\in \\mathbb{R}^n\\)\n\nThe induced distance has numerous applications in various fields, such as:\n\nClustering and classification in machine learning\nMeasuring similarity or dissimilarity between objects or data points\nOptimization problems in operations research\nError analysis and approximation theory in numerical analysis\n\nUnderstanding the relationships between dot product, norm, and distance is crucial in applications in mathematics, physics, computer science, and engineering.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vectors.html#vector-computation-in-python",
    "href": "chapters/vectors.html#vector-computation-in-python",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "1.5 Vector Computation in Python",
    "text": "1.5 Vector Computation in Python\nPython provides various ways to work with vectors and mathematical computations. Here’s an overview of the main approaches:\n\nBuilt-in Python Lists:\n\nBasic vector operations require explicit formulas or list comprehensions\nUseful for understanding the underlying computations\nNot optimized for large-scale numerical calculations\n\nNumPy (Numerical Python):\n\nIndustry-standard library for numerical computing\nProvides efficient array operations and mathematical functions\nOptimized for performance with vectorized operations\nEssential for scientific computing and data analysis\n\nSymPy (Symbolic Python):\n\nComputer algebra system for symbolic mathematics\nHandles mathematical expressions with variables and symbols\nPerfect for mathematical proofs and algebraic manipulations\nUseful for verifying theoretical results\n\n\nIn this class, we will primarily use NumPy and Sympy. For solving linear systems, NumPy uses numerically stable methods like QR decomposition rather than Gaussian elimination. While Gaussian elimination is a foundational algorithm taught in linear algebra courses for its theoretical importance and intuitive approach, it can be numerically unstable in practice. SymPy provides a direct implementation of Gaussian elimination, making it useful for understanding the algorithm and verifying theoretical results.\n\n1.5.1 Representing Vectors\nIn Python, we can represent vectors using lists, in NumPy we use arrays, and in Sympy we use vectors, that are implemented with the function Matrix. Notice that Sympy shows vectors as column vectors.\n\nimport numpy as np\nfrom sympy import Matrix\n\n# Using Python lists\nv = [1, 2, 3]\nw = [4, 5, 6]\nprint(v)  # Output: [51,2,3]\n\n# Using NumPy arrays\nv_np = np.array([1, 2, 3])\nw_np = np.array([4, 5, 6])\nprint(v_np)  # Output: [1 2 3]\n\n# Using Sympy vectors\nv_sp = Matrix([1,2,3])\nw_sp = Matrix([4,5,6])\n\n[1, 2, 3]\n[1 2 3]\n\n\n\n# Showing v_sp\nv_sp\n\n\\(\\displaystyle \\left[\\begin{matrix}1\\\\2\\\\3\\end{matrix}\\right]\\)\n\n\n\n\n1.5.2 Vector Addition\nThe + is already implemented in Numpy and Sympy.\n\n# Using Python lists\nresult = [v[i] + w[i] for i in range(len(v))]\nprint(result)  # Output: [5, 7, 9]\n\n# Using NumPy arrays\nresult_np = v_np + w_np\nprint(result_np)  # Output: [5 7 9]\n\n[5, 7, 9]\n[5 7 9]\n\n\n\n# Using Sympy vectors\nv_sp + w_sp\n\n\\(\\displaystyle \\left[\\begin{matrix}5\\\\7\\\\9\\end{matrix}\\right]\\)\n\n\n\n\n1.5.3 Scalar Multiplication\nThe scalar multiplication is already implemented in Numpy and Sympy.\n\nscalar = 2\n\n# Using Python lists\nresult = [scalar * x for x in v]\nprint(result)  # Output: [2, 4, 6]\n\n# Using NumPy arrays\nresult_np = scalar * v_np\nprint(result_np)  # Output: [2 4 6]\n\n[2, 4, 6]\n[2 4 6]\n\n\n\n# Using Sympy vectors\nscalar * v_sp\n\n\\(\\displaystyle \\left[\\begin{matrix}2\\\\4\\\\6\\end{matrix}\\right]\\)\n\n\n\n\n1.5.4 Dot Product\nThe dot product is already implemented in Numpy and Sympy.\n\n# Using Python lists\ndot_product = sum([v[i] * w[i] for i in range(len(v))])\nprint(dot_product)  # Output: 32\n\n# Using NumPy arrays\ndot_product_np = np.dot(v_np, w_np)\nprint(dot_product_np)  # Output: 32\n\n# Using Sympy vectors\ndot_product_sp = v_sp.dot(w_sp)\nprint(dot_product_sp)  # Output: 32\n\n32\n32\n32\n\n\n\n\n1.5.5 Vector Norms\nThe norm is already implemented in Numpy and Sympy, but it is inside the linalg library of Numpy.\n\nimport math\n\n# Using Python lists\nnorm = math.sqrt(sum([x**2 for x in v]))\nprint(norm)  # Output: 3.7416573867739413\n\n# Using NumPy arrays\nnorm_np = np.linalg.norm(v_np)\nprint(norm_np)  # Output: 3.7416573867739413\n\n# Using Symoy vectors\nnorm_sp = v_sp.norm()\nprint(norm_sp)  # Output: sqrt(14) which is 3.7416573867739413\n\n3.7416573867739413\n3.7416573867739413\nsqrt(14)",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vector_spaces.html",
    "href": "chapters/vector_spaces.html",
    "title": "2  Abstract Vector Spaces",
    "section": "",
    "text": "2.1 Definition\nThe step from \\(\\mathbb{R}^n\\) to abstract vector spaces reflects a fundamental principle in mathematics: identifying common patterns to unify seemingly different objects. While \\(\\mathbb{R}^n\\) provides a concrete and visualizable model, the abstract framework reveals that spaces of functions, polynomials, and solutions to some differential equations share the exact same algebraic structure. This abstraction is not just elegant—it’s immensely practical. When we prove a theorem about vector spaces in general, it automatically applies to all these examples at once.\nA vector space \\(V\\) over \\(\\mathbb{R}\\) is a set equipped with two operations: vector addition (\\(+\\)) and scalar multiplication (\\(\\cdot\\)). For \\(\\mathbf{u},\\mathbf{v},\\mathbf{w}\\in V\\) and scalars \\(a,b\\in\\mathbb{R}\\), these operations must satisfy:\nVector Addition Properties:\nScalar Multiplication Properties:\nExamples: Vector spaces appear naturally throughout mathematics and its applications. While we’ll encounter many examples throughout this course, here are a few fundamental ones to illustrate how diverse they can be.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abstract Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/vector_spaces.html#definition",
    "href": "chapters/vector_spaces.html#definition",
    "title": "2  Abstract Vector Spaces",
    "section": "",
    "text": "Commutativity: \\(\\mathbf{u}+\\mathbf{v}=\\mathbf{v}+\\mathbf{u}\\)\nAssociativity: \\((\\mathbf{u}+\\mathbf{v})+\\mathbf{w}=\\mathbf{u}+(\\mathbf{v}+\\mathbf{w})\\)\nZero vector: There exists \\(\\mathbf{0}\\in V\\) such that \\(\\mathbf{v}+\\mathbf{0}=\\mathbf{v}\\) for all \\(\\mathbf{v}\\in V\\)\nAdditive inverse: For each \\(\\mathbf{v}\\in V\\), there exists \\(-\\mathbf{v}\\in V\\) such that \\(\\mathbf{v}+(-\\mathbf{v})=\\mathbf{0}\\)\n\n\n\nDistributivity over vector addition: \\(a(\\mathbf{u}+\\mathbf{v})=a\\mathbf{u}+a\\mathbf{v}\\)\nDistributivity over scalar addition: \\((a+b)\\mathbf{v}=a\\mathbf{v}+b\\mathbf{v}\\)\nAssociativity: \\(a(b\\mathbf{v})=(ab)\\mathbf{v}\\)\nIdentity: \\(1\\mathbf{v}=\\mathbf{v}\\)\n\n\n\n\\(\\mathbb{R}^n\\): Our familiar space of n-tuples of real numbers with the standard addition and scalar multiplication.\nFunction spaces:\n\n\\(C[a,b]\\): Continuous functions on \\([a,b]\\)\n\nSum: \\((f + g)(x) = f(x) + g(x)\\)\nScalar multiplication: \\((cf)(x) = c\\cdot f(x)\\)\n\n\\(C^\\infty(\\mathbb{R})\\): Infinitely differentiable functions\n\nSum: \\((f + g)(x) = f(x) + g(x)\\)\nScalar multiplication: \\((cf)(x) = c\\cdot f(x)\\)\n\n\nPolynomial spaces:\n\n\\(\\mathbb{P}_n\\): Polynomials of degree \\(\\leq n\\)\n\nSum: \\((p + q)(x) = p(x) + q(x)\\)\nScalar multiplication: \\((cp)(x) = c\\cdot p(x)\\)\n\n\\(\\mathbb{P}\\): All polynomials\n\nSum: \\((p + q)(x) = p(x) + q(x)\\)\nScalar multiplication: \\((cp)(x) = c\\cdot p(x)\\)",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abstract Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/vector_spaces.html#subspaces-of-vector-spaces",
    "href": "chapters/vector_spaces.html#subspaces-of-vector-spaces",
    "title": "2  Abstract Vector Spaces",
    "section": "2.2 Subspaces of Vector Spaces",
    "text": "2.2 Subspaces of Vector Spaces\nWithin any vector space, certain subsets naturally inherit the vector space structure. These special subsets, called subspaces, play a fundamental role in linear algebra.\n\nDefinition: A subset \\(W\\) of the vector space \\(V\\) is called a subspace if it satisfies three conditions:\n\nThe zero vector \\(\\mathbf{0}\\) is in \\(W\\)\nFor all \\(\\mathbf{u},\\mathbf{v}\\in W\\), their sum \\(\\mathbf{u}+\\mathbf{v}\\) is also in \\(W\\) (closed under addition)\nFor all \\(\\mathbf{v}\\in W\\) and all scalars \\(c\\in\\mathbb{R}\\), the vector \\(c\\mathbf{v}\\) is in \\(W\\) (closed under scalar multiplication)\n\n\nThese conditions ensure that \\(W\\) inherits the vector space structure from \\(V\\), making it a vector space in its own right and providing us with a rich source of new examples.\n\nTheorem 2.1 Theorem: Every subspace of a vector space \\(V\\) is itself a vector space.\n\n\nProof. Let \\(W\\) be a subspace of the vector space \\(V\\). We must verify all eight vector space properties.\nVector Addition Properties:\n\nCommutativity: Let \\(\\mathbf{u},\\mathbf{v}\\in W\\). Since \\(W\\subseteq V\\), we know \\(\\mathbf{u}+\\mathbf{v}=\\mathbf{v}+\\mathbf{u}\\), as this holds in \\(V\\).\nAssociativity: Let \\(\\mathbf{u},\\mathbf{v},\\mathbf{w}\\in W\\). Since \\(W\\subseteq V\\), we know that \\((\\mathbf{u}+\\mathbf{v})+\\mathbf{w}=\\mathbf{u}+(\\mathbf{v}+\\mathbf{w})\\), as this holds in \\(V\\).\nZero vector: This is given directly by subspace property 1.\nAdditive inverse: Let \\(\\mathbf{v}\\in W\\). By subspace property 3, \\((-1)\\mathbf{v}\\in W\\). This is the additive inverse of \\(\\mathbf{v}\\) since \\(\\mathbf{v}+(-1)\\mathbf{v}=1\\mathbf{v}+(-1)\\mathbf{v}=(1-1)\\mathbf{v}=0\\mathbf{v}=\\mathbf{0}\\).\n\nScalar Multiplication Properties:\n\nDistributivity over vector addition: Let \\(a\\in\\mathbb{R}\\) and \\(\\mathbf{u},\\mathbf{v}\\in W\\). We know that \\(a(\\mathbf{u}+\\mathbf{v})=a\\mathbf{u}+a\\mathbf{v}\\), as this holds in \\(V\\).\nDistributivity over scalar addition: Let \\(a,b\\in\\mathbb{R}\\) and \\(\\mathbf{v}\\in W\\). We know that \\((a+b)\\mathbf{v}=a\\mathbf{v}+b\\mathbf{v}\\), as this holds in \\(V\\).\nAssociativity of scalar multiplication: Let \\(a,b\\in\\mathbb{R}\\) and \\(\\mathbf{v}\\in W\\). We know that \\(a(b\\mathbf{v})=(ab)\\mathbf{v}\\), as this holds in \\(V\\).\nIdentity scalar multiplication: Let \\(\\mathbf{v}\\in W\\). We know that \\(1\\mathbf{v}=\\mathbf{v}\\), as this holds in \\(V\\), and clearly \\(\\mathbf{v}\\in W\\) by assumption.\n\nTherefore, since all eight vector space properties are satisfied, \\(W\\) is indeed a vector space.\n\nNote that this proof relies heavily on two key facts:\n\nThe vector space operations in \\(W\\) are inherited from \\(V\\)\nThe subspace properties ensure that these operations are well-defined on \\(W\\) (i.e., their outputs remain in \\(W\\))\n\n\n\n\n\n\n\nSubspaces of \\(\\mathbb{R}^n\\)\n\n\n\nSubspaces of \\(\\mathbb{R}^n\\) are fundamental structures that arise naturally in many applications of linear algebra. They have intuitive geometric interpretations that help us visualize abstract concepts:\n\nIn \\(\\mathbb{R}^2\\):\n\nA line through the origin\nThe entire plane \\(\\mathbb{R}^2\\) itself\nThe zero vector \\({\\vec{0}}\\)\n\nIn \\(\\mathbb{R}^3\\):\n\nA line through the origin\nA plane through the origin\nThe entire space \\(\\mathbb{R}^3\\)\nThe zero vector \\({\\mathbf{0}}\\)\n\n\nIn Theorem 1.1, we proved that the span of a set of vectors \\({\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_k}\\) in \\(\\mathbb{R}^n\\) is a subspace.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abstract Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/vector_spaces.html#inner-products",
    "href": "chapters/vector_spaces.html#inner-products",
    "title": "2  Abstract Vector Spaces",
    "section": "2.3 Inner Products:",
    "text": "2.3 Inner Products:\nAn inner product on a vector space \\(V\\) is a function \\(\\langle\\cdot,\\cdot\\rangle:V\\times V\\to\\mathbb{R}\\) satisfying:\n\nSymmetry: \\(\\langle\\mathbf{u},\\mathbf{v}\\rangle=\\langle\\mathbf{v},\\mathbf{u}\\rangle\\)\nLinearity: \\(\\langle a\\mathbf{u}+b\\mathbf{v},\\mathbf{w}\\rangle=a\\langle\\mathbf{u},\\mathbf{w}\\rangle+b\\langle\\mathbf{v},\\mathbf{w}\\rangle\\)\nPositive definiteness: \\(\\langle\\mathbf{v},\\mathbf{v}\\rangle\\geq 0\\) with equality if and only if \\(\\mathbf{v}=\\mathbf{0}\\)\n\nThe inner product generalizes the familiar dot product of \\(\\mathbb{R}^n\\). Different fields use different notations:\n\nIn \\(\\mathbb{R}^n\\): \\(\\mathbf{u}\\cdot\\mathbf{v}\\) (dot product notation)\nIn mathematics: \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle\\) (angle bracket notation)\nIn physics: \\(\\langle \\mathbf{u} | \\mathbf{v} \\rangle\\) (Dirac or bra-ket notation)\n\nExamples of Inner Products:\n\nStandard dot product in \\(\\mathbb{R}^n\\): \\(\\langle\\mathbf{x},\\mathbf{y}\\rangle=\\sum_{i=1}^n x_iy_i\\)\nOn \\(C[a,b]\\): \\(\\langle f,g\\rangle=\\int_a^b f(x)g(x)\\,dx\\)\nOn \\(\\mathbb{P}_n\\): \\(\\langle p,q\\rangle=\\int_{-1}^1 p(x)q(x)\\,dx\\)\n\nJust as in \\(\\mathbb{R}^n\\), these inner products satisfy fundamental properties that make them powerful tools. Every inner product generates a norm through \\(\\|\\mathbf{v}\\|=\\sqrt{\\langle\\mathbf{v},\\mathbf{v}\\rangle}\\), which in turn induces a distance function \\(d(\\mathbf{u},\\mathbf{v})=\\|\\mathbf{u}-\\mathbf{v}\\|\\). The Cauchy-Schwarz inequality holds in any inner product space: \\(|\\langle\\mathbf{u},\\mathbf{v}\\rangle|\\leq\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|\\). And the norm satisfies all the properties we know from \\(\\mathbb{R}^n\\): positivity, homogeneity, and the triangle inequality. Thus, every inner product space inherits the geometric structure that makes \\(\\mathbb{R}^n\\) so useful.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abstract Vector Spaces</span>"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html",
    "href": "worksheets/computing_dot_products.html",
    "title": "Problems",
    "section": "",
    "text": "Vector Operations and Dot Products",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-1",
    "href": "worksheets/computing_dot_products.html#problem-1",
    "title": "Problems",
    "section": "Problem 1",
    "text": "Problem 1\nConsider the following vectors:\n\n\n\n\n\n\n\n\n\n\nFind the coordinates of \\(\\mathbf{u}\\), \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\)\nFind \\(\\mathbf{u} + \\mathbf{v} + \\mathbf{w}\\) (does the answer make geometric sense?)\nFind \\(\\mathbf{u} - \\mathbf{v} - \\mathbf{w}\\) (does the answer make geometric sense?)\nNormalize the vectors \\(\\mathbf{u}\\), \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\). That is, find a vector in the same direction with norm equal to one.\nCheck that \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) satisfy Cauchy-Schwartz inequality.",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-2",
    "href": "worksheets/computing_dot_products.html#problem-2",
    "title": "Problems",
    "section": "Problem 2",
    "text": "Problem 2\nLet \\(\\mathbf{v}\\), \\(\\mathbf{w}\\), \\(\\mathbf{z} \\in \\mathbb{R}^n\\). Suppose that \\(\\mathbf{v} \\cdot \\mathbf{w} = 2\\), \\(\\mathbf{v} \\cdot \\mathbf{z} = -1\\), \\(\\mathbf{z} \\cdot \\mathbf{w} = 1\\), \\(\\|\\mathbf{v}\\| = \\sqrt{3}\\), \\(\\|\\mathbf{w}\\| = 2\\), and \\(\\|\\mathbf{z}\\| = \\sqrt{5}\\). Find the following:\n\n\\((2\\mathbf{v} + 3\\mathbf{w}) \\cdot \\mathbf{z}\\)\n\\(\\mathbf{v} \\cdot (\\mathbf{v} - 2\\mathbf{w} + 3\\mathbf{z})\\)\n\\((3\\mathbf{v} - 4\\mathbf{z}) \\cdot (\\mathbf{w} + 5\\mathbf{z})\\)\n\\(\\|\\mathbf{v} + \\mathbf{w}\\|\\)\n\\(\\|\\mathbf{v} - \\mathbf{w}\\|\\)\n\\(\\|2\\mathbf{v} - 6\\mathbf{z}\\|\\)\nFind the angle between \\(\\mathbf{v}\\) and \\(\\mathbf{z}\\)\nFind \\(c \\in \\mathbb{R}\\) such that \\(\\mathbf{v}\\) is orthogonal to \\(\\mathbf{w} + c\\mathbf{z}\\)\nFind \\(c \\in \\mathbb{R}\\) such that \\(\\mathbf{w} - c\\mathbf{v}\\) is orthogonal to \\(\\mathbf{v}\\)",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-3",
    "href": "worksheets/computing_dot_products.html#problem-3",
    "title": "Problems",
    "section": "Problem 3",
    "text": "Problem 3\nLet \\(\\mathbf{v}\\), \\(\\mathbf{w} \\in \\mathbb{R}^n\\). Suppose that \\(\\|\\mathbf{v}\\| = 2\\), \\(\\|\\mathbf{v} + \\mathbf{w}\\| = \\sqrt{3}\\) and \\(\\|\\mathbf{w}\\| = \\sqrt{2}\\). Find \\(\\mathbf{v} \\cdot \\mathbf{w}\\).",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-4",
    "href": "worksheets/computing_dot_products.html#problem-4",
    "title": "Problems",
    "section": "Problem 4",
    "text": "Problem 4\nLet \\(\\mathbf{v}\\), \\(\\mathbf{w} \\in \\mathbb{R}^n\\). Suppose that \\(\\|\\mathbf{v}\\| = \\sqrt{2}\\), \\(\\|\\mathbf{v} - \\mathbf{w}\\| = \\sqrt{3}\\) and \\(\\mathbf{v} \\cdot \\mathbf{w} = \\frac{1}{2}\\). Find \\(\\|\\mathbf{w}\\|\\).",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-5",
    "href": "worksheets/computing_dot_products.html#problem-5",
    "title": "Problems",
    "section": "Problem 5",
    "text": "Problem 5\nSuppose that \\(\\mathbf{u} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\), \\(\\mathbf{v} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\\) and \\(\\mathbf{w} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix}\\)\n\nFind a constant \\(c \\in \\mathbb{R}\\) such that \\(\\mathbf{v} - c\\mathbf{u}\\) is orthogonal to \\(\\mathbf{u}\\). Name this vector \\(\\mathbf{f_2} = \\mathbf{v} - c\\mathbf{u}\\).\nFind constants \\(c, d \\in \\mathbb{R}\\) such that \\(\\mathbf{w} - c\\mathbf{u} - d\\mathbf{f_2}\\) is orthogonal to \\(\\mathbf{u}\\) and orthogonal to \\(\\mathbf{f_2}\\). Name this vector \\(\\mathbf{f_3} = \\mathbf{w} - c\\mathbf{u} - d\\mathbf{f_2}\\).\nFind constants \\(c, d, e \\in \\mathbb{R}\\) such that the vectors \\(c\\mathbf{u}\\), \\(d\\mathbf{f_2}\\), and \\(e\\mathbf{f_3}\\) have norm one. Rename these vectors \\(\\mathbf{e_1} = c\\mathbf{u_1}\\), \\(\\mathbf{e_2} = d\\mathbf{f_2}\\), and \\(\\mathbf{e_3} = e\\mathbf{f_3}\\) and write them explicitly.",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-6",
    "href": "worksheets/computing_dot_products.html#problem-6",
    "title": "Problems",
    "section": "Problem 6",
    "text": "Problem 6\nLet \\(\\mathbf{v_1}\\), \\(\\mathbf{v_2}\\), \\(\\mathbf{v_3}\\), \\(\\mathbf{v_4}\\) be vectors satisfying:\n\\(\\mathbf{v_i} \\cdot \\mathbf{v_j} = \\begin{cases} 1 & \\text{if } i = j \\\\ 0 & \\text{otherwise} \\end{cases}\\)\n\nFind \\((2\\mathbf{v_1} - 3\\mathbf{v_2}) \\cdot (2\\mathbf{v_3} + 4\\mathbf{v_4})\\)\nFind \\((\\mathbf{v_1} + \\mathbf{v_2}) \\cdot (\\mathbf{v_1} - \\mathbf{v_2})\\)\nFind \\(\\|\\mathbf{v_4}\\|\\)\nFind \\(\\|4\\mathbf{v_1} - 3\\mathbf{v_2}\\|\\)\nFind \\(\\|2\\mathbf{v_1} - 3\\mathbf{v_2} + 4\\mathbf{v_3} - 5\\mathbf{v_4}\\|\\)",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "parts/matrices.html",
    "href": "parts/matrices.html",
    "title": "Matrices",
    "section": "",
    "text": "This section covers matrices.\n\nMatrices\nProblems: Matrix Multiplication\nApplication: Image Transformation with Matrices",
    "crumbs": [
      "Matrices"
    ]
  },
  {
    "objectID": "chapters/matrices.html",
    "href": "chapters/matrices.html",
    "title": "3  Matrices",
    "section": "",
    "text": "3.1 Matrices: Definition and Different Perspectives\nThe following video from the Essence of Linear Algebra, from 3Blue1Brown, is exceptionally good. Watch it carefully\nA matrix is a rectangular array of numbers arranged in rows and columns. Formally, a matrix \\(A\\) with \\(m\\) rows and \\(n\\) columns is written as:\n\\[\nA = \\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m,1} & a_{m,2} & \\cdots & a_{m,n}\n\\end{bmatrix}.\n\\] The matrix \\(A\\) is said to have dimension \\(m\\times n\\). The entry \\(a_{i,j}\\) represents the element at the \\(i\\)-th row and \\(j\\)-th column. This entry is also sometimes denoted as \\(A_{i,j}\\) or \\((A)_{i,j}\\).\nFor example, consider the following matrix:\n\\[M=\\begin{bmatrix}8&6&0&6\\\\-4&-8&2&-7\\\\-8&4&-5&3\\end{bmatrix}\\]\nThis matrix \\(M\\) has 3 rows and 4 columns. We can interpret and view this matrix in several ways:",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrices-definition-and-different-perspectives",
    "href": "chapters/matrices.html#matrices-definition-and-different-perspectives",
    "title": "3  Matrices",
    "section": "",
    "text": "As an array of numbers: We can see \\(M\\) as a collection of numbers arranged in a rectangular grid with 3 rows and 4 columns. Each entry in the matrix is identified by its row and column index. For example, \\(M_{2,3}\\) or \\((M)_{2,3}\\), the entry in the second row and third column is 2.\nAs a collection of column vectors: We can view \\(M\\) as having 4 column vectors, each with 3 elements. The columns of \\(M\\) are:\n\\[\\left [ \\begin{bmatrix}8\\\\-4\\\\-8\\end{bmatrix}, \\begin{bmatrix}6\\\\-8\\\\4\\end{bmatrix}, \\begin{bmatrix}0\\\\2\\\\-5\\end{bmatrix}, \\begin{bmatrix}6\\\\-7\\\\3\\end{bmatrix}\\right ]\\]\nEach column vector can be treated as a separate entity, and matrix operations can be performed on these columns.\nAs a collection of row vectors: We can view \\(M\\) as having 3 row vectors, each with 4 elements. The rows of \\(M\\) are:\n\\[\\biggl [ \\begin{bmatrix}8&6&0&6\\end{bmatrix}, \\begin{bmatrix}-4&-8&2&-7\\end{bmatrix}, \\begin{bmatrix}-8&4&-5&3\\end{bmatrix}\\biggr ]\\]\nEach row vector can be treated as a separate entity, and matrix operations can be performed on these rows.",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#addition-and-scalar-multiplication-of-matrices",
    "href": "chapters/matrices.html#addition-and-scalar-multiplication-of-matrices",
    "title": "3  Matrices",
    "section": "3.2 Addition and Scalar Multiplication of Matrices",
    "text": "3.2 Addition and Scalar Multiplication of Matrices\nGiven two matrices \\(A\\) and \\(B\\) of the same size \\(m \\times n\\), the sum of \\(A\\) and \\(B\\), denoted as \\(A + B\\), is a new matrix \\(C\\) of size \\(m \\times n\\) where each element \\(c_{i,j}\\) is the sum of the corresponding elements \\(a_{i,j}\\) and \\(b_{i,j}\\) from matrices \\(A\\) and \\(B\\), respectively. In other words: \\[c_{i,j} = a_{i,j} + b_{i,j}\\] for all \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\). For example, if \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\\), then: \\[A + B = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 1+5 & 2+6 \\\\ 3+7 & 4+8 \\end{bmatrix} = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix}\\] The scalar multiplication of a matrix \\(A\\) by a scalar \\(k\\), denoted as \\(kA\\), is a new matrix \\(B\\) of the same size as \\(A\\), where each element \\(b_{i,j}\\) is the product of the scalar \\(k\\) and the corresponding element \\(a_{i,j}\\) from matrix \\(A\\). In other words: \\[b_{i,j} = k \\cdot a_{i,j}\\] for all \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\). For example, if \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(k = 2\\), then: \\[2A = 2 \\cdot \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 2 \\cdot 1 & 2 \\cdot 2 \\\\ 2 \\cdot 3 & 2 \\cdot 4 \\end{bmatrix} = \\begin{bmatrix} 2 & 4 \\\\ 6 & 8 \\end{bmatrix}\\]\n\n3.2.1 Properties of addition and scalar product\nLet \\(A\\), \\(B\\), and \\(C\\) be \\(m\\times n\\) matrices and let \\(c\\) and \\(d\\) be scalars. Then we can esily check that\n\nCommutativity of addition: \\(A + B = B + A\\)\nAssociativity of addition: \\((A + B) + C = A + (B + C)\\)\nExistence of zero matrix: There exists a matrix \\(O\\) such that \\(A + O = A\\) for all matrices \\(A\\)\nExistence of additive inverse: For every matrix \\(A\\), there exists a matrix \\(-A\\) such that \\(A + (-A) = O\\)\nDistributivity of scalar multiplication over matrix addition: \\(k(A + B) = kA + kB\\)\nDistributivity of scalar multiplication over field addition: \\((k + l)A = kA + lA\\)\nAssociativity of scalar multiplication: \\((kl)A = k(lA)\\)\nExistence of multiplicative identity: \\(1A = A\\) for all matrices \\(A\\)\n\nSince we have already shown that if \\(A\\) and \\(B\\) are \\(m\\times n\\) matrices and \\(k\\) is a scalar, then \\(A+B\\) and \\(kA\\) are also \\(m\\times n\\) matrices, we can conclude that the set of all \\(m\\times n\\) matrices forms a vector space. This set is commonly denoted using various notations, including: \\(M_{m\\times n}\\), \\(\\mathbb{M}_{m\\times n}\\), and \\(\\mathbb{R}^{m\\times n}\\).",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#the-transpose-of-a-matrix",
    "href": "chapters/matrices.html#the-transpose-of-a-matrix",
    "title": "3  Matrices",
    "section": "3.3 The Transpose of a Matrix",
    "text": "3.3 The Transpose of a Matrix\nGiven a matrix \\(A\\) with \\(m\\) rows and \\(n\\) columns, denoted as: \\[\nA = \\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m,1} & a_{m,2} & \\cdots & a_{m,n}\n\\end{bmatrix},\n\\] the transpose of matrix \\(A\\), denoted as \\(A^T\\) or \\(A'\\), is obtained by interchanging the rows and columns of \\(A\\). In other words, the first row of \\(A\\) becomes the first column of \\(A^T\\), the second row of \\(A\\) becomes the second column of \\(A^T\\), and so on. The resulting matrix \\(A^T\\) has \\(n\\) rows and \\(m\\) columns:\n\\[\nA^T = \\begin{bmatrix}\na_{1,1} & a_{2,1} & \\cdots & a_{m,1}\\\\\na_{1,2} & a_{2,2} & \\cdots & a_{m,2}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\na_{1,n} & a_{2,n} & \\cdots & a_{m,n}.\n\\end{bmatrix}\n\\]\nWhen \\(A\\) is represented by columns or by rows, we can easily determine the form of the transpose. \\[\n\\text{If}\\quad A = \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix},\\text{ then }\\quad\nA^T=\\begin{bmatrix}\n\\leftarrow & \\mathbf{c}_1^T &\\rightarrow \\\\\n\\leftarrow & \\mathbf{c}_2^T &\\rightarrow\\\\\n\\vdots & \\vdots &\\vdots\\\\\n\\leftarrow & \\mathbf{c}_n^T &\\rightarrow\n\\end{bmatrix},\n\\] and \\[\n\\text{if}\\quad A =\n\\begin{bmatrix}\n\\leftarrow & \\mathbf{r}_1 &\\rightarrow \\\\\n\\leftarrow & \\mathbf{r}_2 &\\rightarrow\\\\\n\\vdots & \\vdots &\\vdots\\\\\n\\leftarrow & \\mathbf{r}_m &\\rightarrow\n\\end{bmatrix}, \\text{ then }\\quad\nA^T = \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{r}_1^T & \\mathbf{r}_2^T & \\cdots & \\mathbf{r}_n^T \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}.\n\\]\nNotice that if we take the transpose twice returns the original matrix. In other words, \\[(A^T)^T=A.\\] To check properties of the transpose we use the definition \\((A^T)_{i,j}=A_{j,i}\\).\n\nExercise: Suppose that \\(A\\) and \\(B\\) are \\(m\\times n\\) matrices and that \\(c\\in\\mathbb{R}\\). Prove that \\((A+B)^T=A^T+B^T\\) and that \\((cA)^T=cA^T\\).\n\n\n\n\n\n\nClick to see a sketch of the proof\n\n\n\n\n\n\\[((A+B)^T)_{i,j}=(A+B)_{j,i}=A_{j,i}+B_{j,i}=(A^T)_{i,j}+(B^T)_{i,j}=(A^T+B^T)_{i,j}.\\] The other one is similar",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrix-vector-multiplication",
    "href": "chapters/matrices.html#matrix-vector-multiplication",
    "title": "3  Matrices",
    "section": "3.4 Matrix-Vector Multiplication",
    "text": "3.4 Matrix-Vector Multiplication\nWe now cover one of the most important operations in linear algebra: multiplying a matrix with a vector. Let \\(A\\) be a an \\(m\\times n\\) matrix and \\(\\mathbf{x}\\in\\mathbb{R}^n\\). The product \\(A\\mathbf{x}\\) will be a vector in \\(\\mathbb{R}^m\\). This operation is crucial because it allows us to:\n\nTransform vectors in space (like rotations, reflections, or scaling)\nSolve systems of linear equations in a compact way\nApply linear transformations in computer graphics, data science, and physics\n\nWe look at the \\(m\\times n\\) matrix \\(A\\) in three ways: \\[A = \\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m,1} & a_{m,2} & \\cdots & a_{m,n}\n\\end{bmatrix}\n=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\n=\\begin{bmatrix}\n\\leftarrow & \\mathbf{r}_1 &\\rightarrow \\\\\n\\leftarrow & \\mathbf{r}_2 &\\rightarrow \\\\\n\\vdots & \\vdots &\\vdots\\\\\n\\leftarrow & \\mathbf{r}_m &\\rightarrow\n\\end{bmatrix}.\\]\n\n3.4.1 A Linear Combination of Columns\nWe define the product \\(A\\mathbf{x}\\) as a linear combination of the columns of \\(A\\): \\[\nA\\mathbf{x} =\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\n\\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_n \\end{bmatrix}\n= x_1\\mathbf{c}_1+x_2\\mathbf{c_2}+\\cdots+x_n\\mathbf{c}_n.\n\\tag{3.1}\\]\nExample: Let \\(A=\\begin{bmatrix} 1 & 2 & 0 \\\\ -1 & 3 & 4 \\end{bmatrix}\\) be a \\(2\\times 3\\) matrix and \\(\\mathbf{x}=\\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}\\) be a vector in \\(\\mathbb{R}^3\\). Then \\[\n\\begin{aligned}\nA\\mathbf{x} &=\n\\begin{bmatrix} 1 & 2 & 0 \\\\ -1 & 3 & 4 \\end{bmatrix}\n\\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix} \\\\\n&=2\\begin{bmatrix}1\\\\-1 \\end{bmatrix}\n+ (-1)\\begin{bmatrix}2\\\\3 \\end{bmatrix}\n+3\\begin{bmatrix}0\\\\4 \\end{bmatrix}\n=\\begin{bmatrix}0\\\\6 \\end{bmatrix}\n\\end{aligned}\n\\]\nEquation (Equation 3.1) is one of the most useful formulas.\n\nIt allows us to write matrix multiplications as linear combinations,\nIt allows us write linear combinations as matrix multiplication.\n\n\n\n3.4.2 The Component-wise Formula\nFrom the definition given by (Equation 3.1), we can write \\(A\\mathbf{x}\\) in terms of the \\(a_{i,j}\\)’s: \\[\\begin{aligned}\nA\\mathbf{x} &=\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m,1} & a_{m,2} & \\cdots & a_{m,n}\n\\end{bmatrix}\n\\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix} \\\\\n&= x_1\\begin{bmatrix} a_{1,1}\\\\ a_{2,1}\\\\ \\vdots\\\\ a_{m,1} \\end{bmatrix}\n+ x_2\\begin{bmatrix} a_{1,2}\\\\ a_{2,2}\\\\ \\vdots\\\\ a_{m,2} \\end{bmatrix} +\\cdots\n+ x_n\\begin{bmatrix} a_{1,n}\\\\ a_{2,n}\\\\ \\vdots\\\\ a_{m,n} \\end{bmatrix} \\\\\n% &= \\begin{bmatrix} a_{1,1}x_1\\\\ a_{2,1}x_1\\\\ \\vdots\\\\ a_{m,1}x_1 \\end{bmatrix}\n% + \\begin{bmatrix} a_{1,2}x_2\\\\ a_{2,2}x_2\\\\ \\vdots\\\\ a_{m,2}x_2 \\end{bmatrix} +\\cdots\n% + \\begin{bmatrix} a_{1,n}x_n\\\\ a_{2,n}x_n\\\\ \\vdots\\\\ a_{m,n}x_n \\end{bmatrix} \\\\\nA\\mathbf{x} &= \\begin{bmatrix} a_{1,1}x_1+a_{1,2}x_2+\\cdots+a_{1,n}x_n\\\\ a_{2,1}x_1+a_{2,2}x_2+\\cdots+a_{2,n}x_n\\\\ \\vdots\\\\ a_{m,1}x_1+a_{m,2}x_2+\\cdots+a_{m,n}x_n\\\\ \\end{bmatrix}.\n\\end{aligned}\n\\]\nTherefore, the \\(i\\)-th component of \\(A\\mathbf{x}\\) is: \\[(A\\mathbf{x})_i=a_{i,1}x_1+a_{i,2}x_2+\\cdots+a_{i,n}x_n \\tag{3.2}\\]\n\n\n3.4.3 The Row Dot Product Formula\nFrom (Equation 3.2) we see that the \\(i\\)-th term of \\(A\\mathbf{x}\\) can be written as a dot product: \\[\n(A\\mathbf{x})_i=a_{i,1}x_1+a_{i,2}x_2+\\cdots+a_{i,n}x_n\n=\\begin{bmatrix} a_{i,1}\\\\ a_{i,2}\\\\\\vdots \\\\ a_{i,n}\\end{bmatrix}\\cdot\n\\begin{bmatrix} x_1\\\\ x_2\\\\\\vdots \\\\ x_n\\end{bmatrix}\n=\\mathbf{r}_i^T\\cdot\\mathbf{x}.\\] Recall that vectors in \\(\\mathbb{R}^n\\) are represented by column vectors, and that the first vector is the transpose of the \\(i\\)-th row of \\(A\\). Putting all the compunents togther, we get: \\[\nA\\mathbf{x}=\\begin{bmatrix}\n\\leftarrow & \\mathbf{r}_1 &\\rightarrow \\\\\n\\leftarrow & \\mathbf{r}_2 &\\rightarrow \\\\\n\\vdots & \\vdots &\\vdots\\\\\n\\leftarrow & \\mathbf{r}_m &\\rightarrow \\end{bmatrix}\n\\begin{bmatrix} x_1\\\\ x_2\\\\\\vdots \\\\ x_n\\end{bmatrix}\n=\\begin{bmatrix}\n\\mathbf{r}_1^T\\cdot\\mathbf{x} \\\\\n\\mathbf{r}_2^T\\cdot\\mathbf{x}\\\\ \\vdots \\\\\n\\mathbf{r}_n^T\\cdot\\mathbf{x}\\end{bmatrix}.\n\\tag{3.3}\\]",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrix-matrix-multiplication",
    "href": "chapters/matrices.html#matrix-matrix-multiplication",
    "title": "3  Matrices",
    "section": "3.5 Matrix-Matrix Multiplication",
    "text": "3.5 Matrix-Matrix Multiplication\nMatrix-matrix multiplication, like matrix-vector multiplication, requires compatibility between the dimensions of the matrices involved. For the product \\(AB\\) to be defined, the number of columns in \\(A\\) must equal the number of rows in \\(B\\). When this condition is met, the matrices are said to be compatible for multiplication. Specifically, if \\(A\\) is an \\(m \\times n\\) matrix and \\(B\\) is an \\(n \\times p\\) matrix, their product \\(AB\\) will be an \\(m \\times p\\) matrix.\n\n3.5.1 The Product \\(AB\\): \\(A\\) Acts on the Columns of \\(B\\)\nLet \\(A\\) be an \\(m \\times n\\) matrix and \\(B\\) an \\(n \\times p\\) matrix. Since the number of columns in \\(A\\) matches the number of rows in \\(B\\), the matrices are compatible for multiplication. To define the product \\(AB\\), we express \\(B\\) in terms of its column vectors and let \\(A\\) act on each column individually. Specifically,\n\\[\nAB = A \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{b}_1 & \\mathbf{b}_2 & \\cdots & \\mathbf{b}_p \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\nA\\mathbf{b}_1 & A\\mathbf{b}_2 & \\cdots & A\\mathbf{b}_p \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}.\n\\tag{3.4}\\]\nNotice that \\(AB\\) consists of \\(p\\) columns, where each column \\(A\\mathbf{c}_i \\in \\mathbb{R}^m\\). Therefore, \\(AB\\) is an \\(m \\times p\\) matrix.\n\n\n3.5.2 The Component Formula\nWe use the previous formula to compute the individual entries of \\(AB\\). Consider \\((AB)_{i,j}\\). This is the element of \\(AB\\) in the \\(i\\)-th row and \\(j\\)-th column. Since the \\(j\\)-th column of \\(AB\\) is \\(A\\mathbf{b}_j\\), it follows from (Equation 3.2) that \\((AB)_{i,j}=(A\\mathbf{b}_j)_i= \\sum_{k=1}^na_{i,k}b_{k,j}.\\) Then we have \\[(AB)_{i,j}=\\sum_{k=1}^na_{i,k}b_{k,j}. \\tag{3.5}\\]\n\n\n3.5.3 The Row-Column Dot Product Formula\nFrom (Equation 3.5), it follows that \\((AB)_{i,j}\\) is the dot product of the \\(i\\)-th row of \\(A\\) and the \\(j\\)-th column of \\(B\\). Writing \\(A\\) in terms of its rows and \\(B\\) in terms of its columns, we have:\n\\[\nAB =\n\\begin{bmatrix}\n\\leftarrow & \\mathbf{r}_1 & \\rightarrow \\\\\n\\leftarrow & \\mathbf{r}_2 & \\rightarrow \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\leftarrow & \\mathbf{r}_m & \\rightarrow\n\\end{bmatrix}\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{b}_1 & \\mathbf{b}_2 & \\cdots & \\mathbf{b}_p \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\mathbf{r}_1^T \\cdot \\mathbf{b}_1 & \\mathbf{r}_1^T \\cdot \\mathbf{b}_2 & \\cdots & \\mathbf{r}_1^T \\cdot \\mathbf{b}_p \\\\  \n\\mathbf{r}_2^T \\cdot \\mathbf{b}_1 & \\mathbf{r}_2^T \\cdot \\mathbf{b}_2 & \\cdots & \\mathbf{r}_2^T \\cdot \\mathbf{b}_p \\\\  \n\\vdots & \\vdots & \\ddots & \\vdots \\\\  \n\\mathbf{r}_m^T \\cdot \\mathbf{b}_1 & \\mathbf{r}_m^T \\cdot \\mathbf{b}_2 & \\cdots & \\mathbf{r}_m^T \\cdot \\mathbf{b}_p  \n\\end{bmatrix}.\n\\tag{3.6}\\]\nHere, \\(\\mathbf{r}_i\\) represents the \\(i\\)-th row of \\(A\\), and \\(\\mathbf{b}_j\\) represents the \\(j\\)-th column of \\(B\\). Each entry of \\(AB\\), denoted \\((AB)_{i,j}\\), is the dot product \\(\\mathbf{r}_i^T \\cdot \\mathbf{b}_j\\).",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#the-transpose-of-the-product",
    "href": "chapters/matrices.html#the-transpose-of-the-product",
    "title": "3  Matrices",
    "section": "3.6 The Transpose of the Product",
    "text": "3.6 The Transpose of the Product\nAn important property of matrix multiplication is that the transpose of a product is the product of the transposes in reverse order. This relation is fundamental in many areas of linear algebra, from proving theoretical results about linear transformations to solving practical problems in optimization and data analysis.\n\n3.6.0.1 Theorem\nLet \\(A\\) be an \\(m \\times n\\) matrix and \\(B\\) an \\(n \\times p\\) matrix. Then:\n\\[(AB)^T = B^T A^T.\\]\n\n\nProof. The product \\(AB\\) is an \\(m \\times p\\) matrix, so its transpose \\((AB)^T\\) is a \\(p \\times m\\) matrix. Similarly, \\(B^T\\) is a \\(p \\times n\\) matrix, and \\(A^T\\) is an \\(n \\times m\\) matrix. Thus, the product \\(B^T A^T\\) also has dimensions \\(p \\times m\\), matching those of \\((AB)^T\\).\nTo prove the equality, we verify that the entries of \\((AB)^T\\) and \\(B^T A^T\\) are identical. Consider the \\((i, j)\\)-th entry of \\((AB)^T\\):\n\\[( (AB)^T )_{i,j} = (AB)_{j,i}.\\]\nUsing the definition of matrix multiplication, we expand \\((AB)_{j,i}\\):\n\\[(AB)_{j,i} = \\sum_{k=1}^n A_{j,k} B_{k,i}.\\]\nNext, observe that \\((B^T)_{i,k} = B_{k,i}\\) and \\((A^T)_{k,j} = A_{j,k}\\). Substituting these into the sum, we get:\n\\[(AB)_{j,i} = \\sum_{k=1}^n (B^T)_{i,k} (A^T)_{k,j}.\\]\nTherefore,\n\\[((AB)^T)_{i,j} = (B^T A^T)_{i,j}.\\]\nSince the \\((i, j)\\)-th entries of \\((AB)^T\\) and \\(B^T A^T\\) are equal for all \\(i\\) and \\(j\\), we conclude that:\n\\[(AB)^T = B^T A^T.\\]\n\n\n\n\n\n\n\nClick to see a proof that uses the row column dot product formula\n\n\n\n\n\nWrite \\(A\\) and \\(B\\) in terms of their rows and columns \\[A=\\begin{bmatrix}\n\\leftarrow & \\mathbf{r}_1 & \\rightarrow \\\\\n\\leftarrow & \\mathbf{r}_2 & \\rightarrow \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\leftarrow & \\mathbf{r}_m & \\rightarrow\n\\end{bmatrix} \\quad\\quad\nB=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{b}_1 & \\mathbf{b}_2 & \\cdots & \\mathbf{b}_p \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\\] Then \\[B^T = \\begin{bmatrix} \\leftarrow &\\mathbf{b}_1^T&\\rightarrow \\\\ \\leftarrow& \\mathbf{b}_2^T&\\rightarrow \\\\ \\vdots &\\vdots&\\vdots \\\\\\leftarrow & \\mathbf{b}_p^T &\\rightarrow \\end{bmatrix}\\quad\\quad\nA^T =\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{r}_1^T & \\mathbf{r}_2^T & \\cdots & \\mathbf{r}_m^T \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}.\\] Therefore it follows from the row column dot product formula that \\[(B^TA^T)_{i,j} = (\\mathbf{b}_i^T)^T\\cdot\\mathbf{r}_j^T=\\mathbf{b}_i\\cdot\\mathbf{r}_j^T\n=\\mathbf{r}_j^T\\cdot\\mathbf{b}_i=(AB)_{j,i}=((AB)^T)_{i,j}.\\]",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrix-types-and-the-inverse",
    "href": "chapters/matrices.html#matrix-types-and-the-inverse",
    "title": "3  Matrices",
    "section": "3.7 Matrix Types and the Inverse",
    "text": "3.7 Matrix Types and the Inverse\nMatrices come in various types, each with unique properties that make them fundamental to linear algebra and its applications. Among these, the following play a central role.\n\nSquare Matrices\nA square matrix is a matrix that has an equal number of rows and columns. The collection of all \\(n \\times n\\) square matrices is denoted by \\(M_n\\). This set is closed under several operations: if \\(A, B \\in M_n\\), their product \\(AB\\) also belongs to \\(M_n\\). Similarly, any power of \\(A\\), such as \\(A^k\\) for a positive integer \\(k\\), remains in \\(M_n\\), as does the transpose of \\(A\\).\n\n\nDiagonal Matrices\nA diagonal matrix is a square matrix in which all off-diagonal entries are zero. Formally, a matrix \\(D \\in M_n\\) is diagonal if \\((D)_{i,j} = 0\\) for all \\(i \\neq j\\). The only potentially nonzero entries are located along the main diagonal, from the top-left to the bottom-right. Diagonal matrices are significant because they are easy to work with: addition, multiplication, and finding powers are straightforward operations when the matrices are diagonal.\n\n\nUpper and Lower Triangular Matrices\nAn upper triangular matrix is a square matrix in which all entries below the main diagonal are zero, meaning \\((U)_{i,j} = 0\\) for all \\(i &gt; j\\). Similarly, a lower triangular matrix has all entries above the main diagonal equal to zero, i.e., \\((L)_{i,j} = 0\\) for all \\(i &lt; j\\). These matrices are commonly used in matrix factorizations, and solving systems of linear equations efficiently. Both types are particularly important in numerical methods, as their structure reduces computational complexity in many algorithms.\n\n\nSymmetric Matrices\nA square matrix is symmetric if it is equal to its transpose, meaning \\(A^T=A\\) or, equivalently, \\(A_{i,j}=A_{j,i}\\) for all \\(i,j\\). Symmetric matrices play a crucial role in various fields due to their significant orthogonal properties and frequent appearance in science and engineering applications.\n\n\nIdentity Matrices\nAn identity matrix is a special type of diagonal matrix where all the diagonal entries are 1, and all off-diagonal entries are 0. It is denoted as \\(I_n\\) for an \\(n \\times n\\) matrix. Formally, \\((I_n)_{i,j} = 1\\) if \\(i = j\\) and \\((I_n)_{i,j} = 0\\) if \\(i \\neq j\\).\nThe identity matrix serves as the multiplicative identity in matrix multiplication. Specifically, if \\(A\\) is an \\(n \\times p\\) matrix, then \\(I_n A = A\\). Similarly, if \\(B\\) is an \\(m \\times n\\) matrix, then \\(B I_n = B\\).\n\n\nInverse of a Matrix\nThe inverse of a matrix is a concept that applies to square matrices. A square matrix \\(A\\) is said to be invertible (or nonsingular) if there exists another matrix \\(A^{-1}\\) such that:\n\\[A A^{-1} = A^{-1} A = I_n,\\]\nwhere \\(I_n\\) is the identity matrix. The matrix \\(A^{-1}\\) is called the inverse of \\(A\\).\nNot all square matrices have an inverse. In practical applications, matrix inverses are used to solve systems of linear equations, analyze transformations, and compute solutions in various scientific and engineering contexts. However, for large matrices, explicit inversion is computationally expensive, and alternative methods, such as iterative techniques, are often preferred.",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#elementary-matrices-row-operations",
    "href": "chapters/matrices.html#elementary-matrices-row-operations",
    "title": "3  Matrices",
    "section": "3.8 Elementary Matrices: Row Operations",
    "text": "3.8 Elementary Matrices: Row Operations\nElementary matrices are special square matrices that perform row operations through matrix multiplication. They play an important role in linear algebra, particularly in solving systems of linear equations, characterizing invertible matrices, and understanding and computing determinants. There are three types:\n\nType 1: Switching two rows\nType 2: Multiplying a row by a non-zero constant\nType 3: Adding a multiple of a row to another row\n\nLet’s illustrate each type with 3×3 elementary matrices acting on a generic 3×5 matrix:\nLet A be a 3×5 matrix: \\(A = \\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix}\\)\n\n3.8.1 Example 1: Interchanging Rows 1 and 2\n\\(\\begin{aligned}\nE_1A &=\n\\begin{bmatrix}\n0 & 1 & 0\\\\\n1 & 0 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix} \\\\\n&=\\begin{bmatrix}\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix}\n\\end{aligned}\\)\n\n\n3.8.2 Example 2: Multiplying Row 3 by 2\n\\(\\begin{aligned}\nE_2A &=\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 2\n\\end{bmatrix}\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix} \\\\\n&=\\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\n2a_{3,1} & 2a_{3,2} & 2a_{3,3} & 2a_{3,4} & 2a_{3,5}\n\\end{bmatrix}\n\\end{aligned}\\)\n\n\n3.8.3 Example 3: Adding 3 Times Row 1 to Row 2\n\\(\\begin{aligned}\nE_3A &=\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n3 & 1 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix} \\\\\n&=\\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\n3a_{1,1}+a_{2,1} & 3a_{1,2}+a_{2,2} & 3a_{1,3}+a_{2,3} & 3a_{1,4}+a_{2,4} & 3a_{1,5}+a_{2,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix}\n\\end{aligned}\\)\nNote that each elementary matrix is invertible, and its inverse performs the opposite operation:\n\nFor \\(E_1\\): its own inverse (swapping the same rows again)\nFor \\(E_2\\): multiply the third row by 1/2\nFor \\(E_3\\): subtract 3 times row 1 from row 2\n\nWe saw in (Equation 3.1) that when we multiply a matrix \\(A\\) by a vector \\(\\mathbf{x}\\), the product \\(A\\mathbf{x}\\) is a linear combination of the columns of \\(A\\). Similarly, when we multiply by a row vector \\(\\mathbf{z}\\) from the left, the product \\(\\mathbf{z}A\\) is a linear combination of the rows of \\(A\\). This fundamental principle helps us understand elementary matrices: when we multiply a matrix \\(A\\) by an elementary matrix \\(E\\) on the left, each row of the product \\(EA\\) is a linear combination of the rows of \\(A\\), precisely implementing our desired row operation.",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrices-in-numpy",
    "href": "chapters/matrices.html#matrices-in-numpy",
    "title": "3  Matrices",
    "section": "3.9 Matrices in Numpy",
    "text": "3.9 Matrices in Numpy\nThis section covers fundamental matrix operations using NumPy’s ndarray class. We’ll explore creation, indexing, and basic mathematical operations.\n\n3.9.1 Setup\nFirst, let’s import NumPy:\n\nimport numpy as np\n\n\n\n3.9.2 Creating Matrices\nNumPy provides several ways to create matrices using ndarrays:\n\n# From a list of lists\nA = np.array([[1, 2, 3],\n              [4, 5, 6]])\nprint(\"Matrix A:\")\nprint(A)\n\n# Using special functions\nzeros = np.zeros((2, 3))    # 2x3 matrix of zeros\nones = np.ones((3, 3))      # 3x3 matrix of ones\neye = np.eye(3)             # 3x3 identity matrix\n\nprint(\"\\nZeros matrix:\")\nprint(zeros)\nprint(\"\\nIdentity matrix:\")\nprint(eye)\n\nMatrix A:\n[[1 2 3]\n [4 5 6]]\n\nZeros matrix:\n[[0. 0. 0.]\n [0. 0. 0.]]\n\nIdentity matrix:\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\n\n\n3.9.3 Matrix Properties and Shape\nThe shape attribute tells us the dimensions of the matrix:\n\nA = np.array([[1, 2, 3],\n              [4, 5, 6]])\nprint(f\"Shape: {A.shape}\")\nprint(f\"Number of dimensions: {A.ndim}\")\nprint(f\"Size: {A.size}\")\n\nShape: (2, 3)\nNumber of dimensions: 2\nSize: 6\n\n\n\n\n3.9.4 Indexing and Slicing\nNumPy provides powerful ways to access matrix elements:\n\nA = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\n# Individual elements\nprint(\"First element:\", A[0, 0])\nprint(\"Second row, third column:\", A[1, 2])\n\n# Extracting rows\nprint(\"\\nFirst row:\", A[0, :])\nprint(\"Second row:\", A[1])  # : is implicit\n\n# Extracting columns\nprint(\"\\nFirst column:\", A[:, 0])\nprint(\"Second column:\", A[:, 1])\n\n# Slicing\nprint(\"\\nSubmatrix (first two rows, second and third columns):\")\nprint(A[0:2, 1:3])\n\nFirst element: 1\nSecond row, third column: 6\n\nFirst row: [1 2 3]\nSecond row: [4 5 6]\n\nFirst column: [1 4 7]\nSecond column: [2 5 8]\n\nSubmatrix (first two rows, second and third columns):\n[[2 3]\n [5 6]]\n\n\n\n\n3.9.5 Basic Operations\n\n3.9.5.1 Addition and Subtraction\nMatrix addition and subtraction work element-wise:\n\nA = np.array([[1, 2],\n              [3, 4]])\nB = np.array([[5, 6],\n              [7, 8]])\n\nprint(\"Matrix A:\")\nprint(A)\nprint(\"\\nMatrix B:\")\nprint(B)\nprint(\"\\nA + B:\")\nprint(A + B)\nprint(\"\\nA - B:\")\nprint(A - B)\n\nMatrix A:\n[[1 2]\n [3 4]]\n\nMatrix B:\n[[5 6]\n [7 8]]\n\nA + B:\n[[ 6  8]\n [10 12]]\n\nA - B:\n[[-4 -4]\n [-4 -4]]\n\n\n\n\n3.9.5.2 Scalar Operations\nMultiply or divide a matrix by a scalar:\n\nA = np.array([[1, 2],\n              [3, 4]])\n\nprint(\"Original matrix:\")\nprint(A)\nprint(\"\\nMultiply by 2:\")\nprint(2 * A)\nprint(\"\\nDivide by 2:\")\nprint(A / 2)\n\nOriginal matrix:\n[[1 2]\n [3 4]]\n\nMultiply by 2:\n[[2 4]\n [6 8]]\n\nDivide by 2:\n[[0.5 1. ]\n [1.5 2. ]]\n\n\n\n\n3.9.5.3 Matrix Multiplication\nNumPy provides several ways to perform matrix multiplication:\n\nA = np.array([[1, 2],\n              [3, 4]])\nB = np.array([[5, 6],\n              [7, 8]])\n\nprint(\"Matrix multiplication (A @ B):\")\nprint(A @ B)          # Preferred method (Python 3.5+)\n\nprint(\"\\nElement-wise multiplication (A * B):\")\nprint(A * B)          # Hadamard product\n\nMatrix multiplication (A @ B):\n[[19 22]\n [43 50]]\n\nElement-wise multiplication (A * B):\n[[ 5 12]\n [21 32]]\n\n\n\n\n\n3.9.6 Common Matrix Operations\nHere are some frequently used matrix operations:\n\nA = np.array([[1, 2],\n              [3, 4]])\n\nprint(\"Original matrix:\")\nprint(A)\n\nprint(\"\\nTranspose:\")\nprint(A.T)\n\nprint(\"\\nMatrix trace:\")\nprint(np.trace(A))\n\nprint(\"\\nMatrix determinant:\")\nprint(np.linalg.det(A))\n\nprint(\"\\nMatrix inverse:\")\nprint(np.linalg.inv(A))\n\nOriginal matrix:\n[[1 2]\n [3 4]]\n\nTranspose:\n[[1 3]\n [2 4]]\n\nMatrix trace:\n5\n\nMatrix determinant:\n-2.0000000000000004\n\nMatrix inverse:\n[[-2.   1. ]\n [ 1.5 -0.5]]\n\n\n\n\n3.9.7 Important Notes\n\nAlways check matrix dimensions when performing operations\nUse the appropriate multiplication operator:\n\n@ or np.matmul() for matrix multiplication\n* for element-wise multiplication\n\nRemember that indexing starts at 0, not 1\nWhen extracting rows or columns:\n\nA single row: A[i] or A[i, :]\nA single column: A[:, j]",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrices-in-sympy",
    "href": "chapters/matrices.html#matrices-in-sympy",
    "title": "3  Matrices",
    "section": "3.10 Matrices in Sympy",
    "text": "3.10 Matrices in Sympy\nThis section covers fundamental matrix operations using SymPy’s Matrix class. We’ll explore creation, indexing, and both numeric and symbolic operations.\n\n3.10.1 Setup\nFirst, let’s import SymPy and set up symbolic variables:\n\nfrom sympy import Matrix, Symbol, init_printing, pprint\nimport sympy as sp\n\n# Setup pretty printing\ninit_printing()\n\n# Define some symbolic variables\nx = Symbol('x')\ny = Symbol('y')\n\n\n\n3.10.2 Creating Matrices\nSymPy provides several ways to create matrices:\n\n# From a list of lists\nA = Matrix([[1, 2, 3],\n            [4, 5, 6]])\nprint(\"Matrix A:\")\npprint(A)\n\n# Using special constructors\nzeros = Matrix.zeros(2, 3)    # 2x3 matrix of zeros\nones = Matrix.ones(3, 3)      # 3x3 matrix of ones\neye = Matrix.eye(3)           # 3x3 identity matrix\n\nprint(\"\\nZeros matrix:\")\npprint(zeros)\nprint(\"\\nIdentity matrix:\")\npprint(eye)\n\n# Symbolic matrix\nsymbolic = Matrix([[x, y],\n                  [y, x]])\nprint(\"\\nSymbolic matrix:\")\npprint(symbolic)\n\nMatrix A:\n⎡1  2  3⎤\n⎢       ⎥\n⎣4  5  6⎦\n\nZeros matrix:\n⎡0  0  0⎤\n⎢       ⎥\n⎣0  0  0⎦\n\nIdentity matrix:\n⎡1  0  0⎤\n⎢       ⎥\n⎢0  1  0⎥\n⎢       ⎥\n⎣0  0  1⎦\n\nSymbolic matrix:\n⎡x  y⎤\n⎢    ⎥\n⎣y  x⎦\n\n\n\n\n3.10.3 Matrix Properties and Shape\nSymPy matrices have several useful properties:\n\nA = Matrix([[1, 2, 3],\n            [4, 5, 6]])\nprint(f\"Shape: {A.shape}\")\nprint(f\"Number of rows: {A.rows}\")\nprint(f\"Number of columns: {A.cols}\")\n\nShape: (2, 3)\nNumber of rows: 2\nNumber of columns: 3\n\n\n\n\n3.10.4 Indexing and Slicing\nSymPy uses different indexing methods than NumPy:\n\nA = Matrix([[1, 2, 3],\n            [4, 5, 6],\n            [7, 8, 9]])\n\n# Individual elements (zero-based indexing)\nprint(\"First element:\", A[0, 0])\nprint(\"Second row, third column:\", A[1, 2])\n\n# Extracting rows\nprint(\"\\nFirst row:\")\npprint(A.row(0))\nprint(\"\\nSecond row:\")\npprint(A.row(1))\n\n# Extracting columns\nprint(\"\\nFirst column:\")\npprint(A.col(0))\nprint(\"\\nSecond column:\")\npprint(A.col(1))\n\n# Extracting submatrices\nprint(\"\\nSubmatrix:\")\npprint(A[0:2, 1:3])\n\nFirst element: 1\nSecond row, third column: 6\n\nFirst row:\n[1  2  3]\n\nSecond row:\n[4  5  6]\n\nFirst column:\n⎡1⎤\n⎢ ⎥\n⎢4⎥\n⎢ ⎥\n⎣7⎦\n\nSecond column:\n⎡2⎤\n⎢ ⎥\n⎢5⎥\n⎢ ⎥\n⎣8⎦\n\nSubmatrix:\n⎡2  3⎤\n⎢    ⎥\n⎣5  6⎦\n\n\n\n\n3.10.5 Basic Operations\n\n3.10.5.1 Addition and Subtraction\nMatrix addition and subtraction work both with numeric and symbolic matrices:\n\nA = Matrix([[1, 2],\n            [3, 4]])\nB = Matrix([[5, 6],\n            [7, 8]])\n\nprint(\"Matrix A:\")\npprint(A)\nprint(\"\\nMatrix B:\")\npprint(B)\nprint(\"\\nA + B:\")\npprint(A + B)\nprint(\"\\nA - B:\")\npprint(A - B)\n\n# Symbolic example\nC = Matrix([[x, y],\n            [y, x]])\nprint(\"\\nSymbolic addition A + C:\")\npprint(A + C)\n\nMatrix A:\n⎡1  2⎤\n⎢    ⎥\n⎣3  4⎦\n\nMatrix B:\n⎡5  6⎤\n⎢    ⎥\n⎣7  8⎦\n\nA + B:\n⎡6   8 ⎤\n⎢      ⎥\n⎣10  12⎦\n\nA - B:\n⎡-4  -4⎤\n⎢      ⎥\n⎣-4  -4⎦\n\nSymbolic addition A + C:\n⎡x + 1  y + 2⎤\n⎢            ⎥\n⎣y + 3  x + 4⎦\n\n\n\n\n3.10.5.2 Scalar Operations\nMultiply or divide a matrix by a scalar (numeric or symbolic):\n\nA = Matrix([[1, 2],\n            [3, 4]])\n\nprint(\"Original matrix:\")\npprint(A)\nprint(\"\\nMultiply by 2:\")\npprint(2 * A)\nprint(\"\\nMultiply by symbolic x:\")\npprint(x * A)\n\nOriginal matrix:\n⎡1  2⎤\n⎢    ⎥\n⎣3  4⎦\n\nMultiply by 2:\n⎡2  4⎤\n⎢    ⎥\n⎣6  8⎦\n\nMultiply by symbolic x:\n⎡ x   2⋅x⎤\n⎢        ⎥\n⎣3⋅x  4⋅x⎦\n\n\n\n\n3.10.5.3 Matrix Multiplication\nSymPy matrix multiplication works with both numeric and symbolic matrices:\n\nA = Matrix([[1, 2],\n            [3, 4]])\nB = Matrix([[5, 6],\n            [7, 8]])\n\nprint(\"Matrix multiplication (A * B):\")\npprint(A * B)\n\nMatrix multiplication (A * B):\n⎡19  22⎤\n⎢      ⎥\n⎣43  50⎦\n\n\n\n\n\n3.10.6 Other Matrix Operations\nSymPy provides powerful symbolic matrix operations:\n\nA = Matrix([[1, 2],\n            [3, 4]])\n\nprint(\"Original matrix:\")\npprint(A)\n\nprint(\"\\nTranspose:\")\npprint(A.transpose())\n\nprint(\"\\nMatrix trace:\")\npprint(A.trace())\n\nprint(\"\\nDeterminant:\")\npprint(A.det())\n\nprint(\"\\nMatrix inverse:\")\npprint(A.inv())\n\n# Symbolic example\nS = Matrix([[x, y],\n            [y, x]])\nprint(\"\\n5th power of S:\")\nS**5\n\nOriginal matrix:\n⎡1  2⎤\n⎢    ⎥\n⎣3  4⎦\n\nTranspose:\n⎡1  3⎤\n⎢    ⎥\n⎣2  4⎦\n\nMatrix trace:\n5\n\nDeterminant:\n-2\n\nMatrix inverse:\n⎡-2    1  ⎤\n⎢         ⎥\n⎣3/2  -1/2⎦\n\n5th power of S:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}x^{5} + 10 x^{3} y^{2} + 5 x y^{4} & 5 x^{4} y + 10 x^{2} y^{3} + y^{5}\\\\5 x^{4} y + 10 x^{2} y^{3} + y^{5} & x^{5} + 10 x^{3} y^{2} + 5 x y^{4}\\end{matrix}\\right]\\)\n\n\n\n\n3.10.7 Important Notes\n\nSymPy matrices use * for matrix multiplication (unlike NumPy’s @)\nIndexing is zero-based, similar to NumPy\nSymPy matrices are immutable - operations return new matrices\nRow and column extraction methods return Matrix objects\nSymPy can handle:\n\nSymbolic computations\nExact fractions\nAlgebraic expressions",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html",
    "href": "worksheets/matrix_multiplication.html",
    "title": "Problems",
    "section": "",
    "text": "Problem 1\nFor this problem let\n\\[A = \\begin{bmatrix}\n3 & 1 & -2 \\\\\n5 & -4 & 3\n\\end{bmatrix},\nB = \\begin{bmatrix}\n8 & 2\\\\\n-6 & -3\\\\\n2 & -4\n\\end{bmatrix}\\]\n\\[C = \\begin{bmatrix}\n2 & 3 \\\\\n-3 & 2\n\\end{bmatrix},\nD = \\begin{bmatrix}\n4 & 6 \\\\\n-2 & 5\n\\end{bmatrix},\nE = \\begin{bmatrix}\n-6 \\\\\n4\n\\end{bmatrix}\\]\nCompute the following. If the operation is not possible explain why",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-1",
    "href": "worksheets/matrix_multiplication.html#problem-1",
    "title": "Problems",
    "section": "",
    "text": "\\(-3A+B\\)\n\\(B - 3A^T\\)\n\\(I_3-AB\\)\n\\(AB-2I_2\\)\n\\(AC\\)\n\\(DA\\)\n\\(E^TCE\\)\n\\(BD+A^T\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-2",
    "href": "worksheets/matrix_multiplication.html#problem-2",
    "title": "Problems",
    "section": "Problem 2",
    "text": "Problem 2\nLet \\(A = \\begin{bmatrix} \\uparrow & \\uparrow & \\uparrow \\\\ \\mathbf{c}_1 & \\mathbf{c}_2 & \\mathbf{c}_3 \\\\ \\downarrow & \\downarrow & \\downarrow \\end{bmatrix} = \\begin{bmatrix} \\leftarrow \\mathbf{r}_1 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_2 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_3 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_4 \\rightarrow \\end{bmatrix}\\)\nand suppose that \\(A \\begin{bmatrix} 1 \\\\ 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 0 \\\\ 2 \\\\ -9 \\end{bmatrix}\\). For the following write the vectors in terms of the columns or rows of \\(A\\).\n\nWhat are the dimensions of \\(A\\)?\nIs any row of \\(A\\) orthogonal to \\((1,1,-1)\\)? (Actually, the question is if the transpose of any row is orthogonal to \\((1,1,-1)\\))\nWrite a vector \\(\\mathbf{v}\\) in terms of the rows of \\(A\\) that satisfies \\(\\mathbf{v} \\cdot (1,1,-1) = 3\\)\nWrite a vector \\(\\mathbf{v}\\) in terms of the rows of \\(A\\) that satisfies \\(\\mathbf{v} \\cdot (1,1,-1) = 1\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-3",
    "href": "worksheets/matrix_multiplication.html#problem-3",
    "title": "Problems",
    "section": "Problem 3",
    "text": "Problem 3\nLet \\(A = \\begin{bmatrix} \\uparrow & \\uparrow & \\uparrow \\\\ \\mathbf{c}_1 & \\mathbf{c}_2 & \\mathbf{c}_3 \\\\ \\downarrow & \\downarrow & \\downarrow \\end{bmatrix}\\), \\(B = \\begin{bmatrix} \\uparrow & \\uparrow & \\uparrow \\\\ \\mathbf{d}_1 & \\mathbf{d}_2 & \\mathbf{d}_3 \\\\ \\downarrow & \\downarrow & \\downarrow \\end{bmatrix}\\) and suppose that \\(A^tB = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\)\n\nWhat are the dimensions of \\(A\\) and \\(B\\)?\nFind the following dot products: \\(\\mathbf{c}_1 \\cdot \\mathbf{d}_1\\), \\(\\mathbf{c}_3 \\cdot \\mathbf{d}_1\\), \\(\\mathbf{c}_2 \\cdot \\mathbf{d}_3\\)\nFind a constant \\(\\alpha \\in \\mathbb{R}\\) such that \\(\\mathbf{c}_1 - \\alpha \\mathbf{c}_2\\) is orthogonal to \\(\\mathbf{d}_3\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-4",
    "href": "worksheets/matrix_multiplication.html#problem-4",
    "title": "Problems",
    "section": "Problem 4",
    "text": "Problem 4\nLet \\(A = \\begin{bmatrix} \\uparrow & \\uparrow & \\uparrow \\\\ \\mathbf{c}_1 & \\mathbf{c}_2 & \\mathbf{c}_3 \\\\ \\downarrow & \\downarrow & \\downarrow \\end{bmatrix}\\) and assume that \\(A^tA = \\begin{bmatrix} 1 & -1 & 1 \\\\ -1 & 2 & 0 \\\\ 1 & 0 & 3 \\end{bmatrix}\\)\n\nFind the following dot products: \\(\\mathbf{c}_1 \\cdot \\mathbf{c}_2\\), \\(\\mathbf{c}_1 \\cdot \\mathbf{c}_3\\), \\(\\mathbf{c}_2 \\cdot \\mathbf{c}_3\\)\nFind \\(\\|\\mathbf{c}_1\\|\\), \\(\\|\\mathbf{c}_2\\|\\), \\(\\|\\mathbf{c}_3\\|\\)\nFind \\(\\|\\mathbf{c}_1 + 3\\mathbf{c}_2\\|\\) and \\(\\|3\\mathbf{c}_2 - 4\\mathbf{c}_3\\|\\)\n\n\nFor the following problems we have:\n\\(A = \\begin{bmatrix}\\uparrow&\\uparrow&\\uparrow\\\\\n\\mathbf{c}_1&\\mathbf{c}_2 &\\mathbf{c}_3\\\\\n\\downarrow&\\downarrow&\\downarrow\\end{bmatrix} =\n\\begin{bmatrix} \\leftarrow \\mathbf{r}_1 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_2 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_3 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_4 \\rightarrow \\end{bmatrix}\\)\n\\(E_1 = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\),   \\(E_2 = \\begin{bmatrix} 1 & 0 & -2 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\),  \n\\(E_3 = \\begin{bmatrix} -1 & 0 & 0 & 0 \\\\ 1 & -1 & 0 & 0 \\\\ 0 & 1 & -1 & 0 \\\\ 0 & 0 & 1 & -1 \\end{bmatrix}\\),   \\(E_4 = \\begin{bmatrix} 1 & 0 & 1 & -10 \\\\ 2 & 2 & 1 & 0 \\\\ 0 & -2 & -1 & 0 \\\\ 4 & 0 & 0 & -1 \\end{bmatrix}\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-5",
    "href": "worksheets/matrix_multiplication.html#problem-5",
    "title": "Problems",
    "section": "Problem 5",
    "text": "Problem 5\nFor each of the following problems, indicate if the matrices can be multiplied. If they can, express the answer in terms of the rows of \\(A\\) or the columns of \\(A\\). If they cannot be multiplied, explain why.\n\n\\(AE_1\\)\n\\(E_1A\\)\n\\(AE_2\\)\n\\(E_2A\\)\n\\(AE_3\\)\n\\(E_3A\\)\n\\(AE_4\\)\n\\(E_4A\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-6",
    "href": "worksheets/matrix_multiplication.html#problem-6",
    "title": "Problems",
    "section": "Problem 6",
    "text": "Problem 6\nSuppose that \\(AB_1 = \\begin{bmatrix} \\uparrow & \\uparrow & \\uparrow & \\uparrow \\\\ (\\mathbf{c}_1-3\\mathbf{c}_2) & \\mathbf{c}_3 & \\mathbf{c}_2 & (8\\mathbf{c}_2-\\mathbf{c}_3) \\\\ \\downarrow & \\downarrow & \\downarrow & \\downarrow \\end{bmatrix}\\). Find \\(B_1\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-7",
    "href": "worksheets/matrix_multiplication.html#problem-7",
    "title": "Problems",
    "section": "Problem 7",
    "text": "Problem 7\nSuppose that \\(B_2A = \\begin{bmatrix} \\leftarrow \\mathbf{r}_1 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_2 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_3 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_4 \\rightarrow \\end{bmatrix}\\). Find \\(B_2\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-8",
    "href": "worksheets/matrix_multiplication.html#problem-8",
    "title": "Problems",
    "section": "Problem 8",
    "text": "Problem 8\nSuppose that \\(B_3A = \\begin{bmatrix} \\leftarrow (\\mathbf{r}_1+\\mathbf{r}_2-\\mathbf{r}_3) \\rightarrow \\\\ \\leftarrow (\\mathbf{r}_2-\\mathbf{r}_3+\\mathbf{r}_4) \\rightarrow \\end{bmatrix}\\). Find \\(B_3\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-9",
    "href": "worksheets/matrix_multiplication.html#problem-9",
    "title": "Problems",
    "section": "Problem 9",
    "text": "Problem 9\nSuppose that \\(B_4E_1 = A\\). Find \\(B_4\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-10-optional",
    "href": "worksheets/matrix_multiplication.html#problem-10-optional",
    "title": "Problems",
    "section": "Problem 10 (Optional)",
    "text": "Problem 10 (Optional)\nSuppose that \\(B_5\\begin{bmatrix} \\leftarrow (\\mathbf{r}_1+\\mathbf{r}_2+\\mathbf{r}_3+\\mathbf{r}_4) \\rightarrow \\\\ \\leftarrow (\\mathbf{r}_2+\\mathbf{r}_3+\\mathbf{r}_4) \\rightarrow \\\\ \\leftarrow (\\mathbf{r}_3+\\mathbf{r}_4) \\rightarrow \\\\ \\leftarrow \\mathbf{r}_4 \\rightarrow \\end{bmatrix} = A\\). Find \\(B_5\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html",
    "href": "applications/image_transforms_matrix.html",
    "title": "Real World Application",
    "section": "",
    "text": "Image Transformation with Matrices\nThis section explores how images are represented as matrices and demonstrates various transformations using Python. We’ll cover both grayscale and color images, showing how matrix operations can be used to create different visual effects.",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#image-transformation-with-matrices",
    "href": "applications/image_transforms_matrix.html#image-transformation-with-matrices",
    "title": "Real World Application",
    "section": "",
    "text": "Required Libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#black-and-white-images",
    "href": "applications/image_transforms_matrix.html#black-and-white-images",
    "title": "Real World Application",
    "section": "1. Black and White Images",
    "text": "1. Black and White Images\n\nUnderstanding Grayscale Representation\nGrayscale images are typically represented as 2D arrays (matrices) where each element represents the intensity of a pixel. When working with uint8 (integers) data type, the values range from 0 (black) to 255 (white). When working with float data type, the values should be normalized to the range 0 to 1.\n\n# Create a simple 3x4 grayscale image\ngrayscale_example = np.array([\n    [0, 85, 170, 255],    # Different shades of gray\n    [255, 170, 85, 0],    # Reversed pattern\n    [128, 128, 128, 128]  # Medium gray\n])\n\nplt.figure(figsize=(6, 4))\nplt.imshow(grayscale_example, cmap='gray')\nplt.colorbar()\nplt.title('3x4 Grayscale Example')\nplt.show()",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#color-images-rgb",
    "href": "applications/image_transforms_matrix.html#color-images-rgb",
    "title": "Real World Application",
    "section": "2. Color Images (RGB)",
    "text": "2. Color Images (RGB)\nColors can be represented in various digital formats - from HSV (Hue, Saturation, Value) to CMYK (Cyan, Magenta, Yellow, Key/Black). Here,we’ll work with the RGB (Red, Green, Blue) color model, where each pixel’s color is created by combining different intensities of these three primary colors.\n\nRGB Color Model\nColor images use three channels: Red, Green, and Blue. Each pixel is represented by three values, creating a 3D array with shape (height, width, 3). As with grayscale images, the values can be either in the range 0-255 (uint8) or 0-1 (float).\n\nCommon Colors:\n\nBlack: (0, 0, 0)\nWhite: (255, 255, 255)\nPure Red: (255, 0, 0)\nPure Green: (0, 255, 0)\nPure Blue: (0, 0, 255)\nYellow: (255, 255, 0) [Red + Green]\nMagenta: (255, 0, 255) [Red + Blue]\nCyan: (0, 255, 255) [Green + Blue]\n\n\n\n\nSimple RGB image with common colors\n\nsimple_rgb = np.array([\n    [[255,0,0], [0,255,0], [0,0,255], [255,255,255]],   # top row\n    [[255,255,0], [0,255,255],[0,0,0], [100,100,100]]   # bottom row\n])\n\nplt.figure(figsize=(4, 4))\nplt.imshow(simple_rgb)\nplt.title('2x4 RGB Image')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNavigating Color Images in NumPy\nWhen working with color images in NumPy, image[a,b,c] lets us access specific pixel values. The first two numbers (a,b) are the pixel coordinates - a selects the row (moving down), b selects the column (moving right). The last number c picks the color channel: 0 for red, 1 for green, or 2 for blue. So image[1,2,1] gets the green value at row 2, column 3.\nTo access entire color channels, you can use : to select all rows and columns. For example, image[:,:,0] gives you the complete red channel, image[:,:,1] the green channel, and image[:,:,2] the blue channel. Each channel is a 2D array of intensities from 0 to 255, which we visualize in grayscale in the folling function - bright pixels show where that color is strong, dark pixels where it’s absent.\n\ndef display_rgb_channels(image):\n    \"\"\"Display an image and its RGB channels separately\"\"\"\n    \n    # Create a figure with 2x2 subplots\n    fig, axes = plt.subplots(2, 2, figsize=(8,8))\n    \n    # Original image\n    axes[0,0].imshow(image)\n    axes[0,0].set_title('Original')\n    \n    # Red channel showed as gray\n    axes[0,1].imshow(image[:,:,0], cmap=\"gray\")\n    axes[0,1].set_title('Red Channel')\n    \n    # Green channel showed as gray\n    axes[1,0].imshow(image[:,:,1], cmap=\"gray\")\n    axes[1,0].set_title('Green Channel')\n    \n    # Blue channel showed as gray\n    axes[1,1].imshow(image[:,:,2], cmap=\"gray\")\n    axes[1,1].set_title('Blue Channel')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Load an image\ncircles = plt.imread('rgb_colors.png')\ndisplay_rgb_channels(circles)\nbutterfly = plt.imread('butterfly.jpg')\ndisplay_rgb_channels(butterfly)",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#converting-between-images-and-matrices",
    "href": "applications/image_transforms_matrix.html#converting-between-images-and-matrices",
    "title": "Real World Application",
    "section": "3. Converting Between Images and Matrices",
    "text": "3. Converting Between Images and Matrices\n\nUnderstanding Image Arrays and Reshaping\nA grayscale image is stored as a 2D array with shape (height, width), while a color image uses a 3D array with shape (height, width, 3). For example, a 100x100 color image has shape (100, 100, 3), where the third dimension holds RGB values.\nMatrix operations require 2D arrays, so we need to reorganize our 3D color images. We transform from height × width × 3 to a matrix of (height × width) rows by 3 columns, flattening the spatial dimensions while keeping color information.\nThe reshape(-1,3) method transforms our 3D color image into a 2D matrix. The -1 tells NumPy to automatically calculate the number of rows needed, while 3 specifies we want 3 columns. For example, an image of shape (100,100,3) becomes a matrix of shape (10000,3), where each row represents one pixel’s values. The columns have a specific meaning: the first column contains all red values, the second green, and the third blue.\n\n\nExample\nLet’s illustrate this is a simple example:\n\nimg_2by2 = np.array([\n    [[1,2,3],[4,5,6]],      # top row\n    [[7,8,9],[10,11,12]]    # bottom row\n])\nprint(\"The original image:\")\nprint(img_2by2)\nprint(\"Shape:\", img_2by2.shape)\n\nprint(\"\\n\" + \"-\"*40 + \"\\n\")  # adds a dividing line\n\nM_2by2 = img_2by2.reshape(-1,3)\nprint(\"The reshaped matrix:\")\nprint(M_2by2)\nprint(\"Shape:\", M_2by2.shape)\n\nThe original image:\n[[[ 1  2  3]\n  [ 4  5  6]]\n\n [[ 7  8  9]\n  [10 11 12]]]\nShape: (2, 2, 3)\n\n----------------------------------------\n\nThe reshaped matrix:\n[[ 1  2  3]\n [ 4  5  6]\n [ 7  8  9]\n [10 11 12]]\nShape: (4, 3)\n\n\nNotice that the columns represent red, green, and blue values respectively. This process is reversible. If we write M_2by2.reshape(img_2by2.shape) we get img_2by2 back - no need to remember the original dimensions since they’re stored in the shape attribute. However, if you want, you can also write M_2by2.reshape(2,2).\n\nM_2by2.reshape(img_2by2.shape)\n\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]])\n\n\n\n\nHelper Functions\nWe’ll use these helper functions to convert between image and matrix formats throughout our examples:\n\ndef image_to_matrix(image):\n    \"\"\"Convert image to matrix format (n_pixels × 3)\"\"\"\n    return image.reshape(-1, 3)\n\ndef matrix_to_image(matrix, original_shape):\n    \"\"\"Convert matrix back to image format\"\"\"\n    return matrix.reshape(original_shape)",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#color-transformations-using-permutation-matrices",
    "href": "applications/image_transforms_matrix.html#color-transformations-using-permutation-matrices",
    "title": "Real World Application",
    "section": "4. Color Transformations Using Permutation Matrices",
    "text": "4. Color Transformations Using Permutation Matrices\n\nSwapping Color Channels\nWe can use permutation matrices to swap color channels:\n\ndef swap_colors(image, permutation_matrix):\n    \"\"\"Apply color permutation to image\"\"\"\n    matrix = image_to_matrix(image)\n    transformed = matrix @ permutation_matrix\n    return matrix_to_image(transformed, image.shape)\n\n# Example permutation matrices\nRGB_to_BGR = np.array([\n    [0, 0, 1],\n    [0, 1, 0],\n    [1, 0, 0]\n])\n\n\n\n\n\n\n\nExercise: Find all six 3×3 permutation matrices.\n\n\n\n\n\n\nIdentity:\n\\(\\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&1\\end{bmatrix}\\)\nSingle swap (R-G):\n\\(\\begin{bmatrix}0&1&0\\\\1&0&0\\\\0&0&1\\end{bmatrix}\\)\nSingle swap (R-B):\n\\(\\begin{bmatrix}0&0&1\\\\0&1&0\\\\1&0&0\\end{bmatrix}\\)\nSingle swap (G-B):\n\\(\\begin{bmatrix}1&0&0\\\\0&0&1\\\\0&1&0\\end{bmatrix}\\)\nCyclic (R→G→B→R):\n\\(\\begin{bmatrix}0&0&1\\\\1&0&0\\\\0&1&0\\end{bmatrix}\\)\nCyclic Cyclic (R→B→G→R):\n\\(\\begin{bmatrix}0&1&0\\\\0&0&1\\\\1&0&0\\end{bmatrix}\\)",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#grayscale-conversion-and-negative",
    "href": "applications/image_transforms_matrix.html#grayscale-conversion-and-negative",
    "title": "Real World Application",
    "section": "5. Grayscale Conversion and Negative",
    "text": "5. Grayscale Conversion and Negative\nWhen converting a color image to grayscale, we don’t simply average the RGB values. Our eyes have different sensitivities to different colors, with green light being perceived as brightest and blue as darkest. To create natural-looking grayscale images, we use weighted averages that match human perception: 29.9% for red, 58.7% for green, and 11.4% for blue.\nThe negative of an image can be obtained by subtracting each pixel value from the maximum possible value (255 for 8-bit images). Thanks to NumPy’s broadcasting capabilities, we can simply write 255 - image and this operation will be applied to every pixel value automatically, whether it’s a grayscale or color image. Here’s a simple function to create image negatives:\n\ndef to_grayscale(image):\n    \"\"\"Convert RGB image to grayscale using weighted sum\"\"\"\n    weights = np.array([0.299, 0.587, 0.114])\n    matrix = image_to_matrix(image)\n    grayscale_values = matrix @ weights\n    return grayscale_values.reshape(image.shape[:2])\n\ndef create_negative(image):\n    \"\"\"Create negative of an image\"\"\"\n    return 255 - image\n\n# Upload black and white image of a dog\ndog = np.array(Image.open('grayscale.png').convert('L'))\n# Show image and negative\nfig, axes = plt.subplots(2, 1, figsize=(8, 10))\naxes[0].set_title('Original Grayscale Image')\naxes[0].imshow(dog, cmap='gray')\naxes[1].set_title('Negative Image')\naxes[1].imshow(255-dog,cmap = 'gray')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#color-filters",
    "href": "applications/image_transforms_matrix.html#color-filters",
    "title": "Real World Application",
    "section": "6. Color Filters",
    "text": "6. Color Filters\n\nImplementing a Sepia Filter\nA sepia filter transforms a regular color image into one with a warm, brownish tone reminiscent of vintage photographs. To create this effect, we need to adjust each color channel using specific weights. For each pixel, the new RGB values are calculated as a combination of the original values:\nThe red channel is amplified with warm tones The green channel is moderately reduced The blue channel is significantly reduced\nThis creates the characteristic reddish-brown tint that gives sepia images their antique appearance.\n\ndef apply_sepia(image):\n    \"\"\"Apply sepia filter to image\"\"\"\n    sepia_matrix = np.array([\n        [0.393, 0.349, 0.272],\n        [0.769, 0.686, 0.534],\n        [0.189, 0.168, 0.131]\n    ])\n    \n    matrix = image_to_matrix(image)\n    sepia = matrix @ sepia_matrix\n    \n    # Clip values to valid range\n    sepia = np.clip(sepia, 0, 1)\n    return matrix_to_image(sepia, image.shape)\n\n\n\nImplementing a Color Intensification Filter\nA color intensification filter makes images more vibrant by amplifying the primary colors while reducing color bleeding between channels. To create this effect, each color channel is multiplied by 1.5 (intensifying its own color) while subtracting a quarter of the other colors’ intensities. This process:\n\nBoosts each channel’s own color\nReduces the influence of other colors\nIncreases contrast between different colored areas\n\nThis creates a more vivid appearance with enhanced color separation and impact.\n\ndef intensify_colors(image):\n    \"\"\"Apply color intensification filter to image\"\"\"\n    intensity_matrix = np.array([\n        [1.5, -0.25, -0.25],\n        [-0.25, 1.5, -0.25],\n        [-0.25, -0.25, 1.5]\n    ])\n    \n    matrix = image_to_matrix(image)\n    intensified = matrix @ intensity_matrix\n    \n    # Clip values to valid range\n    intensified = np.clip(intensified, 0, 1)\n    return matrix_to_image(intensified, image.shape)",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#example-usage",
    "href": "applications/image_transforms_matrix.html#example-usage",
    "title": "Real World Application",
    "section": "Example Usage",
    "text": "Example Usage\nHere’s how to use these transformations on an actual image:\n\n# Load an image: Open it in Image, and convert it to a numpy array as a float\nimage = np.array(Image.open('city_river.jpg')).astype(float)/255\n\n# Display original and transformed versions\nfig, axes = plt.subplots(5, 1, figsize=(10,25))\n\naxes[0].imshow(image)\naxes[0].set_title('Original')\n\naxes[1].imshow(swap_colors(image, RGB_to_BGR))\naxes[1].set_title('RGB to BGR')\n\naxes[2].imshow(to_grayscale(image), cmap='gray')\naxes[2].set_title('Grayscale')\n\naxes[3].imshow(apply_sepia(image))\naxes[3].set_title('Sepia')\n\naxes[4].imshow(intensify_colors(image))\naxes[4].set_title('Intensification')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#conclusion",
    "href": "applications/image_transforms_matrix.html#conclusion",
    "title": "Real World Application",
    "section": "Conclusion",
    "text": "Conclusion\nThis section demonstrates how matrices naturally represent and transform digital images. When an RGB image is reshaped, it becomes a matrix with three columns representing the color channels: \\[\n\\begin{bmatrix}\n\\color{red}\\uparrow & \\color{green}\\uparrow & \\color{blue}\\uparrow\\\\\n\\color{red}\\mathbf{c}_1 & \\color{green}\\mathbf{c}_2 & \\color{blue}\\mathbf{c}_3\\\\\n\\color{red}\\downarrow & \\color{green}\\downarrow & \\color{blue}\\downarrow\n\\end{bmatrix}.\n\\]\nThe power of matrix multiplication (Equation 3.1) becomes evident in image processing. When we multiply this matrix by another matrix on the right, we take linear combinations of these color channels, enabling various transformations:\n\nColor channel permutations (by rearranging columns)\nGrayscale conversion (by weighted averaging of channels)\nSepia filter effects (through specific linear combinations)\n\nThis direct connection between abstract matrix operations and visual transformations provides a concrete example of linear combinations in practice. Understanding how matrices act on these color channels helps explain why matrix multiplication works the way it does and illustrates its practical applications.",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "parts/linear_maps.html",
    "href": "parts/linear_maps.html",
    "title": "Linear Transformation",
    "section": "",
    "text": "This section covers Linear Transformations.\n\nLinear Transformations\nProblems",
    "crumbs": [
      "Linear Transformation"
    ]
  },
  {
    "objectID": "chapters/linear_maps.html",
    "href": "chapters/linear_maps.html",
    "title": "4  Linear Transformations in \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "4.1 Motivating Example\nIf \\(A\\) is an \\(m\\times n\\) matrix, Equation 3.1 tells us that \\(A\\) induces a map \\(A:\\mathbb{R}^n\\to\\mathbb{R}^m\\) defined by the Matrix-Vector formula \\(A\\mathbf{x}\\). This map has two important properties:\nThese properties combined yield a more general result. For any vectors \\(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_k \\in \\mathbb{R}^n\\) and scalars \\(c_1, c_2, \\ldots, c_k \\in \\mathbb{R}\\): \\[A(c_1\\mathbf{x}_1+c_2\\mathbf{x}_2+\\cdots+c_k\\mathbf{x}_k)=c_1(A\\mathbf{x}_1)+c_2(A\\mathbf{x}_2)+\\cdots+c_k(A\\mathbf{x}_k) \\tag{4.1}\\]\nLet’s check 1: Let \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) and \\(\\mathbf{y}=(y_1,\\dots,y_n)\\) be two arbtrary elements of \\(\\mathbb{R}^n\\). Then writing the product in terms of the columns of \\(A\\) and using standard operations of vectors we get: \\[\\begin{aligned}\nA(\\mathbf{x}+\\mathbf{y}) &=\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\\left(\n\\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_n \\end{bmatrix}  +    \n\\begin{bmatrix}y_1\\\\y_2\\\\\\vdots\\\\y_n \\end{bmatrix}     \n\\right) \\\\\n&=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\n\\begin{bmatrix}x_1+y_1\\\\x_2+y_2\\\\\\vdots\\\\x_n+y_n \\end{bmatrix} \\\\\n&= (x_1+y_1)\\mathbf{c}_1+\\cdots +(x_n+y_n)\\mathbf{c}_n\\\\\n&=(x_1\\mathbf{c}_1+\\cdots +x_n\\mathbf{c}_n)+(y_1\\mathbf{c}_1+\\cdots + y_n\\mathbf{c}_n)\\\\\n&=A\\mathbf{x}+A\\mathbf{y}.\n\\end{aligned}\n\\]\nExercise: Check property 2. That is, for every \\(\\mathbf{x}\\in\\mathbb{R}^n\\) and every \\(c\\in\\mathbb{R}\\), \\(A(c\\mathbf{x})=cA\\mathbf{x}\\).\nAs a consequence of these properties we obtain",
    "crumbs": [
      "Linear Transformation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Transformations in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/linear_maps.html#motivating-example",
    "href": "chapters/linear_maps.html#motivating-example",
    "title": "4  Linear Transformations in \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "For every \\(\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^n, A(\\mathbf{x}+\\mathbf{y})=A\\mathbf{x}+A\\mathbf{y}\\), and\nFor every \\(\\mathbf{x}\\in\\mathbb{R}^n\\) and every \\(c\\in\\mathbb{R}\\), \\(A(c\\mathbf{x})=cA\\mathbf{x}\\)\n\n\n\n\n\n\n\n\n\n\nClick to see the proof\n\n\n\n\n\nLet \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) be an arbitrary element in \\(\\mathbb{R}^n\\) and let \\(c\\in\\mathbb{R}\\) be a scalar. Writing the product in terms of the columns of \\(A\\) and using standard operations on vectors we get: \\[\n\\begin{aligned}\nA(c\\mathbf{x}) &=\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\n\\left(\nc\\begin{bmatrix}x_1\\\\ x_2\\\\\\vdots\\\\ x_n \\end{bmatrix}  \n\\right) \\\\\n&= \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\\left(\n\\begin{bmatrix}cx_1\\\\ cx_2\\\\ \\vdots\\\\ cx_n \\end{bmatrix}  \n\\right) \\\\\n&= (cx_1)\\mathbf{c}_1+\\cdots +(cx_n)\\mathbf{c}_n\\\\\n&= c(x_1\\mathbf{c}_1+\\cdots +x_n\\mathbf{c}_n)\\\\\n&= c(A\\mathbf{x})\n\\end{aligned}\n\\]\n\n\n\n\n\nLemma 4.1 Suppose that \\(A\\) is an \\(m\\times n\\) matrix and that \\(B\\) is an \\(n\\times p\\) matrix. Then for every \\(\\mathbf{x}\\in\\mathbb{R}^p\\), \\[(AB)\\mathbf{x}=A(B\\mathbf{x})\\]\n\n\nProof. Express \\(B\\) in terms of its columns: \\(B=\\begin{bmatrix}\\mathbf{b}_1&\\mathbf{b}_2&\\cdots&\\mathbf{b}_p\\end{bmatrix}\\), where each \\(\\mathbf{b}_i\\) is an \\(n\\)-dimensional column vector. By Equation 3.4, the product \\(AB\\) is defined by: \\[\nAB=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\nA\\mathbf{b}_1 & A\\mathbf{b}_2 & \\cdots & A\\mathbf{b}_p \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\n\\] Now, for any vector \\(\\mathbf{x}=(x_1,\\ldots,x_p)\\in\\mathbb{R}^p\\), we use Equation 3.1 to compute \\[\\begin{align}\n(AB)\\mathbf{x}&=x_1(A\\mathbf{b}_1)+x_2(A\\mathbf{b}_2)+\\cdots+x_p(A\\mathbf{b}_p),\\quad\\text{and}\\\\\nB\\mathbf{x}&=x_1\\mathbf{b}_1+x_2\\mathbf{b}_2+\\cdots+x_p\\mathbf{b}_p.\n\\end{align}\\] Therefore, using Equation 4.1 we obtain \\[\\begin{align*}\nA(B\\mathbf{x}) &= A(x_1\\mathbf{b}_1+x_2\\mathbf{b}_2+\\cdots+x_p\\mathbf{b}_p) \\\\\n&= x_1(A\\mathbf{b}_1)+x_2(A\\mathbf{b}_2)+\\cdots+x_p(A\\mathbf{b}_p) \\\\\n&= (AB)\\mathbf{x}\\quad\\square\n\\end{align*}\\]",
    "crumbs": [
      "Linear Transformation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Transformations in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/linear_maps.html#linear-transformations",
    "href": "chapters/linear_maps.html#linear-transformations",
    "title": "4  Linear Transformations in \\(\\mathbb{R}^n\\)",
    "section": "4.2 Linear Transformations",
    "text": "4.2 Linear Transformations\n\nDefinition: A function \\(T:\\mathbb{R}^n\\to\\mathbb{R}^m\\) is a linear transformation if it satisfies two properties:\n\nAdditivity: For every \\(\\mathbf{x}_1,\\mathbf{x}_2\\in\\mathbb{R}^n, T(\\mathbf{x}_1+\\mathbf{x}_2)=T(\\mathbf{x}_1)+T(\\mathbf{x}_2)\\), and\nScalar Multiplication: For every \\(\\mathbf{x}\\in\\mathbb{R}^n\\) and every \\(c\\in\\mathbb{R}\\), \\(T(c\\mathbf{x})=cT(\\mathbf{x})\\)\n\n\nThese properties naturally extend to any finite collection of vectors. For vectors \\(\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_k\\in\\mathbb{R}^n\\) and scalars \\(c_1,c_2,\\ldots,c_k\\in\\mathbb{R}\\), we have \\[T(c_1\\mathbf{x}_1+\\cdots+c_k\\mathbf{x}_k)=c_1T(\\mathbf{x}_1)+\\cdots+c_kT(\\mathbf{x}_k) \\tag{4.2}\\] This is an important formula that we will use many times.\nNotice that we just establihed that an \\(m\\times n\\) matrix \\(A\\) induces a linear transformation \\(A:\\mathbb{R}^n\\to\\mathbb{R}^m\\) defined by \\(A\\mathbf{x}\\). In this section we will demonstrate the converse: that any linear transformation from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\) can be expressed as matrix multiplication. The following simple example will illustrate this fundamental property.\n\nExample: Let \\(T:\\mathbb{R}^2\\to\\mathbb{R}^3\\) be defined by \\(T\\left( \\begin{bmatrix}a\\\\ b\\end{bmatrix} \\right)=\\begin{bmatrix}a+b\\\\ b-2a\\\\ a\\end{bmatrix}\\)\n\nThe first step is to show that the function is a linear transformation. Try to do it using the definition, but feel free to click to see the detailed proof.\n\nExercise: Prove that \\(T:\\mathbb{R}^2\\to\\mathbb{R}^3\\) is a linear transformation.\n\n\n\n\n\n\nClick to see the proof\n\n\n\n\n\n\nProof. To prove that \\(T\\) is a linear transformation, we must verify both properties from the definition:\n\nAdditivity: \\(T(\\mathbf{x}+\\mathbf{y})=T(\\mathbf{x})+T(\\mathbf{y})\\) for all \\(\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^2\\)\nScalar multiplication: \\(T(c\\mathbf{x})=cT(\\mathbf{x})\\) for all \\(\\mathbf{x}\\in\\mathbb{R}^2\\) and \\(c\\in\\mathbb{R}\\)\n\nProperty 1 (Additivity): Let \\(\\mathbf{x}=\\begin{bmatrix}x_1\\\\ x_2\\end{bmatrix}\\) and \\(\\mathbf{y}=\\begin{bmatrix}y_1\\\\ y_2\\end{bmatrix}\\) be arbitrary vectors in \\(\\mathbb{R}^2\\).\nFirst, let’s compute \\(T(\\mathbf{x}+\\mathbf{y})\\): \\[\\begin{align*}\nT(\\mathbf{x}+\\mathbf{y}) &= T\\left(\\begin{bmatrix}x_1+y_1\\\\ x_2+y_2\\end{bmatrix}\\right)\\\\\n&= \\begin{bmatrix}(x_1+y_1)+(x_2+y_2)\\\\ (x_2+y_2)-2(x_1+y_1)\\\\ x_1+y_1\\end{bmatrix}\\\\\n&= \\begin{bmatrix}x_1+x_2+y_1+y_2\\\\ x_2+y_2-2x_1-2y_1\\\\ x_1+y_1\\end{bmatrix}\n\\end{align*}\\]\nNow, let’s compute \\(T(\\mathbf{x})+T(\\mathbf{y})\\): \\[\\begin{align*}\nT(\\mathbf{x})+T(\\mathbf{y}) &= \\begin{bmatrix}x_1+x_2\\\\ x_2-2x_1\\\\ x_1\\end{bmatrix} + \\begin{bmatrix}y_1+y_2\\\\ y_2-2y_1\\\\ y_1\\end{bmatrix}\\\\\n&= \\begin{bmatrix}x_1+x_2+y_1+y_2\\\\ x_2-2x_1+y_2-2y_1\\\\ x_1+y_1\\end{bmatrix}\n\\end{align*}\\]\nSince these are equal for any choice of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), Property 1 is verified.\nProperty 2 (Scalar Multiplication): Let \\(\\mathbf{x}=\\begin{bmatrix}x_1\\\\ x_2\\end{bmatrix}\\) be an arbitrary vector in \\(\\mathbb{R}^2\\) and \\(c\\in\\mathbb{R}\\) be an arbitrary scalar.\nFirst, let’s compute \\(T(c\\mathbf{x})\\): \\[\\begin{align*}\nT(c\\mathbf{x}) &= T\\left(\\begin{bmatrix}cx_1\\\\ cx_2\\end{bmatrix}\\right)\\\\\n&= \\begin{bmatrix}cx_1+cx_2\\\\ cx_2-2(cx_1)\\\\ cx_1\\end{bmatrix}\\\\\n&= \\begin{bmatrix}c(x_1+x_2)\\\\ c(x_2-2x_1)\\\\ cx_1\\end{bmatrix}\n\\end{align*}\\]\nNow, let’s compute \\(cT(\\mathbf{x})\\): \\[\\begin{align*}\ncT(\\mathbf{x}) &= c\\begin{bmatrix}x_1+x_2\\\\ x_2-2x_1\\\\ x_1\\end{bmatrix}\\\\\n&= \\begin{bmatrix}c(x_1+x_2)\\\\ c(x_2-2x_1)\\\\ cx_1\\end{bmatrix}\n\\end{align*}\\]\nSince these are equal for any choice of \\(\\mathbf{x}\\) and \\(c\\), Property 2 is verified.\nTherefore, since both properties hold, \\(T\\) is indeed a linear transformation.\n\n\n\n\n\nNow that we know that \\(T:\\mathbb{R}^2\\to\\mathbb{R}^3\\) is linear we use elementary vector operations, Equation 4.2, and Equation 3.1 to obtain: \\[\\begin{aligned}\nT\\left( \\begin{bmatrix}a\\\\ b\\end{bmatrix} \\right)\n&=T\\left( a\\begin{bmatrix}1\\\\ 0\\end{bmatrix} + b\\begin{bmatrix}0\\\\ 1\\end{bmatrix}\\right) \\\\\n&=aT\\left( \\begin{bmatrix}1\\\\ 0\\end{bmatrix}\\right) + bT\\left(\\begin{bmatrix}0\\\\ 1\\end{bmatrix}\\right) \\\\\n&=a\\begin{bmatrix}1\\\\ -1\\\\ 1\\end{bmatrix} + b\\begin{bmatrix}1\\\\ 1\\\\0\\end{bmatrix}\n= \\begin{bmatrix}1 & 1\\\\ -1 &1\\\\ 1&0\\end{bmatrix}\\begin{bmatrix}a\\\\ b\\end{bmatrix}.\n\\end{aligned}\\] This means that for every \\(\\mathbf{x}\\in\\mathbb{R}^2\\), \\(T(\\mathbf{x})=A\\mathbf{x}\\) for \\(A= \\begin{bmatrix}1 & 1\\\\ -1 &1\\\\ 1&0\\end{bmatrix}\\)\n\nTheorem 4.1 Theorem: Let \\(T:\\mathbb{R}^n\\to\\mathbb{R}^m\\) be a linear transformation. Then there exists an \\(m\\times n\\) matrix \\(A\\) such that for every \\(\\mathbf{x}\\in\\mathbb{R}^n\\), \\(T(\\mathbf{x})=A\\mathbf{x}\\).\n\n\nProof. The proof consists of three main steps:\n\nFirst, let’s identify the canonical basis vectors of \\(\\mathbb{R}^n\\). Let \\(\\mathbf{e}_i\\) denote the \\(i\\)-th canonical basis vector: \\[\\mathbf{e}_1=\\begin{bmatrix}1\\\\0\\\\ \\vdots\\\\ 0\\end{bmatrix},\\quad \\mathbf{e}_2=\\begin{bmatrix}0\\\\1\\\\ \\vdots\\\\ 0\\end{bmatrix},\\quad \\ldots, \\quad \\mathbf{e}_n=\\begin{bmatrix}0\\\\0\\\\ \\vdots\\\\ 1\\end{bmatrix}\\]\nAny vector \\(\\mathbf{x}\\in\\mathbb{R}^n\\) can be written uniquely as a linear combination of these basis vectors: \\[\\mathbf{x}=\\begin{bmatrix}x_1\\\\ x_2\\\\ \\vdots\\\\ x_n\\end{bmatrix}=x_1\\mathbf{e}_1 + x_2\\mathbf{e}_2 + \\cdots + x_n\\mathbf{e}_n\\]\nNow, using the linearity of \\(T\\) (see Equation 4.2), we have: \\[\\begin{aligned}\nT(\\mathbf{x})&=T(x_1\\mathbf{e}_1 + x_2\\mathbf{e}_2 + \\cdots + x_n\\mathbf{e}_n)\\\\\n&=x_1T(\\mathbf{e}_1) + x_2T(\\mathbf{e}_2) + \\cdots + x_nT(\\mathbf{e}_n)\n\\end{aligned}\n\\]\ndefine the matrix \\(A\\) by using the transformed basis vectors as its columns: \\[A=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\nT(\\mathbf{e}_1) & T(\\mathbf{e}_2) & \\cdots & T(\\mathbf{e}_n) \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow \\\\\n\\end{bmatrix}\\]\nThen by the definition of matrix multiplication (see Equation 3.1): \\[\\begin{aligned}\nA\\mathbf{x}&=\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\nT(\\mathbf{e}_1) & T(\\mathbf{e}_2) & \\cdots & T(\\mathbf{e}_n) \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow \\\\\n\\end{bmatrix}\n\\begin{bmatrix}x_1\\\\ x_2\\\\ \\vdots\\\\ x_n\\end{bmatrix} \\\\\n&=x_1T(\\mathbf{e}_1) + x_2T(\\mathbf{e}_2) + \\cdots + x_nT(\\mathbf{e}_n)=T(\\mathbf{x})\n\\end{aligned}.\n\\]\n\nTherefore, we have constructed a matrix \\(A\\) such that \\(T(\\mathbf{x})=A\\mathbf{x}\\) for all \\(\\mathbf{x}\\in\\mathbb{R}^n\\). Note that \\(A\\) is \\(m\\times n\\) since it has \\(n\\) columns, and each column \\(T(\\mathbf{e}_i)\\in\\mathbb{R}^m\\).\n\n\n\n\n\n\n\n\nKey Algorithm\n\n\n\nThe definition of the matrix \\(A\\) provides an algorithm for finding the matrix representation of any linear transformation \\(T:\\mathbb{R}^n\\to\\mathbb{R}^m\\). Simply compute \\(T(\\mathbf{e}_i)\\) for each canonical basis vector and use these vectors as the columns of \\(A\\): \\[A=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\nT(\\mathbf{e}_1) & T(\\mathbf{e}_2) & \\cdots & T(\\mathbf{e}_n) \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\\] In other words, the \\(j\\)-th column of \\(A\\) is the output of the transformation \\(T\\) when applied to the \\(j\\)-th canonical basis vector \\(\\mathbf{e}_j\\).",
    "crumbs": [
      "Linear Transformation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Transformations in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/linear_maps.html#composition-of-linear-transformations",
    "href": "chapters/linear_maps.html#composition-of-linear-transformations",
    "title": "4  Linear Transformations in \\(\\mathbb{R}^n\\)",
    "section": "4.3 Composition of Linear Transformations",
    "text": "4.3 Composition of Linear Transformations\nRecall that if \\(T_1:\\mathbb{R}^n\\to\\mathbb{R}^m\\) and \\(T_2:\\mathbb{R}^m\\to \\mathbb{R}^p\\) are functions, the composition \\(T_2\\circ T_1:\\mathbb{R}^n\\to\\mathbb{R}^p\\) is defined by: \\[T_2\\circ T_1(\\mathbf{x})=T_2(T_1(\\mathbf{x}))\\quad\\text{for any}\\quad \\mathbf{x}\\in\\mathbb{R}^n.\\]\nIn this section we will prove that the composition of linear maps is linear and that matrix multiplication corresponds to composition of linear maps.\n\nThoerem: Suppose that \\(T_1:\\mathbb{R}^n\\to\\mathbb{R}^m\\) and \\(T_2:\\mathbb{R}^m\\to \\mathbb{R}^p\\) are linear maps. Then \\(T_2\\circ T_1:\\mathbb{R}^n\\to\\mathbb{R}^p\\) is linear.\n\n\nProof. To prove \\(T_2\\circ T_1\\) is linear, we need to show it satisfies two properties:\n\nAdditivity: \\((T_2\\circ T_1)(\\mathbf{x} + \\mathbf{y}) = (T_2\\circ T_1)(\\mathbf{x}) + (T_2\\circ T_1)(\\mathbf{y})\\) for all \\(\\mathbf{x},\\mathbf{y} \\in \\mathbb{R}^n\\)\nScalar multiplication: \\((T_2\\circ T_1)(c\\mathbf{x}) = c(T_2\\circ T_1)(\\mathbf{x})\\) for all \\(\\mathbf{x} \\in \\mathbb{R}^n\\) and \\(c \\in \\mathbb{R}\\)\n\nLet’s prove each property:\n\nFirst, let’s prove additivity. Let \\(\\mathbf{x},\\mathbf{y} \\in \\mathbb{R}^n\\). Then:\n\\((T_2\\circ T_1)(\\mathbf{x} + \\mathbf{y})\\) = \\(T_2(T_1(\\mathbf{x} + \\mathbf{y}))\\)\nSince \\(T_1\\) is linear: = \\(T_2(T_1(\\mathbf{x}) + T_1(\\mathbf{y}))\\)\nSince \\(T_2\\) is linear: = \\(T_2(T_1(\\mathbf{x})) + T_2(T_1(\\mathbf{y}))\\)\nBy definition of composition: = \\((T_2\\circ T_1)(\\mathbf{x}) + (T_2\\circ T_1)(\\mathbf{y})\\)\nNext, let’s prove the scalar multiplication property. Let \\(\\mathbf{x} \\in \\mathbb{R}^n\\) and \\(c \\in \\mathbb{R}\\). Then:\n\\((T_2\\circ T_1)(c\\mathbf{x})\\) = \\(T_2(T_1(c\\mathbf{x}))\\)\nSince \\(T_1\\) is linear: = \\(T_2(cT_1(\\mathbf{x}))\\)\nSince \\(T_2\\) is linear: = \\(cT_2(T_1(\\mathbf{x}))\\)\nBy definition of composition: = \\(c(T_2\\circ T_1)(\\mathbf{x})\\)\n\nSince both properties hold, we conclude that \\(T_2\\circ T_1\\) is linear. \\(\\square\\)\n\n\nWe now prove that matrix multiplication corresponds to composition of linear maps\n\n\nTheorem: Let \\(T_1:\\mathbb{R}^n\\to\\mathbb{R}^m\\) and \\(T_2:\\mathbb{R}^m\\to \\mathbb{R}^p\\) be linear maps with matrix representations \\(B\\) and \\(A\\) respectively. Then \\(AB\\) is the matrix representation of \\(T_2\\circ T_1\\).\n\n\nProof. Let \\(\\mathbf{x}\\in\\mathbb{R}^n\\). Since \\(B\\) is the matrix representation of \\(T_1:\\mathbb{R}^n\\to\\mathbb{R}^m\\), it follows from Theorem 4.1 that \\(T_1(\\mathbf{x})=B\\mathbf{x}\\). Similarly, since \\(A\\) is the matrix representation of \\(T_2:\\mathbb{R}^m\\to \\mathbb{R}^p\\) and \\(B\\mathbf{x}\\in\\mathbb{R}^m\\) we have that \\(T_2(B\\mathbf{x})=A(B{\\mathbf{x}})\\). Combining these two facts with Lemma 4.1, we get \\[(T_2\\circ T_1)(\\mathbf{x})=T_2(T_1(\\mathbf{x}))=T_2(B\\mathbf{x})=A(B{\\mathbf{x}})=(AB)\\mathbf{x}\\] which implies that \\(AB\\) is the matrix representation of \\(T_2\\circ T_1\\). \\(\\square\\)\n\n\nSince composition of functions is associative, the previous theorem implies that multiplication of matrices is associative\n\n\nCorollary: Suppose that \\(A, B\\) and \\(C\\) are matrices of sizes \\(m\\times n\\), \\(n\\times p\\) and \\(p\\times q\\), respectively. Then \\[(AB)C=A(BC)\\]\n\nWe can also prove associativity using Lemma 4.1 and the definitions of product Equation 3.4, and we can also prove associativity using the formula Equation 3.5 to show that \\(((AB)C)_{ij}=(A(BC))_{ij}\\).",
    "crumbs": [
      "Linear Transformation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Transformations in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "worksheets/linearMapsEquations1.html",
    "href": "worksheets/linearMapsEquations1.html",
    "title": "Problems",
    "section": "",
    "text": "Linear Maps and Linear Equations 1",
    "crumbs": [
      "Linear Transformation",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/linearMapsEquations1.html#linear-maps-and-linear-equations-1",
    "href": "worksheets/linearMapsEquations1.html#linear-maps-and-linear-equations-1",
    "title": "Problems",
    "section": "",
    "text": "Determine if the following functions are linear. If they are, write them as a matrix multiplication. If they aren’t, explain why not:\n\n\\(T : \\mathbb{R}^2 \\to \\mathbb{R}^3\\) defined by \\[T\\left(\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\right) =\n\\begin{bmatrix}\n2a + b - 1 \\\\\nb + 8a \\\\\n2a - 3b\n\\end{bmatrix}\\]\n\\(T : \\mathbb{R}^3 \\to \\mathbb{R}^2\\) defined by \\[T\\left(\\begin{bmatrix}\na \\\\\nb \\\\\nc\n\\end{bmatrix}\\right) =\n\\begin{bmatrix}\n12a - 2b + 3c \\\\\n3c - 4b + a\n\\end{bmatrix}\\]\n\\(T : \\mathbb{R}^2 \\to \\mathbb{R}^2\\) defined by \\[T\\left(\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\right) =\n\\begin{bmatrix}\na^2 - b^2 \\\\\na + b\n\\end{bmatrix}\\]\n\nUse Gaussian elimination by hand to solve \\[\\begin{bmatrix}\n1 & -2 & -1 &1 \\\\\n2 & 1 & 3 &-1 \\\\\n3 & 3 & 6 & -2\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4\n\\end{bmatrix} =\n\\begin{bmatrix}\n5 \\\\\n-1 \\\\\n-4\n\\end{bmatrix}\\] and then determine if \\(\\begin{bmatrix}\n5 \\\\\n-1 \\\\\n-4\n\\end{bmatrix}\\) can be written in terms of the columns of the matrix.\nUse Gaussian elimination by hand to solve \\[\\begin{bmatrix}\n2 & 3 & 1 \\\\\n4 & 6 & -1 \\\\\n2 & 3 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 \\\\\n1 \\\\\n1\n\\end{bmatrix}\\] and then detrmine if \\(\\begin{bmatrix}\n2 \\\\\n1 \\\\\n1\n\\end{bmatrix}\\) can be written in terms of the columns of the matrix.\nLet \\(T : \\mathbb{R}^2 \\to \\mathbb{R}^2\\) be a linear map that satisfies \\[T\\left(\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\\right) =\n\\begin{bmatrix}\n2 \\\\\n5\n\\end{bmatrix}\\] and \\[T\\left(\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\\right) =\n\\begin{bmatrix}\n-1 \\\\\n2\n\\end{bmatrix}\\]\n\nFind \\(T\\left(\\begin{bmatrix}\n3 \\\\\n-4\n\\end{bmatrix}\\right)\\)\nFind \\(T\\left(\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\right)\\) (i.e., find a formula of \\(T\\))\nFind a matrix \\(A\\) such that \\[T\\left(\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\right) = A\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\]\n\nLet \\(T : \\mathbb{R}^2 \\to \\mathbb{R}^2\\) be a linear map that satisfies \\[T\\left(\\begin{bmatrix}\n2 \\\\\n5\n\\end{bmatrix}\\right) =\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\\] and \\[T\\left(\\begin{bmatrix}\n-1 \\\\\n2\n\\end{bmatrix}\\right) =\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\\]\n\nFind \\(T\\left(\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\\right)\\)\nFind \\(T\\left(\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\\right)\\)\nFind \\(T\\left(\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\right)\\) (i.e., find a formula of \\(T\\))\nFind a matrix \\(A\\) such that \\[T\\left(\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\right) = A\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\]\n\nFind all solutions \\[\\begin{align*}\nx + 2y - z &= 4 \\\\\n2y + 3z &= -2 \\\\\nz &= 1\n\\end{align*}\\]\nFind all solutions \\[\\begin{align*}\n2x - 3y + z + 2w &= 5 \\\\\ny - \\frac{1}{2}z - 4w &= -3 \\\\\nz + 2w &= -1\n\\end{align*}\\]\nFind all solutions \\[\\begin{align*}\nx + 3y - 2z + w &= 2 \\\\\ny - 2w &= -1 \\\\\n\\end{align*}\\]",
    "crumbs": [
      "Linear Transformation",
      "Problems"
    ]
  },
  {
    "objectID": "parts/system_equations.html",
    "href": "parts/system_equations.html",
    "title": "Systems of Linear Equations",
    "section": "",
    "text": "This section covers system of linear equations.\n\nSystems of Linear Equations\nComputational Problems\nConceptual Problems",
    "crumbs": [
      "Systems of Linear Equations"
    ]
  },
  {
    "objectID": "chapters/system_equations.html",
    "href": "chapters/system_equations.html",
    "title": "5  Systems of Linear Equations",
    "section": "",
    "text": "5.1 Equivalent Representations\nSystems of linear equations form the cornerstone of linear algebra. They emerge naturally in mathematics, physics, engineering, and data science whenever we need to describe multiple linear relationships simultaneously.\nA key observation in linear algebra is that we can represent the same system in three different ways. Each representation offers unique advantages. Consider:\n\\[\n\\begin{aligned}\n3x + 4y + z &= 9 \\\\\nx - y + 2z &= 4 \\\\\n5x - 4y + z &= 3\n\\end{aligned}\n\\]",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "chapters/system_equations.html#equivalent-representations",
    "href": "chapters/system_equations.html#equivalent-representations",
    "title": "5  Systems of Linear Equations",
    "section": "",
    "text": "As a Vector Equation\nHere, we view the system as a sum of scaled vectors: \\[\nx\\begin{bmatrix} 3 \\\\ 1 \\\\ 5 \\end{bmatrix} +\ny\\begin{bmatrix} 4 \\\\ -1 \\\\ -4 \\end{bmatrix} +\nz\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix} =\n\\begin{bmatrix} 9 \\\\ 4 \\\\ 3 \\end{bmatrix}\n\\]\n\n\nAs a Matrix Equation\nHere, we package all coefficients into a single matrix: \\[\n\\begin{bmatrix}\n3 & 4 & 1 \\\\\n1 & -1 & 2 \\\\\n5 & -4 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\ y \\\\ z\n\\end{bmatrix} =\n\\begin{bmatrix}\n9 \\\\ 4 \\\\ 3\n\\end{bmatrix}\n\\]\nThe vector equation representation offers a geometric interpretation of linear systems. When we write the system as a vector equation, we’re asking whether a target vector can be obtained as a linear combination of given vectors. This connects systems of equations directly to fundamental concepts of linear algebra: linear combinations and spanning sets. We can visualize the solution process as asking whether the target vector lies in the span of our coefficient vectors, and if so, how we can reach it through scaling and adding these vectors.\nMatrix equations package the coefficients efficiently. They are particularly useful in the square case, where the coefficient matrix is \\(n\\times n\\). When this matrix is invertible, we can solve the system directly by multiplying both sides by the inverse matrix: if \\(Ax = b\\) and \\(A\\) is invertible, then \\(x = A^{-1}b\\). This not only gives us a theoretical way to express solutions but also connects to computational methods for solving systems. Moreover, whether a matrix is invertible tells us important information about the existence and uniqueness of solutions.\nThese three representations—systems of equations, vector equations, and matrix equations—each illuminate different aspects of linear systems. Fluency in moving between these representations and understanding their connections is important for both theoretical understanding and practical problem-solving.",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "chapters/system_equations.html#characterization-of-the-solution-set",
    "href": "chapters/system_equations.html#characterization-of-the-solution-set",
    "title": "5  Systems of Linear Equations",
    "section": "5.2 Characterization of the Solution Set",
    "text": "5.2 Characterization of the Solution Set\nA fundamental result of linear algebra is that every system of linear equations must have exactly one of these three outcomes:\n\nNo solution exists (inconsistent system)\nExactly one solution exists (unique solution)\nInfinitely many solutions exist\n\nLet’s visualize each case in \\(\\mathbb{R}^2\\) using vector equations:\n\nCase 1: No Solution\n\\[\nx\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} +\ny\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} =\n\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n\\]\nThis system has no solution because:\n\nThe vectors \\(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) and \\(\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\\) span only a line\nThe target vector \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) lies off this line\nNo combination of the vectors can reach the target\n\n\n\nCase 2: Unique Solution\n\\[\nx\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} +\ny\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} =\n\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n\\]\nThis system has exactly one solution: \\(\\quad x=1\\) and \\(y=-1\\).\n\n\nCase 3: Infinite Solutions\n\\[\nx\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} +\ny\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} =\n\\begin{bmatrix} 4 \\\\ 8 \\end{bmatrix}\n\\]\nThis system has infinitely many solutions because:\n\nThe vectors are parallel (one is a multiple of the other)\nThe target vector lies on their shared line\nSolutions: \\(x = 4+4t, y = -2t\\) for any real \\(t\\)",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "chapters/system_equations.html#solution-methods",
    "href": "chapters/system_equations.html#solution-methods",
    "title": "5  Systems of Linear Equations",
    "section": "5.3 Solution Methods",
    "text": "5.3 Solution Methods\nIn Algebra 1 (typically 8th or 9th grade), students learn three approaches:\n\nGraphing (limited to \\(\\mathbb{R}^2\\))\nSubstitution\nElimination\n\nLet’s examine the algebraic methods using this system: \\[\n\\begin{aligned}\n2x + 3y &= 2 \\\\\nx + y &= 7\n\\end{aligned}\n\\]\n\nSubstitution Method\n\nChoose a variable and solve for it: \\[\nx = 7 - y \\quad \\text{(from second equation)}\n\\]\nSubstitute into the other equation: \\[\n\\begin{aligned}\n2(7-y) + 3y &= 2 \\\\\n14 - 2y + 3y &= 2 \\\\\n14 + y &= 2 \\\\\ny &= -12\n\\end{aligned}\n\\]\nBack-substitute: \\[\nx = 7 - (-12) = 19\n\\]\n\n\n\nElimination Method\n\nSet up the aligned system: \\[\n\\begin{aligned}\n2x + 3y &= 2 \\\\\nx + y &= 7\n\\end{aligned}\n\\]\nMultiply second equation by -2: \\[\n\\begin{aligned}\n2x + 3y &= 2 \\\\\n-2x - 2y &= -14\n\\end{aligned}\n\\]\nAdd equations: \\[\n\\begin{aligned}\n2x + 3y &= 2 \\\\\n-2x - 2y &= -14 \\\\\n\\hline\ny &= -12\n\\end{aligned}\n\\]\nBack-substitute: \\[\n\\begin{aligned}\n2x + 3(-12) &= 2 \\\\\n2x - 36 &= 2 \\\\\n2x &= 38 \\\\\nx &= 19\n\\end{aligned}\n\\]\n\n\nWhile both methods work, elimination systematically transforms the system into an “upper triangular” form, making solutions easier to find. Consider this upper triangular system:\n\n\\[\n\\begin{aligned}\nx_1 + 2x_2 + x_3 - x_4 &= 4 \\\\\nx_3 + x_4 &= 1 \\\\\nx_4 &= -2\n\\end{aligned}\n\\]\nFrom this form, we can easily find all solutions:\n\n\\(x_4 = -2\\)\nSubstituting \\(x_4\\) in the second equation, we get \\(x_3 = 3\\)\n\\(x_2\\) is free (can be any real number)\nSusbtituting \\(x_4\\) and \\(x_3\\) in the first equation, we get \\(x_1 = -2x_2 + 3\\)\n\nThe general solution is: \\[\n\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{bmatrix}\n=\\begin{bmatrix}-2x_2+3\\\\x_2\\\\3\\\\-2\\end{bmatrix}\n=\\begin{bmatrix}3\\\\0\\\\3\\\\-2\\end{bmatrix}+\nx_2\\begin{bmatrix}-2\\\\1\\\\0\\\\0\\end{bmatrix}\n\\]\nThis represents all solutions, with \\(x_2\\) as the free parameter.",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "chapters/system_equations.html#gauss-elimination",
    "href": "chapters/system_equations.html#gauss-elimination",
    "title": "5  Systems of Linear Equations",
    "section": "5.4 Gauss Elimination",
    "text": "5.4 Gauss Elimination\nThe elimination method discussed previously naturally extends to a systematic algorithm known as Gaussian Elimination. This algorithm formalizes and generalizes the process of using row operations to transform a system of equations into “upper triangular” form, also called echelon form. The key insight is that through a careful sequence of row operations (adding multiples of one equation to another, swapping equations, or multiplying an equation by a nonzero scalar), we can systematically create zeros below the diagonal, one column at a time.\nWhen a system is in echelon form, the pattern of coefficients forms a “staircase” shape, making it significantly easier to find solutions through back-substitution. This systematic approach works for systems of any size, making it a fundamental tool in linear algebra. The detailed algorithm and its implementation are presented in the accompanying slide presentation.\n\nWhy does Gaussian Elimination work?\nThe mathematical foundation of Gaussian elimination lies in matrix multiplication: each row operation is equivalent to left multiplication by an invertible matrix. The following theorem shows why these operations preserve the solution set of the original system.\n\nTheorem 5.1 Let \\(A\\) be an \\(m\\times n\\) matrix, \\(\\mathbf{b}\\in\\mathbb{R}^m\\) and \\(E\\) be an invertible \\(m\\times m\\) matrix. Then the following conditions are equivalent for a vector \\(\\mathbf{x}_0\\in\\mathbb{R}^n\\):\n\n\\(\\mathbf{x}_0\\) is a solution of the system \\(A\\mathbf{x}=\\mathbf{b}\\)\n\\(\\mathbf{x}_0\\) is a solution of the system \\((EA)\\mathbf{x}=E\\mathbf{b}\\)\n\n\n\nProof. The equivalence follows because \\(E\\) is invertible:\n\nIf \\(A\\mathbf{x}_0=\\mathbf{b}\\), then multiplying both sides by \\(E\\) gives \\((EA)\\mathbf{x}_0=E\\mathbf{b}\\)\nConversely, if \\((EA)\\mathbf{x}_0=E\\mathbf{b}\\), then multiplying both sides by \\(E^{-1}\\) gives \\(A\\mathbf{x}_0=\\mathbf{b}\\). \\(\\square\\)\n\n\nEach elementary row operation corresponds to multiplication by a specific type of invertible matrix:\n\nRow swap \\((R_i \\leftrightarrow R_j)\\): Multiply by a permutation matrix that differs from the identity matrix only in rows \\(i\\) and \\(j\\), where the 1’s are swapped. The inverse is itself since swapping twice returns to the original\nRow scaling \\((cR_i)\\): Multiply by diagonal matrix with \\(c\\) in position \\(i\\), 1’s elsewhere. The inverse multiplies row \\(i\\) by \\(\\frac{1}{c}\\)\nRow addition \\((R_i + cR_j)\\): Multiply by the matrix that differs from the identity only in position \\((i,j)\\) where there is a \\(c\\). This adds \\(c\\) times row \\(j\\) to row \\(i\\). The inverse has \\(-c\\) in position \\((i,j)\\), which subtracts \\(c\\) times row \\(j\\) from row \\(i\\)\n\nTherefore, if we perform a sequence of row operations to transform \\(A\\mathbf{x}=\\mathbf{b}\\) into row echelon form, we’re effectively multiplying both sides by a product of invertible matrices. This preserves the solution set while making the system easier to solve.\nThis also explains why we use the augmented matrices. Notice that \\([A|\\mathbf{b}]\\) has \\(m\\) rows and \\(n+1\\) columns, then it follows from Equation 3.4 that \\[E[A|\\mathbf{b}]=[EA|E\\mathbf{b}].\\] So row operations on the augmented matrix simultaneously transform both \\(A\\) and \\(\\mathbf{b}\\) while preserving their relationship.",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "chapters/system_equations.html#homogeneous-systems",
    "href": "chapters/system_equations.html#homogeneous-systems",
    "title": "5  Systems of Linear Equations",
    "section": "5.5 Homogeneous Systems",
    "text": "5.5 Homogeneous Systems\nA linear system \\(A\\mathbf{x}=\\mathbf{b}\\) is homogeneous if \\(\\mathbf{b}=\\mathbf{0}\\). That is, a homogeneous system has the form: \\[A\\mathbf{x}=\\mathbf{0}\\] The zero vector \\(\\mathbf{x}=\\mathbf{0}\\) is always a solution to a homogeneous system. This leads to a fundamental property: a homogeneous system has either:\n\nExactly one solution: \\(\\mathbf{x}=\\mathbf{0}\\)\nInfinitely many solutions\n\nThis dichotomy follows because if a homogeneous system has any nonzero solution \\(\\mathbf{v}\\), then \\(c\\mathbf{v}\\) is also a solution for any scalar \\(c\\), yielding infinitely many solutions.\n\nSolving Homogeneous Systems\nTo solve a homogeneous system, we reduce \\(A\\) to row echelon form and analyze the pivots.\nIn row echelon form, a free column is any column that does not contain a pivot. When solving the system, variables corresponding to free columns can take any value, while the other variables are determined by these choices. Each free column thus generates a dimension of solutions.\nConsider the system: \\[\\begin{bmatrix}\n1 & 2 & 1 & 3\\\\\n2 & 4 & 0 & 2\\\\\n3 & 6 & -1 & 1\n\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}\\]\nRow reduction gives: \\[\\begin{bmatrix}\n1 & 2 & 0 & 1\\\\\n0 & 0 & 1 & 2\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}\\]\nNotice that columns 2 and 4 are free because they have no pivots. Using back substitution, we that the solution of the reduced system is:\n\n\\(x_4\\) is free\n\\(x_3=-2x_4\\),\n\\(x_2\\) is free, and\n\\(x_1=-2x_2-x_4\\).\n\nWriting all values in terms of \\(x_2\\) and \\(x_4\\) we see that solutions are of the form: \\[\n\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{bmatrix}\n=\\begin{bmatrix}-2x_2-x_4\\\\x_2\\\\-2x_4\\\\x_4\\end{bmatrix} =x_2\\begin{bmatrix}-2\\\\ 1\\\\ 0\\\\ 0\\end{bmatrix} + x_4\\begin{bmatrix}-1\\\\ 0\\\\ -2\\\\ 1\\end{bmatrix}\\]\nSince \\(x_2,x_4\\) can be any real numbers, the system has infinitely many solutions.\n\n\nConnection Between Solutions and Column Dependencies\nSolutions to homogeneous systems reveal linear dependencies among the columns of the coefficient matrix. Consider:\n\\[A=\\begin{bmatrix}\n1 & 2 & 1 & 3\\\\\n2 & 4 & 0 & 2\\\\\n3 & 6 & -1 & 1\n\\end{bmatrix} =\\begin{bmatrix}\n\\uparrow & \\uparrow & \\uparrow & \\uparrow \\\\\n\\mathbf{a}_1 & \\mathbf{a}_2 & \\mathbf{a}_3 & \\mathbf{a}_4 \\\\\n\\downarrow & \\downarrow & \\downarrow & \\downarrow\n\\end{bmatrix}\\]\nwhere \\(\\mathbf{a}_1,\\mathbf{a}_2,\\mathbf{a}_3,\\mathbf{a}_4\\) are the columns of \\(A\\).\nWe just saw that the general solution to \\(A\\mathbf{x}=\\mathbf{0}\\) is: \\[ \\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{bmatrix} =\nx_2\\begin{bmatrix}-2\\\\ 1\\\\ 0\\\\ 0\\end{bmatrix} +\nx_4\\begin{bmatrix}-1\\\\ 0\\\\ -2\\\\ 1\\end{bmatrix}\\]\nEach basic solution reveals a column dependency (Equation 3.1):\n\nSetting \\(x_2=1, x_4=0\\) gives solution \\((-2,1,0,0)\\in\\mathbb{R}^4\\). This means: \\[-2\\mathbf{a}_1 + \\mathbf{a}_2 = \\mathbf{0}\\quad\\text{which implies}\\quad \\mathbf{a}_2 = 2\\mathbf{a}_1.\\]\nSetting \\(x_2=0, x_4=1\\) gives solution \\((-1,0,-2,1)\\in\\mathbb{R}^4\\). This means: \\[-\\mathbf{a}_1 - 2\\mathbf{a}_3 + \\mathbf{a}_4 = \\mathbf{0}\\quad\\text{which implies}\\quad \\mathbf{a}_4 = \\mathbf{a}_1 + 2\\mathbf{a}_3.\\]\n\nThese relationships, which can be easily verified by replacing the values of the columns, show that free columns (\\(\\mathbf{a}_2\\) and \\(\\mathbf{a}_4\\)) can be expressed as linear combinations of pivot columns (\\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_3\\)).\n\n\nHomogeneous Systems: Dependence and Independence\nThe previous example illustrates important principles that we expand in this section. Let \\(A\\) be an \\(m\\times n\\) matrix with columns \\(\\mathbf{a}_1,\\mathbf{a}_2,\\dots,\\mathbf{a}_n\\in\\mathbb{R}^m\\). Three key ideas connect solutions and column dependencies:\n\nIf \\(\\mathbf{x}\\neq\\mathbf{0}\\) solves \\(A\\mathbf{x}=\\mathbf{0}\\), then \\(t\\mathbf{x}\\) is also a solution for any \\(t\\in\\mathbb{R}\\), giving infinitely many solutions.\nFrom Equation 3.1, \\(\\mathbf{x}=(x_1,\\dots,x_n)\\in\\mathbb{R}^n\\) solves \\(A\\mathbf{x}=\\mathbf{0}\\) if and only if\n\\[x_1\\mathbf{a}_1+x_2\\mathbf{a}_2+\\cdots+x_n\\mathbf{a}_n=\\mathbf{0}\\]\nWhen any \\(x_i\\neq 0\\), we can solve for \\(\\mathbf{a}_i\\), expressing \\(\\mathbf{a}_i\\) as a linear combination of the other columns: \\[\\mathbf{a}_i = -\\frac{x_1}{x_i}\\mathbf{a}_1-\\cdots-\\frac{x_{i-1}}{x_i}\\mathbf{a}_{i-1}-\\frac{x_{i+1}}{x_i}\\mathbf{a}_{i+1}-\\cdots-\\frac{x_n}{x_i}\\mathbf{a}_n.\\] Conversely, if we can write \\(\\mathbf{a}_i\\) in terms of the other columns, there exist constants \\(c_1,\\dots,c_{i-1},c_{i+1},\\dots,c_n\\) such that \\[\\mathbf{a}_i=c_1\\mathbf{a}_1+\\cdots+c_{i-1}\\mathbf{a}_{i-1}+c_{i+1}\\mathbf{a}_{i+1}+\\cdots+c_n\\mathbf{a}_n.\\] Then subtracting \\(\\mathbf{a}_i\\) from both sides, we obtain \\[\\mathbf{0}=c_1\\mathbf{a}_1+\\cdots+c_{i-1}\\mathbf{a}_{i-1}+1\\mathbf{a}_i+c_{i+1}\\mathbf{a}_{i+1}+\\cdots+c_n\\mathbf{a}_n,\\] and we obtain a non-zero solution of the homogeneous system because the coefficient of \\(\\mathbf{a}_i\\) is 1.\n\nThe following two theorems capture these important relations:\n\nTheorem 5.2 Suppose that \\(A\\) is an \\(m\\times n\\) matrix. The following are equivalent:\n\nThe homogeneous system \\(A\\mathbf{x}=\\mathbf{0}\\) has infintely many solutions\nThe homogeneous system \\(A\\mathbf{x}=\\mathbf{0}\\) has a non-zero solution\nWe can write a column of \\(A\\) as a linear combination of the other columns\nThe row reduced Echelon form of \\(A\\) has a free column\n\n\n\nTheorem 5.3 Suppose that \\(A\\) is an \\(m\\times n\\) matrix with columns \\(\\mathbf{a}_1,\\mathbf{a}_2\\dots,\\mathbf{a}_n\\in\\mathbb{R}^m\\). The following are equivalent:\n\nThe homogeneous system \\(A\\mathbf{x}=\\mathbf{0}\\) has a unique solution\nLet \\(\\mathbf{x}\\in\\mathbb{R}^n\\). If \\(A\\mathbf{x}=\\mathbf{0}\\), then \\(\\mathbf{x}=\\mathbf{0}\\)\nLet \\(c_1,\\dots,c_n\\in\\mathbb{R}\\). If \\(c_1\\mathbf{a}_1+c_2\\mathbf{a}_2+\\cdots+c_n\\mathbf{a}_n=\\mathbf{0}\\), then \\(c_1=c_2=\\cdots=c_n=0.\\)\nWe cannot write any column of \\(A\\) as a linear combination of the other columns\nThe row reduced Echelon form of \\(A\\) doesn’t have any free columns\n\n\n\nCondition 3 of Theorem 5.3 motivates the following definition\n\nDefinition 5.1 Suppose that \\(V\\) is a vector space and that \\(\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\in V\\). The set \\(S=\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\}\\) is linearly independent if for every \\(c_1,\\dots,c_k\\in\\mathbb{R}\\), if \\(c_1\\mathbf{v}_1+c_2\\mathbf{v}_2+\\cdots+c_k\\mathbf{v}_k=\\mathbf{0}\\), then \\(c_1=c_2=\\cdots=c_k=0.\\)\n\n\nNotice that \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\}\\) is linearly independent if and only if we cannot write any of these vectors as a linear combination of the other ones. However, checking this requires solving \\(k\\) vector equations.\nFrom a computational point of view, the statement in the definition is much more practical. We only solve one vector equation. We look at: \\[ c_1\\mathbf{v}_1+c_2\\mathbf{v}_2+\\cdots+c_k\\mathbf{v}_k=\\mathbf{0}\\] and we solve it. If the only solution is \\(c_1=c_2=\\cdots=c_k=0\\), the set is linearly independent.\n\nIf \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\}\\) is not linearly dependent, then it is linearly dependent. In this case we can write a vector as a linear combination of the others.",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "chapters/system_equations.html#linear-systems-and-the-span-of-columns",
    "href": "chapters/system_equations.html#linear-systems-and-the-span-of-columns",
    "title": "5  Systems of Linear Equations",
    "section": "5.6 Linear Systems and the Span of Columns",
    "text": "5.6 Linear Systems and the Span of Columns\nLet \\(A\\) be an \\(m\\times n\\) matrix with columns \\(\\mathbf{a}_1,\\mathbf{a}_2,\\dots,\\mathbf{a}_n\\in\\mathbb{R}^m\\). In this section we address three questions:\n\nGiven \\(\\mathbf{b}\\in\\mathbb{R}^m\\), when does \\(A\\mathbf{x}=\\mathbf{b}\\) have at least one solution?\nIf \\(A\\mathbf{x}=\\mathbf{b}\\) has a solution, when is that solution unique?\nUnder what conditions does \\(A\\mathbf{x}=\\mathbf{b}\\) have solutions for every \\(\\mathbf{b}\\in\\mathbb{R}^m\\)\n\nLet’s start with existence. Suppose that \\(A\\mathbf{x}=\\mathbf{b}\\) . By Equation 3.1: \\[A\\mathbf{x} = x_1\\mathbf{a}_1 + x_2\\mathbf{a}_2 + \\cdots + x_n\\mathbf{a}_n = \\mathbf{b}\\]\nTherefore, \\(\\mathbf{b}\\) must be a linear combination of the columns of \\(A\\). The set of all such linear combinations is called the span of the columns (see Section 1.3). Then we have\n\nTheorem 5.4 Let \\(A\\) be an \\(m\\times n\\) matrix and \\(\\mathbf{b}\\in\\mathbb{R}^m\\). Then the following conditions are equivalent:\n\n\\(A\\mathbf{x}=\\mathbf{b}\\) has a solution\n\\(\\mathbf{b}\\) can be written as a linear combination of the columns of \\(A\\)\n\n\nLet’s examine some examples. Consider: \\[A=\\begin{bmatrix}1&2&1&3\\\\2&4&0&2\\\\3&6&-1&1\\end{bmatrix},\\quad\\mathbf{b}_1=\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix},\\quad\\mathbf{b}_2=\\begin{bmatrix}1\\\\1\\\\0\\end{bmatrix}\\]\nFirst, let’s solve \\(A\\mathbf{x}=\\mathbf{b}_1\\). The row reduced echelon form of \\([A|\\mathbf{b}_1]\\) is: \\[\n\\left[\\begin{array}{cccc|c}1&2&1&3&1\\\\2&4&0&2&1\\\\3&6&-1&1&1\\end{array}\\right] \\xrightarrow{\\text{RREF}}\n\\left[\\begin{array}{cccc|c}1&2&0&1&\\frac{1}{2}\\\\0&0&1&2&\\frac{1}{2}\\\\0&0&0&0&0\\end{array}\\right]\n\\]\nThis system has solutions. Using back substitution:\n\n\\(x_4\\) is free\n\\(x_3=\\frac{1}{2}-2x_4\\)\n\\(x_2\\) is free\n\\(x_1=\\frac{1}{2}-2x_2-x_4\\)\n\nThe general solution is: \\[\n\\begin{bmatrix}x_1\\\\ x_2\\\\ x_3\\\\ x_4\\end{bmatrix} =\n\\begin{bmatrix}\\frac{1}{2}\\\\ 0\\\\ \\frac{1}{2}\\\\ 0\\end{bmatrix} +x_2 \\begin{bmatrix}-2\\\\ 1\\\\ 0\\\\ 0\\end{bmatrix}\n+x_4 \\begin{bmatrix}-1\\\\ 0\\\\ -2\\\\ 1\\end{bmatrix}\n\\] Since \\(x_2\\) and \\(x_4\\) can take arbitrary elements, there are infinitely many solutions. When \\(x_2=x_4=0\\), we get \\((\\frac{1}{2},0,\\frac{1}{2},0)\\), showing that: \\[\\mathbf{b}_1=\\frac{1}{2}\\mathbf{a}_1+\\frac{1}{2}\\mathbf{a}_3,\\] which we can verify by replacing the values of the columns of \\(A\\) in the equation.\nThis example illustrates the following\n\nTheorem 5.5 Let \\(A\\) be an \\(m\\times n\\) matrix, \\(\\mathbf{b}\\in\\mathbb{R}^m\\), and suppose that \\(A\\mathbf{x}=\\mathbf{b}\\) has at least one solution. Then the following conditions are equivalent:\n\n\\(A\\mathbf{x}=\\mathbf{b}\\) has a unique solution\nThe matrix \\(R\\), the reduced row echelon form of \\(A\\), has a pivot position in every column.\nThe matrix \\(R\\), the reduced row echelon form of \\(A\\), does not have a free column.\n\n\nNow consider \\(A\\mathbf{x}=\\mathbf{b}_2\\). The row reduced echelon form of \\([A|\\mathbf{b}_2]\\) is: \\[\n\\left[\\begin{array}{cccc|c}1&2&1&3&1\\\\2&4&0&2&1\\\\3&6&-1&1&0\\end{array}\\right] \\xrightarrow{\\text{RREF}}\n\\left[\\begin{array}{cccc|c}1&2&0&1&0\\\\0&0&1&2&0\\\\0&0&0&0&1\\end{array}\\right]\n\\]\nThis system has no solution since the last equation becomes: \\[0x_1+0x_2+0x_3+0x_4=1\\]\nFor the general case, let \\(\\mathbf{b}_3=\\begin{bmatrix}a\\\\b\\\\c\\end{bmatrix}\\) be arbitrary in \\(\\mathbb{R}^3\\). We can find solutions by reducing the augmented matrix \\([A|\\mathbf{b}_3]\\) to echelon form. We can do this in SymPy using the echelon_form() method that finds the row echelon form and not the row reduced echelon form.\n\nfrom sympy import Matrix, symbols\n\na,b,c = symbols(\"a b c\")\nA = Matrix([[1,2,1,3],[2,4,0,2],[3,6,-1,1]])\nB = Matrix.hstack(A,Matrix([a,b,c]))    # finds the augmented matrix\nB.echelon_form()    # finds the echelon form\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 2 & 1 & 3 & a\\\\0 & 0 & -2 & -4 & - 2 a + b\\\\0 & 0 & 0 & 0 & - 2 a + 4 b - 2 c\\end{matrix}\\right]\\)\n\n\nThe last equation is: \\[0x_1+0x_2+0x_3+0x_4=-2a+4b-2c.\\] Then we conclude that \\(A\\mathbf{x}=\\mathbf{b}_3\\) has a solution if and only if \\(-2a+4b-2c=0\\), which explains why we have a solution for \\(\\mathbf{b}_1=(1,1,1)\\) but not for \\(\\mathbf{b}_2=(1,1,0)\\).\nThe last two examples illustrate the following\n\nTheorem 5.6 Let \\(A\\) be an \\(m\\times n\\) matrix and let \\(R\\) be the reduced row echelon form of \\(A\\). The following conditions are equivalent:\n\n\\(A\\mathbf{x}=\\mathbf{b}\\) has a solution for every \\(\\mathbf{b}\\in\\mathbb{R}^m\\).\n\\(R\\) has a pivot position in every row.\n\\(R\\) does not have a zero row.\nEvery \\(\\mathbf{b}\\in\\mathbb{R}^m\\) can be written as a linear combination of the columns of \\(A\\)\n\n\nThe last condition motivates the following definition\n\nDefinition 5.2 Suppose that \\(V\\) is a vector space and that \\(\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\in V\\). The set \\(S=\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\}\\) spans \\(V\\) if for every \\(\\mathbf{v}\\in V\\), there exist \\(c_1,\\dots,c_k\\in\\mathbb{R}\\) such that \\[c_1\\mathbf{v}_1+c_2\\mathbf{v}_2+\\cdots+c_k\\mathbf{v}_k=\\mathbf{v}\\]",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "chapters/system_equations.html#finding-inverses",
    "href": "chapters/system_equations.html#finding-inverses",
    "title": "5  Systems of Linear Equations",
    "section": "5.7 Finding Inverses",
    "text": "5.7 Finding Inverses\nLet \\(A\\) be an \\(n\\times n\\) matrix. Recall that \\(A\\) is invertible if there exists an \\(n\\times n\\) matrix \\(A^{-1}\\) such that: \\[AA^{-1}=I_n\\quad\\text{and}\\quad A^{-1}A=I_n\\] where \\(I_n\\) is the \\(n\\times n\\) identity matrix. If \\(A\\) and \\(B\\) are invertible \\(n\\times n\\) matrices. Then \\(AB\\) is also invertible and the inverse is the product of their inverses in reverse order: \\[(AB)^{-1} = B^{-1}A^{-1} \\tag{5.1}\\] We can easily verify this: \\[(AB)(B^{-1}A^{-1}) = A(BB^{-1})A^{-1} = AI_nA^{-1} = AA^{-1} = I_n\\] And similarly for \\((B^{-1}A^{-1})(AB)\\).\nSuppose now that \\(A\\) is an invertible \\(n\\times n\\) matrix. Then for any \\(\\mathbf{b}\\in\\mathbb{R}^n\\), the system \\(A\\mathbf{x}=\\mathbf{b}\\) has unique solution: \\[\\mathbf{x}=A^{-1}\\mathbf{b}.\\] From Theorem 5.6, the row reduced echelon form \\(R\\) of an invertible matrix \\(A\\) must have:\n\nA pivot in every row\na pivot in every column, because \\(R\\) has an equal number of rows and columns\nNo free columns\n\nThis leads to a key result:\n\nTheorem: If \\(A\\) is an invertible \\(n\\times n\\) matrix, its row reduced echelon form \\(R\\) is the identity matrix \\(I_n\\).\n\nThis gives us an algorithm to find \\(A^{-1}\\):\n\nForm the augmented matrix \\([A|I_n]\\)\nRow reduce to get \\([I_n|B]\\) for some matrix \\(B\\)\nThen \\(B = A^{-1}\\)\n\nWe’ll explore this algorithm in detail during the lab.\n\n5.7.1 Writing Invertible Matrices as Products of Elementary Matrices\nLet \\(A\\) be an invertible matrix. Then the row reduced echelon form of \\(A\\) is the identity. Each step of the row reduction process corresponds to left multiplication by an elementary matrix. Recall that elementary matrices are invertible and their inverses are also elementary matrices. Therefore, we can find elementary matrices \\(E_1,\\ldots,E_k\\) such that: \\[E_k\\cdots E_2E_1A = I_n\\] Solving for \\(A\\) we get that \\[A=E_k^{-1}E_{k-1}^{-1}\\cdots E_2^{-1}E_1^{-1}.\\]\nTo illustrate, consider the case where \\(E_4E_3E_2E_1A = I_n\\). We can solve for \\(A\\):\n\nStart with \\(E_4E_3E_2E_1A = I_n\\)\nMultiply from the left by \\(E_4^{-1}\\):\n\n\\(E_3E_2E_1A = E_4^{-1}\\)\n\nMultiply from the left by \\(E_3^{-1}\\):\n\n\\(E_2E_1A = E_3^{-1}E_4^{-1}\\)\n\nMultiply from the left by \\(E_2^{-1}\\):\n\n\\(E_1A = E_2^{-1}E_3^{-1}E_4^{-1}\\)\n\nFinally multiply from the left by \\(E_1^{-1}\\):\n\n\\(A = E_1^{-1}E_2^{-1}E_3^{-1}E_4^{-1}\\)",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "worksheets/gaussianElimination.html",
    "href": "worksheets/gaussianElimination.html",
    "title": "Computational Problems",
    "section": "",
    "text": "Gaussian Elimination",
    "crumbs": [
      "Systems of Linear Equations",
      "Computational Problems"
    ]
  },
  {
    "objectID": "worksheets/gaussianElimination.html#gaussian-elimination",
    "href": "worksheets/gaussianElimination.html#gaussian-elimination",
    "title": "Computational Problems",
    "section": "",
    "text": "Problem 1:\nFind the reduced row echelon form of the following matrices by hand:\n\n\\(\\begin{bmatrix}\n-1 & 1  \\\\\n-1 & 0  \\\\\n0 & -1  \\\\\n-1 & 2\n\\end{bmatrix}\\)\n\\(\\begin{bmatrix}\n1 & 2 & 0 \\\\\n1 & 3 & 3 \\\\\n-1 & 0 & -1 \\\\\n-3 & 0 & 0\n\\end{bmatrix}\\)\n\\(\\begin{bmatrix}\n0 & -3 & 2 & -2 \\\\\n0 & 2 & 2 & -2\n\\end{bmatrix}\\)",
    "crumbs": [
      "Systems of Linear Equations",
      "Computational Problems"
    ]
  },
  {
    "objectID": "worksheets/gaussianElimination.html#problem-2",
    "href": "worksheets/gaussianElimination.html#problem-2",
    "title": "Computational Problems",
    "section": "Problem 2:",
    "text": "Problem 2:\nSolve the following system of equations by hand:\n\\[\n\\begin{aligned}\nx_2 + 5x_3 &= -4 \\\\\nx_1 + 4x_2 + 3x_3 &= -2 \\\\\n2x_1 + 7x_2 + x_3 &= -2\n\\end{aligned}\n\\]",
    "crumbs": [
      "Systems of Linear Equations",
      "Computational Problems"
    ]
  },
  {
    "objectID": "worksheets/gaussianElimination.html#problem-3",
    "href": "worksheets/gaussianElimination.html#problem-3",
    "title": "Computational Problems",
    "section": "Problem 3:",
    "text": "Problem 3:\nSolve the following system of equations. Use SymPy to find the rref.\n\\[\n\\begin{aligned}\n6x_3 + 2x_4 - 4x_5 - 8x_6 &= 8 \\\\\n3x_3 + x_4 - 2x_5 - 4x_6 &= 4 \\\\\n2x_1 - 3x_2 + x_3 + 4x_4 - 7x_5 + x_6 &= 2 \\\\\n6x_1 - 9x_2 + 11x_4 - 19x_5 + 3x_6 &= 1\n\\end{aligned}\n\\]",
    "crumbs": [
      "Systems of Linear Equations",
      "Computational Problems"
    ]
  },
  {
    "objectID": "worksheets/alwaysSometimesNever.html",
    "href": "worksheets/alwaysSometimesNever.html",
    "title": "Conceptual Problems",
    "section": "",
    "text": "Understanding the relation between solutions of system of linear equations, linear independence, and spanning\nFor each the following statements, answer: “always”, “sometimes”, or “never”, and provide a clear explanation that justifies your answer.",
    "crumbs": [
      "Systems of Linear Equations",
      "Conceptual Problems"
    ]
  },
  {
    "objectID": "worksheets/alwaysSometimesNever.html#understanding-the-relation-between-solutions-of-system-of-linear-equations-linear-independence-and-spanning",
    "href": "worksheets/alwaysSometimesNever.html#understanding-the-relation-between-solutions-of-system-of-linear-equations-linear-independence-and-spanning",
    "title": "Conceptual Problems",
    "section": "",
    "text": "Suppose that \\(A\\) is a \\(3\\times4\\) matrix.\n\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a unique solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has infinitely many solutions.\n\nSuppose that \\(A\\) is a \\(5\\times4\\) matrix.\n\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a unique solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has infinitely many solutions.\n\nLet \\(A\\) be a \\(3\\times 4\\) matrix, with columns \\(A=\\begin{bmatrix}\\boldsymbol{c}_1&\\boldsymbol{c}_2&\\boldsymbol{c}_3&\\boldsymbol{c}_4\\end{bmatrix}\\) and with rows represented by the columns of the transpose \\(A^T=\\begin{bmatrix}\\boldsymbol{r}_1&\\boldsymbol{r}_2&\\boldsymbol{r}_3\\end{bmatrix}\\)\n\nThe columns of \\(A\\), \\(\\{\\boldsymbol{c}_1,\\boldsymbol{c}_2,\\boldsymbol{c}_3,\\boldsymbol{c}_4\\}\\), are linearly independent.\nThe columns of \\(A\\), \\(\\{\\boldsymbol{c}_1,\\boldsymbol{c}_2,\\boldsymbol{c}_3,\\boldsymbol{c}_4\\}\\), span \\(\\mathbb{R}^3\\)\nThe columns of \\(A^T\\), \\(\\{\\boldsymbol{r}_1,\\boldsymbol{r}_2,\\boldsymbol{r}_3\\}\\), are linearly independent.\nThe columns of \\(A^T\\), \\(\\{\\boldsymbol{r}_1,\\boldsymbol{r}_2,\\boldsymbol{r}_3\\}\\), span \\(\\mathbb{R}^4\\)\n\nLet \\(\\boldsymbol{b}\\in \\mathbb{R}^4\\) and let \\(A\\) be a \\(4\\times 4\\) matrix, with columns \\(A=\\begin{bmatrix}\\boldsymbol{c}_1& \\boldsymbol{c}_2&\\boldsymbol{c}_3&\\boldsymbol{c}_4\\end{bmatrix}\\). Suppose that \\(\\{\\boldsymbol{c}_1,\\boldsymbol{c}_2,\\boldsymbol{c}_3,\\boldsymbol{c}_4\\}\\) is linearly independent.\n\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a unique solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has infinitely many solutions.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a unique solution (assume there is at least one solution).\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has infinitely many solution (assume there is at least one solution).\nThe columns \\(\\{\\boldsymbol{c}_1,\\boldsymbol{c}_2,\\boldsymbol{c}_3,\\boldsymbol{c}_4\\}\\) span \\(\\mathbb{R}^4\\).\n\nLet \\(\\boldsymbol{b}\\in \\mathbb{R}^4\\) and let \\(A\\) be a \\(4\\times 4\\) matrix, with columns \\(A=\\begin{bmatrix}\\boldsymbol{c}_1& \\boldsymbol{c}_2&\\boldsymbol{c}_3&\\boldsymbol{c}_4\\end{bmatrix}\\). Suppose that \\(\\{\\boldsymbol{c}_1,\\boldsymbol{c}_2,\\boldsymbol{c}_3,\\boldsymbol{c}_4\\}\\) spans \\(\\mathbb{R}^4\\).\n\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a unique solution (assume there is at least one solution).\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has infinitely many solutions (assume there is at least one solution).\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a unique solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has infinitely many solutions.\nThe columns \\(\\{\\boldsymbol{c}_1,\\boldsymbol{c}_2,\\boldsymbol{c}_3,\\boldsymbol{c}_4\\}\\) are linearly independence.\n\nLet \\(\\boldsymbol{b}\\in \\mathbb{R}^5\\) and let \\(A\\) be a \\(5\\times 4\\) matrix with columns \\(A=\\begin{bmatrix}\\boldsymbol{c}_1& \\boldsymbol{c}_2&\\boldsymbol{c}_3&\\boldsymbol{c}_4\\end{bmatrix}\\). Suppose that \\(\\{\\boldsymbol{c}_1,\\boldsymbol{c}_2,\\boldsymbol{c}_3,\\boldsymbol{c}_4\\}\\) is linearly independent.\n\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a unique solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has infinitely many solutions.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a unique solution (assume there is at least one solution).\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has infinitely many solution (assume there is at least one solution).\n\nLet \\(\\boldsymbol{b}\\in \\mathbb{R}^4\\) and let \\(A\\) be a \\(4\\times 5\\) matrix with columns \\(A=\\begin{bmatrix}\\boldsymbol{c}_1& \\boldsymbol{c}_2&\\boldsymbol{c}_3&\\boldsymbol{c}_4&\\boldsymbol{c}_5\\end{bmatrix}\\) and suppose that the row reduced matrix of \\(A\\) has exactly one free variable.\n\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a unique solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has infinitely many solutions.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a unique solution (assume there is at least one solution).\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has infinitely many solution (assume there is at least one solution).",
    "crumbs": [
      "Systems of Linear Equations",
      "Conceptual Problems"
    ]
  },
  {
    "objectID": "parts/subspaces_bases.html",
    "href": "parts/subspaces_bases.html",
    "title": "Subspaces and Bases",
    "section": "",
    "text": "This section covers subspaces and bases\n\nBases\nSubspaces of \\(\\mathbb{R}^n\\)\nProblems",
    "crumbs": [
      "Subspaces and Bases"
    ]
  },
  {
    "objectID": "chapters/bases.html",
    "href": "chapters/bases.html",
    "title": "6  Bases of Vector Spaces",
    "section": "",
    "text": "6.1 Definition and Properties\nBases are fundamental structures in linear algebra that provide a systematic way to represent vectors in a vector space. They act as coordinate systems, allowing us to uniquely express every vector as a combination of a minimal set of basis vectors. A deep understanding of bases is essential for:\nHere are three fundamental definitions of the theory of vector spaces.",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bases of Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/bases.html#definition-and-properties",
    "href": "chapters/bases.html#definition-and-properties",
    "title": "6  Bases of Vector Spaces",
    "section": "",
    "text": "Definition 6.1 Let \\(V\\) be a vector space. A set of vectors \\(S = \\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_k\\}\\) in \\(V\\) is linearly independent if the only solution to \\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k = \\mathbf{0}\\] is the trivial solution \\(c_1 = c_2 = \\cdots = c_k = 0\\).\nEquivalently, no vector in \\(S\\) can be written as a linear combination of the other vectors in \\(S\\).\n\n\nDefinition 6.2 Let \\(V\\) be a vector space. A set of vectors \\(S = \\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_k\\}\\) in \\(V\\) spans \\(V\\) if every vector \\(\\mathbf{v}\\in V\\) can be written as a linear combination of vectors in \\(S\\). That is, for every \\(\\mathbf{v}\\in V\\), there exist constants \\(c_1,\\ldots,c_k\\in\\mathbb{R}\\) such that \\[\\mathbf{v} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k\\]\n\n\nDefinition 6.3 Let \\(V\\) be a vector space. A set of vectors \\(S = \\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_k\\}\\) in \\(V\\) is a basis of \\(V\\) if it satisfies both:\n\n\\(S\\) is linearly independent\n\\(S\\) spans \\(V\\)\n\nIn other words, a basis is a linearly independent spanning set.\n\n\n\n\n\n\n\nThink About This\n\n\n\nA basis gives us a minimal spanning set: it contains just enough vectors to span the space, with no redundant vectors. This is why bases are so useful - they provide an efficient way to represent all vectors in the space.\n\n\n\n6.1.1 Verifying Linear Independence and Spanning\nWhen working with a set of vectors \\(S=\\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_k\\}\\), we often need to verify whether it is linearly independent or whether it spans a space. Here’s how to check each property:\n\nChecking Linear Independence\nTo verify if the set of vectors is linearly independent, we look at the homogeneous equation \\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k = \\mathbf{0}\\] and we find all solutions. If the only solution is the trivial solution \\(c_1=c_2=\\cdots=c_k=0\\), the set is linearly independent. Otherwise it is linearly dependent.\n\nExample in \\(\\mathbb{R}^3\\)\nLet’s check if the following vectors are linearly independent: \\[S=\\left\\{\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\1\\end{bmatrix}, \\begin{bmatrix}1\\\\1\\\\3\\end{bmatrix}\\right\\}\\]\nSet up the equation: \\[c_1\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix} + c_2\\begin{bmatrix}0\\\\1\\\\1\\end{bmatrix} + c_3\\begin{bmatrix}1\\\\1\\\\3\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix},\\] and solve it by converting it to an augmented matrix and then use Gaussian Elimination: \\[\n\\left[\\begin{array}{ccc|c}1&0&1&0\\\\0&1&1&0\\\\1&1&3&0\\end{array}\\right] \\xrightarrow{\\text{RREF}}\n\\left[\\begin{array}{ccc|c}1&0&0&0\\\\0&1&0&0\\\\0&0&1&0\\end{array}\\right]\n\\]\nFrom the row reduced echelon matrix we conclude that the only solution is \\(c_1=c_2=c_3=0\\), which proves that the set is linear independence.\n\n\n\nChecking Spanning\nTo verify if a set of vectors spans the vector space \\(V\\), we look at the vector equation: \\[\\mathbf{b} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k,\\] where \\(\\mathbf{b}\\in V\\) is an arbitrary vector. If this equation has a solution for every \\(\\mathbf{b}\\), the set of vectors spans \\(V\\), otherwise it doesn’t span \\(V\\).\n\nExample in \\(\\mathbb{R}^3\\)\nLet’s check if the following vectors span \\(\\mathbb{R}^3\\): \\[S=\\left\\{\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\1\\end{bmatrix}, \\begin{bmatrix}1\\\\1\\\\3\\end{bmatrix}\\right\\}\\]\nAn arbitrary vector in \\(\\mathbb{R}^3\\) is of the form \\(\\mathbf{b}=(x_1,x_2,x_3)\\).\nSet up the vector equation: \\[c_1\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix} + c_2\\begin{bmatrix}0\\\\1\\\\1\\end{bmatrix} + c_3\\begin{bmatrix}1\\\\1\\\\3\\end{bmatrix}=\\begin{bmatrix}x_1\\\\x_2\\\\x_2\\end{bmatrix},\\] and solve it by converting it to an augmented matrix and then use Gaussian Elimination: \\[\n\\left[\\begin{array}{ccc|c}1&0&1&x_1\\\\0&1&1&x_2\\\\1&1&3&x_3\\end{array}\\right] \\xrightarrow{\\text{RREF}}\n\\left[\\begin{array}{ccc|c}1&0&0&2x_1+x_2-x_3\\\\0&1&0&x_1+2x_2-x_3\\\\0&0&1&-x_1-x_2+x_3\\end{array}\\right]\n\\]\nFrom the row reduced echelon matrix we conlude that the vector equation has a solution for every \\(\\mathbf{b}\\in\\mathbf{R}^3\\), which proves that the set spans \\(\\mathbb{R}^3\\).\n\n\n\n\n\n\nImportant Points\n\n\n\n\nLinear Independence: We set the linear combination equal to zero and check for only the trivial solution\nSpanning: We set the linear combination equal to an arbitrary vector and check if solutions always exist\nLinear independence deals with uniqueness, while spanning deals with existence\n\n\n\n\n\n\n\n6.1.2 Examples of Bases\nThe most fundamental example in any \\(\\mathbb{R}^n\\) is the canonical basis (also called the standard basis) \\[\\{\\mathbf{e}_1,\\mathbf{e}_2,\\ldots,\\mathbf{e}_n\\}\\] where \\(\\mathbf{e}_i\\) has a 1 in position \\(i\\) and 0’s elsewhere: \\[\\mathbf{e}_1=\\begin{bmatrix}1\\\\0\\\\\\vdots\\\\0\\end{bmatrix},\n\\mathbf{e}_2=\\begin{bmatrix}0\\\\1\\\\\\vdots\\\\0\\end{bmatrix}, \\ldots,\n\\mathbf{e}_n=\\begin{bmatrix}0\\\\0\\\\\\vdots\\\\1\\end{bmatrix}.\\]\nHere are several examples of bases for \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\):\n\nExamples of Bases in \\(\\mathbb{R}^2\\):\n\\[S_1=\\left\\{\\begin{bmatrix}1\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\end{bmatrix}\\right\\},\\quad\n   S_2=\\left\\{\\begin{bmatrix}1\\\\1\\end{bmatrix}, \\begin{bmatrix}-1\\\\1\\end{bmatrix}\\right\\}, \\quad\n   S_3=\\left\\{\\begin{bmatrix}2\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\3\\end{bmatrix}\\right\\}.\\]\n\n\nExamples of Bases in \\(\\mathbb{R}^3\\)\n\\[S_1=\\left\\{\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\1\\end{bmatrix}, \\begin{bmatrix}1\\\\1\\\\0\\end{bmatrix}\\right\\}, \\quad\n   S_2 = \\left\\{\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}, \\begin{bmatrix}1\\\\1\\\\0\\end{bmatrix}, \\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\\right\\}\\]\n\n\n\n6.1.3 Uniqueness of Representations\nFor any vector \\(\\mathbf{v} \\in V\\), if \\(S\\) is a basis, then \\(\\mathbf{v}\\) can be written uniquely as: \\[\\mathbf{v} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + ... + c_n\\mathbf{v}_n\\]\nExistence follows from the spanning property. To prove uniqueness, assume we have two representations:\n\n\\(\\mathbf{v} = c_1\\mathbf{v}_1 + ... + c_n\\mathbf{v}_n\\) and\n\\(\\mathbf{v} = d_1\\mathbf{v}_1 + ... + d_n\\mathbf{v}_n\\)\n\nThen, subtracting these equations, we get \\[\\mathbf{0} = (c_1-d_1)\\mathbf{v}_1 + ... + (c_n-d_n)\\mathbf{v}_n\\] Since \\(S\\) is linearly independent, we must have: \\(c_1-d_1 = c_2-d_2 = \\cdots = c_n-d_n = 0\\) and we conclude that \\(c_i = d_i\\) for all \\(i\\)",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bases of Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/bases.html#coordinates",
    "href": "chapters/bases.html#coordinates",
    "title": "6  Bases of Vector Spaces",
    "section": "6.2 Coordinates",
    "text": "6.2 Coordinates\nGiven a basis \\(S = \\{\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_n\\}\\) of a vector space \\(V\\) and a vector \\(\\mathbf{v}\\in V\\), there exist unique constants \\(c_1, c_2, ..., c_n\\in\\mathbb{R}\\) such that\n\\[\\mathbf{v} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n\\]\nThese coefficients \\((c_1,c_2,\\cdots,c_n)\\) are called the coordinates of \\(\\mathbf{v}\\) with respect to the basis \\(S\\). We write this coordinate vector as \\([\\mathbf{v}]_S\\), which is an element of \\(\\mathbb{R}^n\\).\nThe relationship between a vector and its coordinates can be expressed as:\n\\[[\\mathbf{v}]_S = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}\\quad\\Longleftrightarrow\\quad\n\\mathbf{v} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n \\tag{6.1}\\]\nThis equivalence leads to two fundamental problems in working with coordinates:\n\nCoordinate Problem: Given a vector \\(\\mathbf{v}\\), find its coordinates \\([\\mathbf{v}]_S\\)\nReconstruction Problem: Given coordinates \\([\\mathbf{v}]_S\\), find the vector \\(\\mathbf{v}\\)\n\nLet’s explore these problems through an example:\n\nExercise: Let \\(S=\\left\\{\\begin{bmatrix}1\\\\1\\end{bmatrix},\\begin{bmatrix}0\\\\1\\end{bmatrix}\\right\\}\\) be a basis of \\(\\mathbb{R}^2\\). Find:\n\nIf \\(\\mathbf{v}=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\), find \\([\\mathbf{v}]_S\\)\nIf \\([\\mathbf{v}]_S=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\), find \\(\\mathbf{v}\\)\n\n\n\n\n\n\n\n\nClick to see detailed answers\n\n\n\n\n\n\n6.3 Problem 1\nWe have that \\(\\mathbf{v}=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\) and we need to find \\([\\mathbf{v}]_S\\)\nTo find \\([\\mathbf{v}]_S\\), we need to find scalars \\(c_1\\) and \\(c_2\\) such that:\n\\[\\mathbf{v} = c_1\\begin{bmatrix}1\\\\1\\end{bmatrix} + c_2\\begin{bmatrix}0\\\\1\\end{bmatrix}\\]\nThis gives us the system: \\(\\begin{bmatrix}2\\\\3\\end{bmatrix} = c_1\\begin{bmatrix}1\\\\1\\end{bmatrix} + c_2\\begin{bmatrix}0\\\\1\\end{bmatrix}\\)\nWhich yields: \\[\\begin{cases}\nc_1 = 2 \\quad \\text{(from first row)}\\\\\nc_1 + c_2 = 3 \\quad \\text{(from second row)}\n\\end{cases}\\]\nSolving:\n\nFrom first equation: \\(c_1 = 2\\)\nSubstitute into second: \\(2 + c_2 = 3\\)\nTherefore: \\(c_2 = 1\\)\n\nThus, \\([\\mathbf{v}]_S=\\begin{bmatrix}2\\\\1\\end{bmatrix}\\)\n\n6.3.1 Verification:\nWe can verify by checking that: \\(2\\begin{bmatrix}1\\\\1\\end{bmatrix} + 1\\begin{bmatrix}0\\\\1\\end{bmatrix} = \\begin{bmatrix}2\\\\3\\end{bmatrix}\\)\n\n\n\n6.4 Problem 2\nWe have that \\([\\mathbf{v}]_S=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\) and we need to find \\(\\mathbf{v}\\)\nFrom Equation 6.1 we have that: \\[\\mathbf{v}\n= 2\\begin{bmatrix}1\\\\1\\end{bmatrix} + 3\\begin{bmatrix}0\\\\1\\end{bmatrix}\n= \\begin{bmatrix}2\\\\2\\end{bmatrix} + \\begin{bmatrix}0\\\\3\\end{bmatrix}\n= \\begin{bmatrix}2\\\\5\\end{bmatrix}\n\\]\nTherefore, \\(\\mathbf{v}=\\begin{bmatrix}2\\\\5\\end{bmatrix}\\)\n\n6.4.1 General Method:\n\nTo find \\([\\mathbf{v}]_S\\): Solve the equation \\(\\mathbf{v} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2\\) for the coefficients\nTo find \\(\\mathbf{v}\\) from \\([\\mathbf{v}]_S\\): Multiply each basis vector by its corresponding coordinate and sum\n\n\n\n\n\n\nEvery basis \\(S\\) gives us a way to represent vectors through their coordinates. This defines a function from the vector space \\(V\\) to \\(\\mathbb{R}^n\\): \\[\\begin{align*}\n\\varphi_S: V &\\to \\mathbb{R}^n\\\\\n\\mathbf{v} &\\mapsto [\\mathbf{v}]_S\n\\end{align*}\\]\nThe following theorem shows that this coordinate map \\(\\varphi_S\\) is linear - it preserves both addition and scalar multiplication.\n\nTheorem 6.1 Suppose that \\(S=\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) is a basis of the vector space \\(V\\). Then\n\nFor every \\(\\mathbf{u},\\mathbf{w}\\in V\\), \\([\\mathbf{u}+\\mathbf{v}]_S=[\\mathbf{u}]_S+[\\mathbf{w}]_S\\).\nFor every \\(\\mathbf{u}\\in V\\) and \\(c\\in\\mathbf{R}\\), \\([c\\mathbf{u}]_S=c[\\mathbf{u}]_S\\).\n\n\n\nExercise: Prove Theorem 6.1.\n\n\n\n\n\n\n\nClick to see the proof\n\n\n\n\n\nLet’s prove that coordinates respect vector addition and scalar multiplication.\n\n6.4.2 Part 1: Addition Property\nClaim: For every \\(\\mathbf{u},\\mathbf{w}\\in V\\), \\([\\mathbf{u}+\\mathbf{w}]_S=[\\mathbf{u}]_S+[\\mathbf{w}]_S\\)\nProof: Let \\([\\mathbf{u}]_S=(a_1,\\ldots,a_n)\\) and \\([\\mathbf{w}]_S=(b_1,\\ldots,b_n)\\)\nBy definition of coordinates, this means: \\[\\mathbf{u}=a_1\\mathbf{v}_1+\\cdots+a_n\\mathbf{v}_n\\] \\[\\mathbf{w}=b_1\\mathbf{v}_1+\\cdots+b_n\\mathbf{v}_n\\]\nConsider \\(\\mathbf{u}+\\mathbf{w}\\): \\[\\begin{align*}\n   \\mathbf{u}+\\mathbf{w}&=(a_1\\mathbf{v}_1+\\cdots+a_n\\mathbf{v}_n)+(b_1\\mathbf{v}_1+\\cdots+b_n\\mathbf{v}_n)\\\\\n   &=(a_1+b_1)\\mathbf{v}_1+\\cdots+(a_n+b_n)\\mathbf{v}_n\n   \\end{align*}\\]\nTherefore, \\([\\mathbf{u}+\\mathbf{w}]_S=(a_1+b_1,\\ldots,a_n+b_n)\\)\nBut this is exactly \\([\\mathbf{u}]_S+[\\mathbf{w}]_S=(a_1,\\ldots,a_n)+(b_1,\\ldots,b_n)\\)\nTherefore, \\([\\mathbf{u}+\\mathbf{w}]_S=[\\mathbf{u}]_S+[\\mathbf{w}]_S\\)\n\n\n6.4.3 Part 2: Scalar Multiplication Property\nClaim: For every \\(\\mathbf{u}\\in V\\) and \\(c\\in\\mathbb{R}\\), \\([c\\mathbf{u}]_S=c[\\mathbf{u}]_S\\)\nProof: Let \\([\\mathbf{u}]_S=(a_1,\\ldots,a_n)\\)\nBy definition of coordinates: \\[\\mathbf{u}=a_1\\mathbf{v}_1+\\cdots+a_n\\mathbf{v}_n\\]\nConsider \\(c\\mathbf{u}\\): \\[\\begin{align*}\n   c\\mathbf{u}&=c(a_1\\mathbf{v}_1+\\cdots+a_n\\mathbf{v}_n)\\\\\n   &=(ca_1)\\mathbf{v}_1+\\cdots+(ca_n)\\mathbf{v}_n\n   \\end{align*}\\]\nTherefore, \\([c\\mathbf{u}]_S=(ca_1,\\ldots,ca_n)\\)\nBut this is exactly \\(c[\\mathbf{u}]_S=c(a_1,\\ldots,a_n)\\)\nTherefore, \\([c\\mathbf{u}]_S=c[\\mathbf{u}]_S\\)",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bases of Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/bases.html#problem-1",
    "href": "chapters/bases.html#problem-1",
    "title": "6  Bases of Vector Spaces",
    "section": "6.3 Problem 1",
    "text": "6.3 Problem 1\nWe have that \\(\\mathbf{v}=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\) and we need to find \\([\\mathbf{v}]_S\\)\nTo find \\([\\mathbf{v}]_S\\), we need to find scalars \\(c_1\\) and \\(c_2\\) such that:\n\\[\\mathbf{v} = c_1\\begin{bmatrix}1\\\\1\\end{bmatrix} + c_2\\begin{bmatrix}0\\\\1\\end{bmatrix}\\]\nThis gives us the system: \\(\\begin{bmatrix}2\\\\3\\end{bmatrix} = c_1\\begin{bmatrix}1\\\\1\\end{bmatrix} + c_2\\begin{bmatrix}0\\\\1\\end{bmatrix}\\)\nWhich yields: \\[\\begin{cases}\nc_1 = 2 \\quad \\text{(from first row)}\\\\\nc_1 + c_2 = 3 \\quad \\text{(from second row)}\n\\end{cases}\\]\nSolving:\n\nFrom first equation: \\(c_1 = 2\\)\nSubstitute into second: \\(2 + c_2 = 3\\)\nTherefore: \\(c_2 = 1\\)\n\nThus, \\([\\mathbf{v}]_S=\\begin{bmatrix}2\\\\1\\end{bmatrix}\\)\n\n6.3.1 Verification:\nWe can verify by checking that: \\(2\\begin{bmatrix}1\\\\1\\end{bmatrix} + 1\\begin{bmatrix}0\\\\1\\end{bmatrix} = \\begin{bmatrix}2\\\\3\\end{bmatrix}\\)",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bases of Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/bases.html#problem-2",
    "href": "chapters/bases.html#problem-2",
    "title": "6  Bases of Vector Spaces",
    "section": "6.4 Problem 2",
    "text": "6.4 Problem 2\nWe have that \\([\\mathbf{v}]_S=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\) and we need to find \\(\\mathbf{v}\\)\nFrom Equation 6.1 we have that: \\[\\mathbf{v}\n= 2\\begin{bmatrix}1\\\\1\\end{bmatrix} + 3\\begin{bmatrix}0\\\\1\\end{bmatrix}\n= \\begin{bmatrix}2\\\\2\\end{bmatrix} + \\begin{bmatrix}0\\\\3\\end{bmatrix}\n= \\begin{bmatrix}2\\\\5\\end{bmatrix}\n\\]\nTherefore, \\(\\mathbf{v}=\\begin{bmatrix}2\\\\5\\end{bmatrix}\\)\n\n6.4.1 General Method:\n\nTo find \\([\\mathbf{v}]_S\\): Solve the equation \\(\\mathbf{v} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2\\) for the coefficients\nTo find \\(\\mathbf{v}\\) from \\([\\mathbf{v}]_S\\): Multiply each basis vector by its corresponding coordinate and sum",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bases of Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/bases.html#dimension",
    "href": "chapters/bases.html#dimension",
    "title": "6  Bases of Vector Spaces",
    "section": "6.5 Dimension",
    "text": "6.5 Dimension\nA very important linear algebra result is that all bases of the same vector space have the same number of elements. This isn’t obvious at first glance – after all, we can find many different bases for a vector space. Yet this common number, which we call the dimension of the vector space, is a fundamental invariant.\nSince the cannonical basis of \\(\\mathbb{R}^n\\) has \\(n\\) elements, all bases have \\(n\\)-elements, and \\(\\mathbb{R}^n\\) has dimension equal to \\(n\\). This result is stated in the next theorem:\n\nTheorem 6.2 Suppose that \\(S = \\{\\mathbf{v}_1,...,\\mathbf{v}_k\\}\\) is a subset of \\(\\mathbb{R}^n\\). Then\n\nIf \\(S\\) is linearly independent, \\(k\\leq n\\).\nIf \\(S\\) spans \\(\\mathbb{R}^n\\), then \\(k\\geq n\\).\n\nConsequently, if \\(S\\) is a basis (both linearly independent and spanning), then \\(k=n\\).\n\n\nProof. To prove this theorem, we analyze the \\(n\\times k\\) matrix whose columns are the vectors in \\(S\\): \\[\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{v}_1 & \\mathbf{v}_2 & \\cdots & \\mathbf{v}_k \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}.\\]\nLet \\(R\\) be the reduced row echelon form (RREF) of this matrix. We then consider two cases:\n\nCase of Linear Independence: If \\(S\\) is linearly independent, then by Theorem 5.3, \\(R\\) has a pivot position in every column. Since \\(S\\) contains \\(k\\) vectors, \\(R\\) has \\(k\\) columns and therefore \\(k\\) pivots. However, in a matrix with \\(n\\) rows, we cannot have more than \\(n\\) pivots (since we can have at most one pivot per row). Therefore, \\(k \\leq n\\).\nCase of Spanning: If \\(S\\) spans \\(\\mathbb{R}^n\\), then by Theorem 5.6, \\(R\\) has a pivot position in every row. Since \\(R\\) has \\(n\\) rows, it must have \\(n\\) pivots. However, we cannot have more pivots than columns (since we can have at most one pivot per column), and \\(R\\) has \\(k\\) columns (one for each vector in \\(S\\)). Therefore, \\(k \\geq n\\). \\(\\square\\)\n\n\n\n6.5.1 Special Case: \\(n\\) Vectors in \\(\\mathbb{R}^n\\)\nThe previous theorem has a remarkable consequence when the number of vectors equals the dimension. In this case, either property (linear independence or spanning) automatically implies the other.\n\nCorollary 6.1 Let \\(S = \\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\}\\) be a set of exactly \\(n\\) vectors in \\(\\mathbb{R}^n\\). Then:\n\nIf \\(S\\) is linearly independent, then \\(S\\) automatically spans \\(\\mathbb{R}^n\\)\nIf \\(S\\) spans \\(\\mathbb{R}^n\\), then \\(S\\) is automatically linearly independent\n\nIn other words, for a set of \\(n\\) vectors in \\(\\mathbb{R}^n\\), either property alone is sufficient to prove that \\(S\\) is a basis.\n\n\n\n\n\n\n\nIdea of the Proof\n\n\n\nThe proof follows from the same matrix analysis as before:\n\nLinear independence gives us \\(n\\) pivots in \\(n\\) columns, forcing every row to have a pivot\nSpanning gives us \\(n\\) pivots in \\(n\\) rows, forcing every column to have a pivot",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bases of Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/bases.html#orthonormal-bases-of-mathbbrn",
    "href": "chapters/bases.html#orthonormal-bases-of-mathbbrn",
    "title": "6  Bases of Vector Spaces",
    "section": "6.6 Orthonormal Bases of \\(\\mathbb{R}^n\\)",
    "text": "6.6 Orthonormal Bases of \\(\\mathbb{R}^n\\)\nA particularly useful type of basis combines orthogonality and normalization:\n\nA basis \\(S = \\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\}\\) of \\(\\mathbb{R}^n\\) is orthonormal if:\n\n\\(\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0\\) for all \\(i \\neq j\\) (orthogonal vectors)\n\\(\\|\\mathbf{v}_i\\| = 1\\) for all \\(i\\) (unit vectors)\n\n\nThe standard basis \\(\\{\\mathbf{e}_1,\\ldots,\\mathbf{e}_n\\}\\) is the most obvious example of an orthonormal basis, but there are many, like this example in \\(\\mathbb{R}^3\\):\n\\[\\left\\{\\frac{1}{\\sqrt{3}}\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}, \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\-1\\\\0\\end{bmatrix}, \\frac{1}{\\sqrt{6}}\\begin{bmatrix}1\\\\1\\\\-2\\end{bmatrix}\\right\\}.\\]\n\n6.6.1 Finding Coordinates\nWorking with orthonormal bases simplifies finding coordinates dramatically. For any basis, finding \\([\\mathbf{v}]_S\\) means solving a system of equations. However, with an orthonormal basis \\(S = \\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\}\\), we can find each coordinate independently: \\[[\\mathbf{v}]_S = \\begin{bmatrix} \\mathbf{v}\\cdot\\mathbf{v}_1 \\\\ \\vdots \\\\ \\mathbf{v}\\cdot\\mathbf{v}_n \\end{bmatrix}\\]\nTo see this, suppose that \\([\\mathbf{v}]_S=(c_1,c_2,\\dots,c_n)\\). Then \\[\\mathbf{v}=c_1\\mathbf{v}_1+c_2\\mathbf{v}_2+\\cdots+c_n\\mathbf{v}_n.\\] Fix \\(i\\leq n\\) and take the dot product of \\(\\mathbf{v}\\) with \\(\\mathbf{v}_i\\) to get: \\[\\mathbf{v}\\cdot\\mathbf{v}_i =c_1(\\mathbf{v}_1\\cdot\\mathbf{v}_i)+c_2(\\mathbf{v}_2\\cdot\\mathbf{v}_i)+\\cdots+c_n(\\mathbf{v}_n\\cdot\\mathbf{v}_i).\\] Since \\(S = \\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\}\\) is orthonormal, \\(\\mathbf{v}_j\\cdot\\mathbf{v}_i=1\\) if \\(j=i\\) and \\(\\mathbf{v}_j\\cdot\\mathbf{v}_i=0\\) if \\(j\\not=i\\). Therefore \\[\\mathbf{v}\\cdot\\mathbf{v}_i=c_i.\\] This shows we can find each coefficient by computing a dot product.\n\n\n6.6.2 Constructing Orthonormal Bases\nFinding orthonormal bases usually requires computation (like the Gram-Schmidt process, which we’ll see in the lab). However, some special orthonormal bases are known. One remarkable example is the Hadamard basis, which exists in dimensions that are powers of 2. For \\(n=4\\), the Hadamard basis is:\n\\[\\left\\{\n\\frac{1}{2}\\begin{bmatrix}1\\\\1\\\\1\\\\1\\end{bmatrix},\n\\frac{1}{2}\\begin{bmatrix}1\\\\1\\\\-1\\\\-1\\end{bmatrix},\n\\frac{1}{2}\\begin{bmatrix}1\\\\-1\\\\1\\\\-1\\end{bmatrix},\n\\frac{1}{2}\\begin{bmatrix}1\\\\-1\\\\-1\\\\1\\end{bmatrix}\n\\right\\}\\]\n\n\n6.6.3 Orthogonal Matrices\n\nDefinition 6.4 An \\(n\\times n\\) matrix \\(A\\) is said to be an orthogonal matrix if \\[A^TA=AA^T=I.\\]\n\nIf follows from Equation 3.6 that the \\(n\\times n\\) matrix \\(A\\) is orthogonal if and only if the columns of \\(A\\) are an orthonormal basis of \\(\\mathbb{R}^n\\).\nThis property makes orthogonal matrices particularly useful because:\n\nThe inverse is simply the transpose: \\(A^{-1} = A^T\\)\nThey preserve distances: \\(\\|A\\mathbf{v}\\| = \\|\\mathbf{v}\\|\\)\nThey preserve angles: if \\(\\mathbf{u}\\cdot\\mathbf{v} = 0\\), then \\((A\\mathbf{u})\\cdot(A\\mathbf{v}) = 0\\)\n\nCommon examples include:\n\nRotation matrices in \\(\\mathbb{R}^2\\): \\(\\begin{bmatrix}\\cos\\theta & -\\sin\\theta\\\\\\sin\\theta & \\cos\\theta\\end{bmatrix}\\)\nReflection matrices across a unit vector \\(\\mathbf{u}\\): \\(I - 2\\mathbf{u}\\mathbf{u}^T\\)\n\n\nEvery orthogonal matrix represents either a rotation, a reflection, or a combination of both. These transformations preserve distances and angles, making them crucial in applications from computer graphics to quantum mechanics.",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bases of Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/subspaces_Rn.html",
    "href": "chapters/subspaces_Rn.html",
    "title": "7  Subspaces of \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "7.1 The Null Space\nRecall that a subset \\(W\\) of \\(\\mathbb{R}^n\\) is a subspace of \\(\\mathbb{R}^n\\) if it satisfies the following three conditions:\nWe showed in Theorem 2.1 that subspaces are also vector spaces with the operations they inherit.\nSince \\(\\text{nul}(A)\\) consists of the solutions of the homogeneous system \\(A\\mathbf{x}=\\mathbf{0}\\), we solve it using Gaussain elimination and back substitution. We will see that this process gives a basis of \\(\\text{nul}(A)\\).\nExample 1: Find a basis for the null space of \\[A = \\begin{bmatrix}\n-1 & 2 & 0 & 1 & 1  \\\\\n2 & 2 & 6 & 4 & -1  \\\\\n1 & -1 & 1 & 0 & 1  \\\\\n1 & 2 & 4 & 3 & 2\n\\end{bmatrix}.\\]\nTo find the solutions of \\(A\\mathbf{x}=\\mathbf{0}\\), we look at the augmented matrix \\([A|\\mathbf{0}]\\), row reduce it:\n\\[\\left[\n\\begin{array}{ccccc|c}\n-1 & 2 & 0 & 1 & 1 & 0 \\\\\n2 & 2 & 6 & 4 & -1 & 0 \\\\\n1 & -1 & 1 & 0 & 1 & 0 \\\\\n1 & 2 & 4 & 3 & 2 & 0\n\\end{array}\\right]\n\\xrightarrow{\\text{RREF}}\n\\left[\n\\begin{array}{ccccc|c}\n1 & 0 & 2 & 1 & 0 & 0\\\\\n0 & 1 & 1 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\n\\right],\\] and then we solve the system by back substitution:\nWrting the solution in vector form we get:\n\\[\\mathbf{x}=\\begin{bmatrix}x_1\\\\ x_2\\\\ x_3\\\\ x_4\\\\ x_5\\end{bmatrix}\n=\\begin{bmatrix}-2x_3-x_4\\\\-x_3-x_4\\\\ x_3\\\\ x_4\\\\0\\end{bmatrix}\n=x_3\\begin{bmatrix}-2\\\\-1\\\\ 1\\\\ 0\\\\0\\end{bmatrix}\n+x_4\\begin{bmatrix}-1\\\\-1\\\\ 0\\\\ 1\\\\0\\end{bmatrix}.\n\\]\nWe claim that \\[S=\\left\\{\\begin{bmatrix}-2\\\\-1\\\\ 1\\\\ 0\\\\0\\end{bmatrix},\n\\begin{bmatrix}-1\\\\-1\\\\ 0\\\\ 1\\\\0\\end{bmatrix}\\right\\}\\] is a basis for \\(\\text{nul}(A)\\). Indeed, we just showed that any solution of \\(A\\mathbf{x}=\\mathbf{0}\\) can be written as a linear combination of these vectors, showing that \\(S\\) spans \\(\\text{nul}(A)\\). Then notice that \\(S\\) is linearly independent, proving that \\(S\\) is a basis of \\(\\text{nul}(A)\\).",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Subspaces of $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/subspaces_Rn.html#the-null-space",
    "href": "chapters/subspaces_Rn.html#the-null-space",
    "title": "7  Subspaces of \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "Definition 7.1 Let \\(A\\) be an \\(m\\times n\\) matrix, the null space of the matrix \\(A\\) is defined by \\[\\text{nul}(A) = \\{\\mathbf{x}\\in\\mathbb{R}^n:A\\mathbf{x}=\\mathbf{0}\\}\\]\n\n\nProposition: Let \\(A\\) be an \\(m\\times n\\) matrix. Then nul\\((A)\\) is a subspace of \\(\\mathbb{R}^n\\)\n\n\nProof. We need to check the three properties:\n\nZero Vector Property: \\(A\\mathbf{0} = \\mathbf{0}\\) by properties of matrix multiplication. Therefore, \\(\\mathbf{0} \\in \\text{nul}(A)\\)\nClosure under Addition: Let \\(\\mathbf{x}, \\mathbf{y} \\in \\text{nul}(A)\\). Then \\(A\\mathbf{x} = \\mathbf{0}\\) and \\(A\\mathbf{y} = \\mathbf{0}\\). Then \\(A(\\mathbf{x} + \\mathbf{y}) = A\\mathbf{x} + A\\mathbf{y} = \\mathbf{0} + \\mathbf{0} = \\mathbf{0}\\). Therefore, \\(\\mathbf{x} + \\mathbf{y} \\in \\text{nul}(A)\\)\nClosure under Scalar Multiplication: Let \\(\\mathbf{x} \\in \\text{nul}(A)\\) and \\(c \\in \\mathbb{R}\\). Then \\(A(c\\mathbf{x}) = cA\\mathbf{x} = c\\mathbf{0} = \\mathbf{0}\\). Therefore, \\(c\\mathbf{x} \\in \\text{nul}(A)\\) \\(\\square\\)\n\n\n\nIf \\(A\\) is an \\(m\\times n\\) matrix:\n\n\\(\\text{nul}(A)\\) is a subspace of \\(\\mathbb{R}^n\\), and\n\\(\\text{nul}(A^T)\\) is a subspace of \\(\\mathbb{R}^m\\).\n\n\n\n\n\n\n\nFrom the third row: \\(x_5=0\\)\n\\(x_4\\) is free\n\\(x_3\\) is free\nFrom the second row: \\(x_2= -x_3-x_4\\), and\nFrom the first row: \\(x_1=-2x_3-x_4\\).\n\n\n\n\n\n\n\n\n\n\nGaussian Elimination Naturally Produces Basis Vectors\n\n\n\nThe point is that back substitution gives us vectors that:\n\nAre automatically linearly independent (due to the staggered positioning of pivots and free variables)\nGenerate all solutions (since we systematically express each variable in terms of free variables)\n\nThese two properties are exactly what’s needed for a basis. The systematic nature of Gaussian elimination ensures this happens every time.",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Subspaces of $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/subspaces_Rn.html#the-span-of-a-set-of-vectors",
    "href": "chapters/subspaces_Rn.html#the-span-of-a-set-of-vectors",
    "title": "7  Subspaces of \\(\\mathbb{R}^n\\)",
    "section": "7.2 The Span of a Set of Vectors",
    "text": "7.2 The Span of a Set of Vectors\nLet \\(\\mathbf{v}_1, ..., \\mathbf{v}_k \\in \\mathbb{R}^n\\). Recall (Section 1.3) that the span of a set of vectors is defined by \\[\\text{span}\\{\\mathbf{v}_1, ..., \\mathbf{v}_k\\} = \\{c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + ... + c_k\\mathbf{v}_k : c_1, c_2, ..., c_k \\in \\mathbb{R}\\}.\\]\nParts of the following theorem were discussed in Theorem 1.1\n\nProposition: \\(\\text{span}\\{\\mathbf{v}_1, ..., \\mathbf{v}_k\\}\\) is a subspace of \\(\\mathbb{R}^n\\)\n\n\nProof. Let \\(W=\\text{span}\\{\\mathbf{v}_1, ..., \\mathbf{v}_k\\}\\). We need to check the three conditions:\n\nZero Vector Property: Let \\(c_1 = c_2 = ... = c_k = 0\\) Then \\(0\\mathbf{v}_1 + 0\\mathbf{v}_2 + ... + 0\\mathbf{v}_k = \\mathbf{0}\\) Therefore, \\(\\mathbf{0} \\in W\\)\nClosure under Addition: Let \\(\\mathbf{x}, \\mathbf{y} \\in W\\) where: \\(\\mathbf{x} = a_1\\mathbf{v}_1 + ... + a_k\\mathbf{v}_k\\) \\(\\mathbf{y} = b_1\\mathbf{v}_1 + ... + b_k\\mathbf{v}_k\\)\nThen: \\(\\mathbf{x} + \\mathbf{y} = (a_1+b_1)\\mathbf{v}_1 + ... + (a_k+b_k)\\mathbf{v}_k \\in W\\)\nClosure under Scalar Multiplication: Let \\(\\mathbf{x} \\in W\\) where \\(\\mathbf{x} = a_1\\mathbf{v}_1 + ... + a_k\\mathbf{v}_k\\) For any \\(c \\in \\mathbb{R}\\): \\(c\\mathbf{x} = (ca_1)\\mathbf{v}_1 + ... + (ca_k)\\mathbf{v}_k \\in W\\) \\(\\square\\)\n\n\nAn important example is the column space of a matrix:\n\nDefinition 7.2 Let \\(A\\) be an \\(m\\times n\\) matrix \\[A = \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix},\\] \\(\\text{col}(A)=\\text{span}\\{\\mathbf{c}_1,\\mathbf{c}_2,\\dots,\\mathbf{c}_n\\}\\)\n\n\nIf \\(A\\) is an \\(m\\times n\\) matrix:\n\n\\(\\text{col}(A)\\) is a subspace of \\(\\mathbb{R}^m\\), and\n\\(\\text{col}(A^T)\\) is a subspace of \\(\\mathbb{R}^n\\).\n\n\n\n7.2.1 Finding bases\nLet \\(W=\\text{span}\\{\\mathbf{v}_1, ..., \\mathbf{v}_k\\}\\subset\\mathbb{R}^n\\). A basis of \\(W\\) is a set of vectors that spans \\(W\\) and that is linearly independent. We already have a spanning set \\(\\{\\mathbf{v}_1, ..., \\mathbf{v}_k\\}\\). If this set is linearly independent, it forms a basis. Otherwise, some vectors can be written as linear combinations of others. Either way, the first step is to check if the set is linearly independent. This leads to a homogeneous system that is solved using Gaussain elimination. This process will tell us how to remove the dependent vectors systematically keeping only the vectors corresponding to pivot columns. This process yields a basis that spans the same subspace \\(W\\).\nExample: Find a basis for \\[W=\\text{span}\\left\\{   \n   \\begin{bmatrix}-1\\\\2\\\\1\\\\1\\end{bmatrix},\n   \\begin{bmatrix}2\\\\2\\\\-1\\\\2\\end{bmatrix},\n   \\begin{bmatrix}0\\\\6\\\\1\\\\4\\end{bmatrix},\n   \\begin{bmatrix}1\\\\4\\\\0\\\\3\\end{bmatrix},\n   \\begin{bmatrix}1\\\\-1\\\\1\\\\2\\end{bmatrix}\n\\right\\}\\]\nNotice that this is the same problem as finding a basis for \\(\\text{col}(A)\\), where \\(A\\) is the matrix of Example 1: \\[A = \\begin{bmatrix}\n-1 & 2 & 0 & 1 & 1  \\\\\n2 & 2 & 6 & 4 & -1  \\\\\n1 & -1 & 1 & 0 & 1  \\\\\n1 & 2 & 4 & 3 & 2\n\\end{bmatrix}=\n  \\begin{bmatrix}\n  \\uparrow & \\uparrow & \\uparrow & \\uparrow &\\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\mathbf{c}_3 & \\mathbf{c}_4 & \\mathbf{c}_5 \\\\\n\\downarrow & \\downarrow & \\downarrow & \\downarrow & \\downarrow\n  \\end{bmatrix}.\\]\nThe first step is to check if the set is linearly independent. We look at the equation \\[x_1\\begin{bmatrix}-1\\\\2\\\\1\\\\1\\end{bmatrix}+\n  x_2 \\begin{bmatrix}2\\\\2\\\\-1\\\\2\\end{bmatrix}+\n  x_3 \\begin{bmatrix}0\\\\6\\\\1\\\\4\\end{bmatrix}+\n  x_4 \\begin{bmatrix}1\\\\4\\\\0\\\\3\\end{bmatrix}+\n  x_5 \\begin{bmatrix}1\\\\-1\\\\1\\\\2\\end{bmatrix} =\n  \\begin{bmatrix}0\\\\0\\\\0\\\\0\\end{bmatrix},\n  \\] and we solve it by writing the augmented matrix and finding the row reduced echelon form.\n\\[\\left[\n\\begin{array}{ccccc|c}\n-1 & 2 & 0 & 1 & 1 & 0 \\\\\n2 & 2 & 6 & 4 & -1 & 0 \\\\\n1 & -1 & 1 & 0 & 1 & 0 \\\\\n1 & 2 & 4 & 3 & 2 & 0\n\\end{array}\\right]\n\\xrightarrow{\\text{RREF}}\n\\left[\n\\begin{array}{ccccc|c}\n1 & 0 & 2 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\n\\right].\\]\nWith three pivots and two free columns, the columns are not linearly independent. We use the null space basis from Example 1: \\[S=\\left\\{\\begin{bmatrix}-2\\\\-1\\\\ 1\\\\ 0\\\\0\\end{bmatrix},\n\\begin{bmatrix}-1\\\\-1\\\\ 0\\\\ 1\\\\0\\end{bmatrix}\\right\\}\\]\nThese vectors reveal the dependency of non-pivot columns on pivot columns. Since: \\[A\\begin{bmatrix}-2\\\\-1\\\\ 1\\\\ 0\\\\0\\end{bmatrix}=\\mathbf{0} \\quad\\text{and}\\quad A\\begin{bmatrix}-1\\\\-1\\\\ 0\\\\ 1\\\\0\\end{bmatrix}=\\mathbf{0}\\]\nIt follows from Equation 3.1 that \\(-2\\mathbf{c}_1-\\mathbf{c}_2+\\mathbf{c}_3=\\mathbf{0}\\) and \\(-\\mathbf{c}_1-\\mathbf{c}_2+\\mathbf{c}_4=\\mathbf{0}\\), yielding: \\[\\begin{align}\n\\mathbf{c}_3 &= 2\\mathbf{c}_1+\\mathbf{c}_2\\\\\n\\mathbf{c}_4 &=\\mathbf{c}_1+\\mathbf{c}_2\n\\end{align}\\]\nThese equations show we can express \\(\\mathbf{c}_3\\) and \\(\\mathbf{c}_4\\) using \\(\\{\\mathbf{c}_1,\\mathbf{c}_2,\\mathbf{c}_5\\}\\). Therefore, \\(\\{\\mathbf{c}_1,\\mathbf{c}_2,\\mathbf{c}_5\\}\\) spans the column space of \\(A\\). Since this set is linearly independent, it forms a basis: \\[\\left\\{   \n   \\begin{bmatrix}-1\\\\2\\\\1\\\\1\\end{bmatrix},\n   \\begin{bmatrix}2\\\\2\\\\-1\\\\2\\end{bmatrix},\n   \\begin{bmatrix}1\\\\-1\\\\1\\\\2\\end{bmatrix}\n\\right\\}\\]\n\n\n\n\n\n\nPivot Columns Form a Basis for Col(A)\n\n\n\nThe pivot columns from RREF give us a basis because:\n\nThey are linearly independent (due to the pivot structure in RREF)\nEvery non-pivot column can be written as a combination of pivot columns (using RREF coefficients)\n\nThese two properties are precisely what defines a basis. Since this comes from the systematic row reduction process, it works every time.",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Subspaces of $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/subspaces_Rn.html#sec-orthogonal-complement",
    "href": "chapters/subspaces_Rn.html#sec-orthogonal-complement",
    "title": "7  Subspaces of \\(\\mathbb{R}^n\\)",
    "section": "7.3 The Orthogonal Complement",
    "text": "7.3 The Orthogonal Complement\nLet \\(W\\) be a subspace of \\(\\mathbb{R}^n\\), the set \\[W^\\perp = \\{\\mathbf{v} \\in \\mathbb{R}^n : \\mathbf{v} \\cdot \\mathbf{w} = 0 \\text{ for all } \\mathbf{w} \\in W\\}\\] is called the orthogonal complement of \\(WS\\).\n\nProposition: \\(W^\\perp\\) is a subspace of \\(\\mathbb{R}^n\\)\n\n\nProof. We need to check the three conditions:\n\nZero Vector Property: For any \\(\\mathbf{w} \\in W\\), \\(\\mathbf{0} \\cdot \\mathbf{w} = 0\\) Therefore, \\(\\mathbf{0} \\in W^\\perp\\)\nClosure under Addition: Let \\(\\mathbf{x}, \\mathbf{y} \\in W^\\perp\\) and \\(\\mathbf{w} \\in W\\) \\((\\mathbf{x} + \\mathbf{y}) \\cdot \\mathbf{w} = \\mathbf{x} \\cdot \\mathbf{w} + \\mathbf{y} \\cdot \\mathbf{w} = 0 + 0 = 0\\) Therefore, \\(\\mathbf{x} + \\mathbf{y} \\in W^\\perp\\)\nClosure under Scalar Multiplication: Let \\(\\mathbf{x} \\in W^\\perp\\), \\(c \\in \\mathbb{R}\\), and \\(\\mathbf{w} \\in W\\) \\((c\\mathbf{x}) \\cdot \\mathbf{w} = c(\\mathbf{x} \\cdot \\mathbf{w}) = c(0) = 0\\) Therefore, \\(c\\mathbf{x} \\in W^\\perp\\) \\(\\square\\)",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Subspaces of $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/subspaces_Rn.html#fundamental-subspaces-of-a-matrix",
    "href": "chapters/subspaces_Rn.html#fundamental-subspaces-of-a-matrix",
    "title": "7  Subspaces of \\(\\mathbb{R}^n\\)",
    "section": "7.4 Fundamental Subspaces of a Matrix",
    "text": "7.4 Fundamental Subspaces of a Matrix\nEvery matrix gives rise to four fundamental subspaces, which are connected through important orthogonality relationships.\nLet \\(A\\) be an \\(m \\times n\\) matrix. Consider its transpose \\(A^T\\), which is an \\(n \\times m\\) matrix. The four fundamental subspaces are:\n\nThe Column Space of \\(A\\): \\(\\text{col}(A)\\)\n\nIt is a subspace of \\(\\mathbb{R}^m\\)\n\nThe Null Space of \\(A\\): \\(\\text{nul}(A)\\)\n\nIt is a subspace of \\(\\mathbb{R}^n\\)\nAlso called the kernel of \\(A\\)\n\nThe column space of \\(A^T\\): \\(\\text{col}(A^T)\\)\n\nIt is a subspace of \\(\\mathbb{R}^n\\)\nAlso called the row space of \\(A\\)\n\nThe Null Space of \\(A^T\\): \\(\\text{nul}(A^T)\\)\n\nIt is a subspace of \\(\\mathbb{R}^m\\)\n\n\n\nTheorem 7.1 Suppose that \\(A\\) is an \\(m\\times n\\) matrix. then \\[\\text{col}(A)^\\perp = \\text{nul}(A^T)\\]\n\n\nProof. Let \\(A\\) be an \\(m\\times n\\) matrix and represent \\(A\\) and \\(A^T\\) using the columns of \\(A\\): \\[A = \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\\quad\\text{and}\\quad\nA^T = \\begin{bmatrix}\n\\leftarrow & \\mathbf{c}_1^T &\\rightarrow\\\\\n\\leftarrow & \\mathbf{c}_2^T &\\rightarrow\\\\\n\\vdots &\\vdots&\\vdots\\\\\n\\leftarrow & \\mathbf{c}_n^T &\\rightarrow\\\\\n\\end{bmatrix}\\]\nLet \\(\\mathbf{w}\\in\\text{col}(A)\\) and \\(\\mathbf{v}\\in \\mathbb{R}^m\\). The proof follows from the following observations:\n\n\\(\\mathbf{w}=x_1\\mathbf{c}_1+\\cdots+x_n\\mathbf{c}_n\\) for some \\(x_1,\\dots,x_n\\in\\mathbb{R}\\).\n\\(\\mathbf{w}\\cdot\\mathbf{v}=x_1(\\mathbf{c}_1\\cdot\\mathbf{v})+\\cdots+x_n(\\mathbf{c}_n\\cdot\\mathbf{v})\\)\nFrom Equation 3.3, \\[A^T\\mathbf{v}=\\begin{bmatrix}\n\\leftarrow & \\mathbf{c}_1^T &\\rightarrow\\\\\n\\leftarrow & \\mathbf{c}_2^T &\\rightarrow\\\\\n\\vdots &\\vdots&\\vdots\\\\\n\\leftarrow & \\mathbf{c}_n^T &\\rightarrow\\\\\n\\end{bmatrix}\\mathbf{v}\n=\\begin{bmatrix}\n\\mathbf{c}_1\\cdot\\mathbf{v}\\\\\\mathbf{c}_2\\cdot\\mathbf{v}\\\\\\vdots\\\\ \\mathbf{c}_n\\cdot\\mathbf{v}\n\\end{bmatrix}.\n\\]\n\nThen, from 1. and 2. we see that \\(\\mathbf{v}\\in(\\text{col}(A))^\\perp\\) if and only if \\(\\mathbf{c}_i\\cdot\\mathbf{v}=0\\) for \\(i\\leq n\\). From 3. we see that these statements are equivalent to \\(A^T\\mathbf{v}=\\mathbf{0}\\), which is the definition of \\(\\mathbf{v}\\in\\text{nul}(A)\\). Therefore, \\((\\text{col}(A))^\\perp=\\text{nul}(A^T)\\)\n\nReplacing \\(A\\) by \\(A^T\\) in Theorem 7.1 we get that \\[(\\text{col}(A^T))^\\perp=\\text{nul}(A).\\]",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Subspaces of $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "worksheets/bases_subspaces.html",
    "href": "worksheets/bases_subspaces.html",
    "title": "Problems",
    "section": "",
    "text": "Bases Coordinates and Subspaces\nFor these problems, you should use SymPy to find row reduced echelon matrices. Once you write a matrix \\(A\\), you can simply write the matrix \\(R\\) indicating that it is the row reduced one. You can use symbols like this: \\(A\\xrightarrow{\\text{RREF}}R\\).",
    "crumbs": [
      "Subspaces and Bases",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/bases_subspaces.html#bases-coordinates-and-subspaces",
    "href": "worksheets/bases_subspaces.html#bases-coordinates-and-subspaces",
    "title": "Problems",
    "section": "",
    "text": "Problem 1\nLet \\(\\mathcal{B} = \\left\\{\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 2 \\end{bmatrix}\\right\\}\\) be a basis for \\(\\mathbb{R}^3\\).\n\nFind the coordinates of \\(\\mathbf{v} = (3,2,4)\\) with respect to \\(\\mathcal{B}\\). That is, find scalars \\(c_1, c_2, c_3\\) such that:\n\n\\[\\mathbf{v} = c_1(1,1,0) + c_2(-1,1,0) + c_3(0,0,2)\\]\n\nFind the coordinates of the following vectors with respect to \\(\\mathcal{B}\\) simultaneously. You can solve three systems of equations, but if you pay attention, you can solve one system that gives you the coordinates of the three vectors at once.\n\n\n\\(\\mathbf{v}_1 = (1,0,1)\\)\n\\(\\mathbf{v}_2 = (2,2,0)\\)\n\\(\\mathbf{v}_3 = (-1,1,3)\\)\n\n\n\nProblem 2\nGiven the basis \\(\\mathcal{B} = \\{(1,1,1), (1,1,-1), (1,-2,0)\\}\\) for \\(\\mathbb{R}^3\\):\n\nFind the coordinates of the standard basis vectors with respect to \\(\\mathcal{B}\\):\n\n\n\\(\\mathbf{e}_1 = (1,0,0)\\)\n\\(\\mathbf{e}_2 = (0,1,0)\\)\n\\(\\mathbf{e}_3 = (0,0,1)\\)\n\n\n\nProblem 3\nLet \\(\\mathcal{B} = \\{(2,0,1), (0,1,1), (1,1,0)\\}\\) be a basis for \\(\\mathbb{R}^3\\).\n\nFind the vector \\(\\mathbf{v}\\) whose coordinates with respect to \\(\\mathcal{B}\\): \\([v]_\\mathcal{B}=(2,-1,3)\\).\nIf a vector has coordinates \\((1,1,1)\\) with respect to \\(\\mathcal{B}\\), what is this vector in standard coordinates?\n\n\n\nProblem 4\nLet \\(S_1 =\n\\left\\{\n\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix},\n\\begin{bmatrix}0\\\\2\\\\1\\end{bmatrix},\n\\begin{bmatrix}1\\\\1\\\\2\\end{bmatrix}\n\\right\\}\\) and \\(S_2\n=\n\\left\\{\n\\begin{bmatrix}2\\\\1\\\\0\\end{bmatrix},\n\\begin{bmatrix}1\\\\0\\\\2\\end{bmatrix},\n\\begin{bmatrix}3\\\\1\\\\1\\end{bmatrix}\n\\right\\}\\) be bases of \\(\\mathbb{R}^3\\).\n\nIf \\([v]_{S_1} = (2,-1,3)\\), find \\([v]_{S_2}\\).\n\nIf \\([v]_{S_1} = (1,2,0)\\), find \\([v]_{S_2}\\).\n\nIf \\([v]_{S_1} = (0,0,1)\\), find \\([v]_{S_2}\\).\n\n\n\nProblem 5\nLet \\(S_1 =\n\\left\\{\n\\begin{bmatrix}2\\\\1\\\\1\\end{bmatrix},\n\\begin{bmatrix}1\\\\2\\\\0\\end{bmatrix},\n\\begin{bmatrix}0\\\\1\\\\3\\end{bmatrix}\n\\right\\}\\) and \\(S_2 = \\left\\{\n\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix},\n\\begin{bmatrix}2\\\\0\\\\1\\end{bmatrix},\n\\begin{bmatrix}1\\\\2\\\\2\\end{bmatrix}\n\\right\\}\\) be bases of \\(\\mathbb{R}^3\\).\n\nIf \\([w]_{S_1} = (1,-1,2)\\), find \\([w]_{S_2}\\).\n\nIf \\([w]_{S_1} = (3,-0,0)\\), find \\([w]_{S_2}\\).\n\nIf \\([w]_{S_1} = (2,2,1)\\), find \\([w]_{S_2}\\).\n\n\n\nProblem 6\nFor the following matrices\n\n\nFind a basis for \\(\\text{col}(A)\\)\nFind a basis for \\(\\text{nul}(A)\\)\nWrite the columns of \\(A\\) that you didn’t include in the basis in terms of the ones you kept\n\n\n\n\\(A=\\begin{bmatrix}1&4&-1&3\\\\1&5&0&2\\\\0&3&3&1 \\end{bmatrix}\\)\n\\(A=\\begin{bmatrix}\n5 & 10 & 12 & 14 & 11 & 3\\\\\n-4 & -8 & -5 & -2 & -27 & -16\\\\\n2 & 4 & 6 & 8 & 0 & -2\n\\\\1 & 2 & 8 & 14 & -19 & -15\n\\end{bmatrix}\\)\n\n\n\nProblem 7\nFor each of the following sets,\n\n\nwrite the set as the span of some vectors,\nexpress the set as the column space of a matrix \\(A\\),\nfind a basis of the subspace and find the dimension.\n\n\nNotice that it follows from (1) or (2) that the set is a subspace\n\nThe set of all vectors of the form \\[\n\\left\\{ \\begin{bmatrix}\nx + 2y \\\\\ny - z \\\\\n2x - z\n\\end{bmatrix} : x,y,z \\in \\mathbb{R} \\right\\}\n\\]\nThe set of all vectors of the form \\[\n\\left\\{ \\begin{bmatrix}\n-3s + 6t - u + v - 7w \\\\\ns - 2t + 2u + 3v - w \\\\\n2s - 4t + 5u + 8v - 4w\n\\end{bmatrix} : s,t,u,v,w \\in \\mathbb{R} \\right\\}\n\\]\nThe set of all vectors of the form \\[\n\\left\\{ \\begin{bmatrix}\n-2s - 5t + 8u - 17w \\\\\ns + 3t - 5u + v + 5w \\\\\n3s + 11t - 19u + 7v + w \\\\\ns + 7t - 13u + 5v - 3w\n\\end{bmatrix} : s,t,u,v,w \\in \\mathbb{R} \\right\\}\n\\]",
    "crumbs": [
      "Subspaces and Bases",
      "Problems"
    ]
  },
  {
    "objectID": "parts/least_squares.html",
    "href": "parts/least_squares.html",
    "title": "Least Squares",
    "section": "",
    "text": "This section covers orthogonal complements, the projection theorem and regression.\n\nLeast Squares\nProblems",
    "crumbs": [
      "Least Squares"
    ]
  },
  {
    "objectID": "chapters/projection_theorem.html",
    "href": "chapters/projection_theorem.html",
    "title": "8  Least Squares",
    "section": "",
    "text": "8.1 Orthogonal Projection\nLeast squares problems naturally arise when we seek the “best” solution to an inconsistent system of linear equations \\(𝐴\\mathbf{x}=\\mathbf{b}\\). While we typically classify linear systems as having no solution, exactly one solution, or infinitely many solutions, real-world data often leads to situations where \\(\\mathbf{b}\\) lies outside the column space of \\(𝐴\\), making an exact solution impossible. Instead of giving up, we ask a more refined question: what vector \\(\\mathbf{x}\\) minimizes the discrepancy between \\(A\\mathbf{x}\\) and \\(\\mathbf{b}\\)? In other words, we seek to minimize \\[\\min_{\\mathbf{x}}\\|A\\mathbf{x}-\\mathbf{b}\\|,\\] where \\(\\|\\cdot\\|\\) is the Euclidean norm, induced by the dot product.\nThis problem is fundamental in statistics, applied science, and modern machine learning, where data rarely fits models exactly. Least squares provides a principled way to handle noisy observations, from experimental measurements to regression analysis and AI algorithms.\nAt its core, least squares is an optimization problem: we seek the vector \\(\\mathbf{x}\\) that minimizes a given objective function. This idea extends far beyond linear systems—modern machine learning methods, including deep learning, rely on solving large-scale optimization problems where the goal is to minimize a loss function that quantifies error. Least squares serves as an important first example of how mathematical optimization underpins data-driven modeling.\nThe concept of orthogonal projection is fundamental to understanding how we can find the closest point in a subspace to any given vector. This builds from our understanding of orthogonal complements (Section 7.3).\nThe vector \\(\\hat{\\mathbf{v}}\\) is called the orthogonal projection of \\(\\mathbf{v}\\) onto \\(W\\). The term “orthogonal” comes from the second condition: the error vector \\(\\mathbf{v}-\\hat{\\mathbf{v}}\\) is perpendicular to the subspace \\(W\\). Two fundamental geometric properties give us the solution: the Pythagorean Theorem (Theorem 1.2) demonstrates why this orthogonality guarantees that \\(\\hat{\\mathbf{v}}\\) is the closest point in \\(W\\) to \\(\\mathbf{v}\\), while the Parallelogram Law (Equation 1.1) ensures this closest point is unique. Note that we solve this minimization problem using purely geometric arguments, without resorting to calculus.\nThis projection theorem forms the theoretical foundation for solving least squares problems.",
    "crumbs": [
      "Least Squares",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "chapters/projection_theorem.html#orthogonal-projection",
    "href": "chapters/projection_theorem.html#orthogonal-projection",
    "title": "8  Least Squares",
    "section": "",
    "text": "Theorem 8.1 Suppose that \\(W\\) is a subspace of \\(\\mathbb{R}^n\\) and that \\(\\mathbf{v}\\in\\mathbf{R}^n\\). Then there exists a unique \\(\\hat{\\mathbf{v}}\\in W\\) such that \\[\\|\\mathbf{v}-\\hat{\\mathbf{v}}\\|=\\min_{\\mathbf{w}\\in W}\\|\\mathbf{v}-\\mathbf{w}\\|. \\] Moreover, \\(\\hat{\\mathbf{v}}\\) is characterized by\n\n\\(\\hat{\\mathbf{v}}\\in W\\)\n\\(\\mathbf{v}-\\hat{\\mathbf{v}}\\in W^\\perp\\)",
    "crumbs": [
      "Least Squares",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "chapters/projection_theorem.html#least-squares-via-projection-onto-cola",
    "href": "chapters/projection_theorem.html#least-squares-via-projection-onto-cola",
    "title": "8  Least Squares",
    "section": "8.2 Least Squares via Projection onto col(A)",
    "text": "8.2 Least Squares via Projection onto col(A)\nThe projection theorem provides what we need to solve the least squares problem \\(\\min \\|A\\mathbf{x} - \\mathbf{b}\\|\\). When we seek to minimize \\(\\|A\\mathbf{x} - \\mathbf{b}\\|\\), we are effectively looking for the closest point in \\(\\text{col}(A)\\) to the vector \\(\\mathbf{b}\\). By the projection theorem, this closest point must be the orthogonal projection of \\(\\mathbf{b}\\) onto \\(\\text{col}(A)\\).\n\nTheorem 8.2 Let \\(A\\) be an \\(m \\times n\\) matrix and \\(\\mathbf{b}\\in\\mathbb{R}^m\\). Then \\(\\mathbf{b}\\) can be uniquely decomposed as \\[\\mathbf{b} = \\mathbf{\\hat{b}} + \\mathbf{e}\\] where \\(\\mathbf{\\hat{b}} \\in \\text{col}(A)\\) and \\(\\mathbf{e} = \\mathbf{b} - \\hat{\\mathbf{b}} \\in (\\text{col}(A))^\\perp = \\text{null}(A^T)\\). Moreover:\n\n\\(\\mathbf{\\hat{b}} = A\\mathbf{\\hat{x}}\\) where \\(\\mathbf{\\hat{x}}\\) is any solution to the normal equations \\(A^TA\\mathbf{x} = A^T\\mathbf{b}\\)\n\\(\\|\\mathbf{b} - A\\mathbf{x}\\| \\geq \\|\\mathbf{e}\\|\\) for all \\(\\mathbf{x} \\in \\mathbb{R}^n\\), with equality if and only if \\(\\mathbf{x} = \\mathbf{\\hat{x}}\\)\n\nIf \\(A^TA\\) is invertible, then \\(\\mathbf{\\hat{x}}\\) is unique and\n\n\\(\\mathbf{\\hat{x}}=(A^TA)^{-1}A^T\\mathbf{b}\\), and\n\\(\\mathbf{\\hat{b}}=A(A^TA)^{-1}A^T\\mathbf{b}\\)\n\n\nThis theorem translates the abstract projection theorem into concrete matrix computations. The normal equations arise naturally because \\((\\text{col}(A))^\\perp = \\text{null}(A^T)\\) Theorem 7.1, connecting the algebraic and geometric perspectives of least squares solutions.\nThe matrix \\(P=A(A^TA)^{-1}A^T\\) in part 4 has two remarkable properties: \\(P^2=P\\) and \\(P^T=P\\). A matrix satisfying \\(P^2=P\\) is called a projection matrix: applying it twice doesn’t change anything, just as geometrically projecting a point that’s already in the subspace doesn’t move it. When a projection matrix is also symmetric (\\(P^T=P\\)), we call it an orthogonal projection matrix, as it projects vectors orthogonally onto its range. These properties capture the geometric meaning of our least squares solution: \\(P\\mathbf{b}\\) is the orthogonal projection of \\(\\mathbf{b}\\) onto \\(\\text{col}(A)\\).",
    "crumbs": [
      "Least Squares",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "chapters/projection_theorem.html#invertibility-of-ata-optional",
    "href": "chapters/projection_theorem.html#invertibility-of-ata-optional",
    "title": "8  Least Squares",
    "section": "8.3 Invertibility of \\(A^TA\\) (Optional)",
    "text": "8.3 Invertibility of \\(A^TA\\) (Optional)\nIn this section we will prove that if the columns of \\(A\\) are linearly independent, then \\(A^TA\\) is invertible. The main tool is Theorem 5.3.\nSuppose that \\(A\\) is an \\(m\\times n\\) matrix satifying \\(A\\mathbf{x}=\\mathbf{0}\\) has a unique solution. We will show that this implies that \\(A^TA\\mathbf{x}=\\mathbf{0}\\) also has a unique solution. This implies that the RREF of \\(A^TA\\) has \\(n\\) pivots, and since \\(A^TA\\) is \\(n\\times n\\), the RREF of \\(A\\) is the identity, which implies that \\(A^TA\\) is invertible, as we showed in the lab.\nThe main tool to prove this is the following important Lemma. Recall that if \\(\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^n\\), we can write their dot product as the transpose of the first vector times the second one: \\[\\mathbf{x}\\cdot\\mathbf{y}=\\begin{bmatrix}x_1&x_2&\\cdots &x_n\\end{bmatrix}\n\\begin{bmatrix}\ny_1\\\\ y_2\\\\ \\vdots\\\\ y_n\n\\end{bmatrix}\\]\n\nLemma 8.1 Suppose that \\(A\\) is an \\(m\\times n\\) matrix, that \\(\\mathbf{x}\\in\\mathbb{R}^m\\) and \\(\\mathbf{y}\\in\\mathbb{R}^n\\). Then \\[(A^T\\mathbf{x})\\cdot\\mathbf{y}=\\mathbf{x}\\cdot(A\\mathbf{y})\\]\n\n\nProof. Using the fact that the dot product can be written as matrix multiplication of a row vector with a column vector:\n\\[(A^T\\mathbf{x})\\cdot\\mathbf{y} = ((A^T\\mathbf{x})^T)\\mathbf{y}.\\]\nThen\n\\[((A^T\\mathbf{x})^T)\\mathbf{y} = \\mathbf{x}^TA\\mathbf{y} = \\mathbf{x}\\cdot(A\\mathbf{y})\\]\nwhere the first equality follows from the property \\((A^T\\mathbf{x})^T = \\mathbf{x}^TA\\) of matrix transpose.\n\n\nProposition: Let \\(A\\) be an \\(m\\times n\\) matrix. If \\(A\\mathbf{x}=\\mathbf{0}\\) has a unique solution, then \\(A^TA\\mathbf{x}=\\mathbf{0}\\) also has a unique solution.\n\n\nProof. Suppose that \\(A^TA\\mathbf{x}=\\mathbf{0}\\). Then \\(A^TA\\mathbf{x}\\cdot \\mathbf{x}=0\\). Applying Lemma 8.1 we get \\[0=A^TA\\mathbf{x}\\cdot\\mathbf{x}=A^T(A\\mathbf{x})\\cdot\\mathbf{x}=A\\mathbf{x}\\cdot A\\mathbf{x}=\\|A\\mathbf{x}\\|^2.\\] This implies that \\(A\\mathbf{x}=\\mathbf{0}\\) and the assumption on \\(A\\) tells us that \\(\\mathbf{x}=\\mathbf{0}\\) \\(\\square\\)",
    "crumbs": [
      "Least Squares",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "chapters/projection_theorem.html#regression-problems",
    "href": "chapters/projection_theorem.html#regression-problems",
    "title": "8  Least Squares",
    "section": "8.4 Regression Problems",
    "text": "8.4 Regression Problems\nSee slide presentation",
    "crumbs": [
      "Least Squares",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "chapters/projection_theorem.html#working-with-data-using-pandas",
    "href": "chapters/projection_theorem.html#working-with-data-using-pandas",
    "title": "8  Least Squares",
    "section": "8.5 Working with Data using Pandas",
    "text": "8.5 Working with Data using Pandas\n\n8.5.1 What is Pandas?\n\nPython library for data manipulation and analysis\nBuilt on top of NumPy, provides DataFrame objects\nEfficiently handles tabular data with labeled rows and columns\nPerfect for preprocessing data before linear regression\n\n\n\n8.5.2 Basic Syntax\nimport pandas as pd\nimport numpy as np\n\n# Create DataFrame from CSV\ndf = pd.read_csv('data.csv')\n\n# View first few rows\ndf.head()\n\n# Basic information about the dataset\ndf.info()\n\n\n8.5.3 Essential Functions for Linear Regression\n\nData Selection\n# Select single column (returns Series)\ny = df['target_variable']\n\n# Select multiple columns (returns DataFrame)\nX = df[['feature1', 'feature2', 'feature3']]\nConverting to NumPy\n# Convert to numpy arrays for matrix operations\nX_matrix = X.to_numpy()\ny_vector = y.to_numpy()\nWorking Example: Least Squares\n# Read data\ndf = pd.read_csv('housing_data.csv')\n\n# Select features and target\nX = df[['square_feet', 'bedrooms', 'age']]\ny = df['price']\n\n# Convert to numpy arrays\nX_np = X.to_numpy()\ny_np = y.to_numpy()\n\n# Add column of ones for intercept term\nX_np = np.column_stack([np.ones(len(X_np)), X_np])\n\n# Calculate least squares coefficients: β = (X^T X)^{-1} X^T y\nbeta = np.linalg.inv(X_np.T @ X_np) @ X_np.T @ y_np\n\n\n\n8.5.4 Helpful Data Cleaning Functions\n\ndf.dropna(): Remove rows with missing values\ndf.fillna(value): Fill missing values\ndf.describe(): Summary statistics\ndf['column'].value_counts(): Count unique values",
    "crumbs": [
      "Least Squares",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "parts/eigenvalues.html",
    "href": "parts/eigenvalues.html",
    "title": "Eigenvalues and Eigenvectors",
    "section": "",
    "text": "This section covers determinats, eigenvalues and eigenvectors\n\nDeterminants\nEigenvalues and Eigenvectors\nLab: Visualizing Quadratic Forms",
    "crumbs": [
      "Eigenvalues and Eigenvectors"
    ]
  },
  {
    "objectID": "chapters/determinants.html",
    "href": "chapters/determinants.html",
    "title": "9  Determinants",
    "section": "",
    "text": "9.1 Axioms\nDeterminants are a fundamental concept in linear algebra, associated with square matrices. They have various applications, such as:\nDeterminants are functions that assign a unique scalar value to each square matrix. We denote the determinant of a matrix \\(A\\) as \\(\\det(A)\\) or \\(|A|\\). The axioms that characterize determinants are:\n\\[|I_n| = 1,\\quad\\text{for every }n\\in\\mathbb{N},\\]\nwhere \\(I_n\\) is the \\(n\\times n\\) identity matrix.\nThe three axioms uniquely define the determinant function, even though this may not be immediately apparent. It can be shown that these axioms lead to a formula for the determinant of an \\(n \\times n\\) matrix \\(A\\): \\[\\det(A) = \\sum_{\\pi \\in S_n} \\text{sgn}(\\pi) , a_{1\\pi(1)} , a_{2\\pi(2)} , \\cdots , a_{n\\pi(n)}\\] where:\nHowever, this formula is not practical for calculating determinants, as there are \\(n!\\) permutations in \\(S_n\\), leading to a very large number of terms in the sum as \\(n\\) increases.\nInstead, we will see that the axioms are useful for keeping track of elementary row operations. This property allows us to compute determinants efficiently and prove various properties about them, without relying on the permutation formula directly.\nWe can also view the determinant on \\(n\\times n\\) matrices as a function that maps an ordered set of \\(n\\) vectors in \\(\\mathbb{R}^n\\) to a real number: \\[\\det: \\mathbb{R}^n \\times \\mathbb{R}^n \\times \\cdots \\times \\mathbb{R}^n \\to \\mathbb{R}\\] Here, we interpret each element of \\(\\mathbb{R}^n\\) as a row of the matrix, meaning that the determinant takes \\(𝑛\\) row vectors from \\(\\mathbb{R}^n\\) and returns a scalar. If an \\(n\\times n\\) matrix is represented in terms of its rows: \\[A = \\begin{bmatrix}\n\\leftarrow &\\mathbf{r}_1 &\\rightarrow\\\\\n\\leftarrow &\\mathbf{r}_2 & \\rightarrow\\\\\n&\\vdots & \\\\\n\\leftarrow & \\mathbf{r}_n & \\rightarrow\n\\end{bmatrix}\\quad\\text{then}\\quad\\det(A)=\\det(\\mathbf{r}_1,\\mathbf{r}_2,\\dots,\\mathbf{r}_n).\\]\nUsing this notation, the identity axiom states that: \\[\\det(\\mathbf{e}_1,\\mathbf{e}_2,\\cdots,\\mathbf{e}_n)=1\\] where \\({\\mathbf{e}_1,\\mathbf{e}_2,\\cdots,\\mathbf{e}_n}\\) is the standard basis of \\(\\mathbb{R}^n\\), defined as: \\[\\mathbf{e}_1=\\begin{bmatrix}1\\\\0\\\\ \\vdots\\\\0\\end{bmatrix},\\quad\n\\mathbf{e}_2=\\begin{bmatrix}0\\\\1\\\\ \\vdots\\\\0\\end{bmatrix},\\quad\n\\cdots,\\quad\n\\mathbf{e}_n=\\begin{bmatrix}0\\\\0\\\\ \\vdots\\\\1\\end{bmatrix}.\\]",
    "crumbs": [
      "Eigenvalues and Eigenvectors",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Determinants</span>"
    ]
  },
  {
    "objectID": "chapters/determinants.html#axioms",
    "href": "chapters/determinants.html#axioms",
    "title": "9  Determinants",
    "section": "",
    "text": "Identity Axiom: The determinant of the identity matrix is 1.\n\n\n\n\nLinearity Axiom: The determinant is a linear function of each row separately.\n\n\n(scalar multiplication) If a row is multiplied by a scalar, the determinant is multiplied by the same scalar.\n(additivity) \\[\\det\n  \\begin{pmatrix}\n\\leftarrow & \\mathbf{r_1} & \\rightarrow \\\\\n& \\vdots & \\\\\n\\leftarrow & \\mathbf{x} + \\mathbf{y} & \\rightarrow \\\\\n& \\vdots & \\\\\n\\leftarrow & \\mathbf{r_n} & \\rightarrow\n\\end{pmatrix}\n=\n\\det \\begin{pmatrix}\n\\leftarrow & \\mathbf{r_1} & \\rightarrow \\\\\n& \\vdots & \\\\\n\\leftarrow & \\mathbf{x} & \\rightarrow \\\\\n& \\vdots & \\\\\n\\leftarrow & \\mathbf{r_n} & \\rightarrow\n\\end{pmatrix} +\\det\n\\begin{pmatrix}\n\\leftarrow & \\mathbf{r_1} & \\rightarrow \\\\\n& \\vdots & \\\\\n\\leftarrow & \\mathbf{y} & \\rightarrow \\\\\n& \\vdots & \\\\\n\\leftarrow & \\mathbf{r_n} & \\rightarrow\n\\end{pmatrix}.\\]\n\n\nAlternating Property Axiom: If two rows are swapped, the determinant changes sign.\n\n\n\n\n\\(S_n\\) is the set of all permutations of the indices \\({1, 2, \\dots, n}\\)\n\\(\\text{sgn}(\\pi)\\) is the sign of the permutation \\(\\pi\\), which is +1 for even permutations and -1 for odd permutations\n\\(a_{i\\pi(i)}\\) is the entry in the \\(i\\)-th row and \\(\\pi(i)\\)-th column of \\(A\\)",
    "crumbs": [
      "Eigenvalues and Eigenvectors",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Determinants</span>"
    ]
  },
  {
    "objectID": "chapters/determinants.html#properties-of-determinant",
    "href": "chapters/determinants.html#properties-of-determinant",
    "title": "9  Determinants",
    "section": "9.2 Properties of determinant",
    "text": "9.2 Properties of determinant\nFrom the axioms we infer many properties of determinants:\n\nIf \\(A\\) has a zero row, then \\(\\det(A) = 0\\)\n\n\nLet \\(A\\) be a matrix with a zero row in position \\(i\\). By the Linearity Axiom in row \\(i\\), for any scalar \\(c\\),\n\\[\\begin{align}\n\\det(A) &= \\det(\\mathbf{r}_1,\\ldots,\\mathbf{0},\\ldots,\\mathbf{r}_n)\\\\\n& = \\det(\\mathbf{r}_1,\\ldots,c\\mathbf{0},\\ldots,\\mathbf{r}_n)\\\\\n& = c\\det(\\mathbf{r}_1,\\ldots,\\mathbf{0},\\ldots,\\mathbf{r}_n) = c\\det(A)\n\\end{align}\\] Since \\(c\\) is arbitrary, \\(\\det(A)=0\\).\n\n\nIf \\(A\\) has two equal rows, then \\(\\det(A) = 0\\)\n\n\nLet \\(A\\) be a matrix where \\(\\mathbf{r}_i = \\mathbf{r}_j\\) for some \\(i \\neq j\\). Then\n\\[\\begin{align}\n\\det(A) &= \\det(\\ldots,\\mathbf{r}_i,\\ldots,\\mathbf{r}_j,\\ldots)&&\\text{definition}\\\\\n&= -\\det(\\ldots,\\mathbf{r}_j,\\ldots,\\mathbf{r}_i,\\ldots)&&\\text{Alternating Property Axiom}\\\\\n&= -\\det(\\ldots,\\mathbf{r}_i,\\ldots,\\mathbf{r}_j,\\ldots)&&\\text{because }\\mathbf{r}_i=\\mathbf{r}_j\\\\\n&= -\\det(A)\n\\end{align}\\] Therefore, \\(\\det(A) = -\\det(A)\\), which implies \\(\\det(A) = 0\\).\n\n\nFor a scalar \\(c\\), \\(\\det(cA) = c^n\\det(A)\\)\n\n\nNote that \\(cA\\) means multiplying each row by \\(c\\). Using multilinearity \\(n\\) times:\n\\[\\begin{align}\n\\det(cA) &= \\det(c\\mathbf{r}_1,c\\mathbf{r}_2,\\ldots,c\\mathbf{r}_n)\\\\\n&= c\\det(\\mathbf{r}_1,c\\mathbf{r}_2,\\ldots,c\\mathbf{r}_n)\\\\\n&= c^2\\det(\\mathbf{r}_1,\\mathbf{r}_2,c\\mathbf{r}_3\\ldots,c\\mathbf{r}_n)\\\\\n&\\;\\;\\vdots\\\\\n&= c^n\\det(\\mathbf{r}_1,\\mathbf{r}_2,\\ldots,\\mathbf{r}_n)\\\\\n&= c^n\\det(A)\n\\end{align}\\]\n\n\nFor diagonal matrices, \\(\\det(A)\\) is the product of the diagonal terms.\n\n\nLet \\(A\\) be a diagonal matrix and \\(\\{\\mathbf{e}_1,\\dots,\\mathbf{e}_n\\}\\) be the canonical basis of \\(\\mathbb{R}^n\\). Then using linearity \\(n\\) times we get: \\[\\begin{align}\n\\det(A)&=\\det(a_{11}\\mathbf{e}_1,a_{22}\\mathbf{e}_2,\\dots,a_{nn}\\mathbf{e}_n)\\\\\n&=a_{11}\\det(\\mathbf{e}_1,a_{22}\\mathbf{e}_2,\\dots,a_{nn}e_n)\\\\\n&=a_{11}a_{22}\\det(\\mathbf{e}_1,\\mathbf{e}_2,a_{33}\\mathbf{e}_3\\dots,a_{nn}\\mathbf{e}_n)\\\\\n&\\;\\;\\vdots \\\\\n&=a_{11}a_{22}\\cdots a_{nn}\\det(\\mathbf{e}_1,\\mathbf{e}_2,\\dots,\\mathbf{e}_n)\\\\\n&=a_{11}a_{22}\\cdots a_{nn}&& \\text{By the identity axiom}\\\\\n\\end{align}\\]\n\n\nSuppose that the matrix \\(B\\) is obtained from the matrix \\(A\\) by applying the type III elementary row operation \\(c\\mathbf{r}_j+\\mathbf{r}_i\\to\\mathbf{r}_i\\), where \\(i\\neq j\\) and \\(c\\) is a scalar. This operation adds \\(c\\) times the \\(j\\)-th row to the \\(i\\)-th row. Then, \\(\\det(B)=\\det(A)\\).\n\n\n\\[\\begin{align}\n\\det(B) &= \\det(\\ldots,c\\mathbf{r}_j+\\mathbf{r}_i,\\ldots,\\mathbf{r}_j,\\ldots)&&\\text{definition of $B$}\\\\\n&= c\\det(\\ldots,\\mathbf{r}_j,\\ldots,\\mathbf{r}_j,\\ldots) +\\det(\\ldots,\\mathbf{r}_i,\\ldots,\\mathbf{r}_j,\\ldots)&&\\text{ by linearity}\\\\\n&= 0+\\det(\\ldots,\\mathbf{r}_i,\\ldots,\\mathbf{r}_j,\\ldots)&&\\text{two equal rows}\\\\\n&= \\det(A)\n\\end{align}\\]\n\nThese properties allow us to find the determinants of the elementary matrices. Suppose that \\(E\\) is an elementary matrix. If \\(E\\) multiplies row \\(i\\) by a non-zero constant \\(c\\), \\(E\\) is diagonal with \\(c\\) in the \\(i\\)-th term and 1’s elsewhere. Then \\(\\det(E)=c\\). If \\(E\\) exchanges two rows, its determinant is -1. And if \\(E\\) adds a multiple of a row to another row its determinant is 1.\nWe summarize these properties here:\n\n\n\n\n\n\nDeterminants of Elementary Matrices\n\n\n\nSuppose that \\(E\\) is an elementary matrix\n\nIf \\(E\\) multiplies a row by a non-zero constant \\(c\\), then \\(\\det(E)=c\\)\nIf \\(E\\) exchanges two rows, then \\(\\det(E)=-1\\)\nIf \\(E\\) adds a multiple of a row to another row, then \\(\\det(E)=1\\)\n\n\n\nRepeating the arguments on matrices and elementary row matrices, we obtain the following important result\n\nLemma 9.1 Suppose that \\(A\\) is an \\(n\\times n\\) matrix and that \\(E\\) is an \\(n\\times n\\) elementary matrix. Then \\[\\det(EA)=\\det(E)\\det(A)\\]\n\nLet’s illustrate this when \\(E\\) multiplies the \\(i\\)-th row by a non-zero constant \\(c\\). Then \\[\\begin{align}\n\\det(EA) &= \\det(\\mathbf{r}_1,\\cdots,\\mathbf{r}_{i-1},c\\mathbf{r}_i,\\mathbf{r}_{i+1},\\cdots,\\mathbf{r}_n)\\\\\n&= c\\det(\\mathbf{r}_1,\\cdots,\\mathbf{r}_{i-1},\\mathbf{r}_i,\\mathbf{r}_{i+1},\\cdots,\\mathbf{r}_n)\\\\\n&=\\det(E)\\det(A)\n\\end{align}.\\] A similar argument is used for the other elementary matrices.\nWe use the Lemma to characterize invertibility of matrices in terms of determinants\n\nTheorem 9.1 Let \\(A\\) be an \\(n\\times n\\) matrix. Then \\(A\\) is invertible if and only if \\(\\det(A)\\not=0\\).\n\n\nProof. Let \\(A\\) be an \\(n\\times n\\) matrix with row reduced echelon form \\(R\\). Find elementary row operations \\(E_i\\) such that \\[E_k\\cdots E_2E_1A=R.\\] Applying Lemma 9.1 repeatedly we get \\[\\det(E_k)\\cdots \\det(E_2)\\det(E_1)\\det(A)=\\det(R).\\]\nIf \\(A\\) is invertible, then \\(R=I_n\\) and \\(\\det(E_k)\\cdots \\det(E_2)\\det(E_1)\\det(A)=\\det(I_n)=1,\\) which implies that \\(\\det(A)\\not=0\\).\nOn the other hand, if \\(A\\) is not invertible, \\(R\\) has a row of zeros and then \\(\\det(R)=0\\). Hence, \\(\\det(E_k)\\cdots \\det(E_2)\\det(E_1)\\det(A)=\\det(R)=0\\). Since \\(\\det(E_i)\\not=0\\) for \\(i\\leq k\\), we get that \\(\\det(A)=0\\). \\(\\square\\)\n\nLemma 9.1 and Theorem 9.1 have several consequences:\n\nCorollary 9.1 Let \\(A\\) and \\(B\\) be \\(n\\times n\\) matrices, then \\(\\det(AB)=\\det(A)\\det(B)\\)\n\nIndeed, if \\(A\\) and \\(B\\) are invertible, then each of them can be written as a product of elementary matrices, and the result follows from Lemma 9.1. On the other hand, if either \\(A\\) or \\(B\\) is not invertible, then \\(AB\\) is not invertible either and \\(\\det(AB)=0\\). Since \\(\\det(A)=0\\) or \\(\\det(B)=0\\), we see that \\(\\det(A)\\det(B)=0\\) as well.\nIf \\(A\\) is invertible, \\(A^{-1}A=I_n\\) and then \\(1=\\det(I_n)=\\det(A^{-1}A)=\\det(A^{-1})\\det(A)\\) and we get:\n\nCorollary 9.2 Suppose that \\(A\\) is an invertible be \\(n\\times n\\) matrix. Then \\[\\det(A^{-1}) =\\frac{1}{\\det(A)}.\\]\n\nLemma 9.1 and Theorem 9.1 also allow us to compute the determinant efficiently when the matrix is in echelon form, or if it is upper or lower triangular.\n\nCorollary 9.3 Suppose that \\(A\\) is an upper triangular (or lower triangular) matrix. Then \\[\\det(A)=a_{11}a_{22}\\cdots a_{nn}\\]\n\n\nProof. Let \\(A\\) be an \\(n\\times n\\) upper triangular matrix. We consider two cases:\nCase 1: If any of the diagonal entries \\(a_{ii}\\) is zero, then \\(A\\) has fewer than \\(n\\) pivots and is not invertible. By Theorem 9.1, \\(\\det(A)=0\\). Since one of the diagonal entries is zero, the product \\(a_{11}a_{22}\\cdots a_{nn}\\) is also zero.\nCase 2: If all of the diagonal entries \\(a_{ii}\\) are non-zero, then \\(A\\) is already in row echelon form. We can apply elementary row operations of type III (multiplying a multiple of a row to another row) to transform \\(A\\) into a diagonal matrix \\(D\\) with the same diagonal entries as \\(A\\). By the properties of determinants, type III elementary row operations do not change the determinant. Therefore, \\(\\det(A)=\\det(D)\\).\nThe determinant of a diagonal matrix \\(D\\) is the product of its diagonal entries and since \\(A\\) and \\(D\\) have the same diagonals, \\[\\det(A)=\\det(D)=a_{11}a_{22}\\cdots a_{nn}.\\quad \\square\\]",
    "crumbs": [
      "Eigenvalues and Eigenvectors",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Determinants</span>"
    ]
  },
  {
    "objectID": "chapters/eigenvalues.html",
    "href": "chapters/eigenvalues.html",
    "title": "10  Eigenvalues and Eigenvectors",
    "section": "",
    "text": "10.1 Introduction\nWatch the following video from the Essence of Linear Algebra, from 3Blue1Brown\nIn this section we study eigenvalues and eigenvectors.\nChecking whether a nonzero vector \\(\\mathbf{x}\\) is an eigenvector of a matrix \\(A\\) is straightforward: we compute \\(A\\mathbf{x}\\) and verify if the result is a scalar multiple of \\(\\mathbf{x}\\).\nDetermining whether a scalar \\(\\lambda\\) is an eigenvalue requires solving a system of equations. The key observation is that the equation \\(A\\mathbf{x} = \\lambda\\mathbf{x}\\) is equivalent to\n\\[\n(A - \\lambda I)\\mathbf{x} = \\mathbf{0},\n\\] where \\(I\\) is the identity matrix. For \\(\\lambda\\) to be an eigenvalue, this system must have a nonzero solution, meaning \\(A - \\lambda I\\) must be singular. Then we solve:\n\\[\n(A - \\lambda I)\\mathbf{x} = \\mathbf{0}.\n\\tag{10.1}\\] If there are infinitely many solutions, \\(\\lambda\\) is an eigenvalue, and all nonzero solutions are eigenvectors. If the system has only the trivial solution \\(\\mathbf{x} = \\mathbf{0}\\), then \\(\\lambda\\) is not an eigenvalue.",
    "crumbs": [
      "Eigenvalues and Eigenvectors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Eigenvalues and Eigenvectors</span>"
    ]
  },
  {
    "objectID": "chapters/eigenvalues.html#introduction",
    "href": "chapters/eigenvalues.html#introduction",
    "title": "10  Eigenvalues and Eigenvectors",
    "section": "",
    "text": "Definition 10.1 Let \\(A\\) be an \\(n\\times n\\) matrix. A non-zero vector \\(\\mathbf{x}\\in\\mathbb{R}^n\\) is an eigenvector of \\(A\\) if \\(A\\mathbf{x}=\\lambda\\mathbf{x}\\) for some \\(\\lambda\\in\\mathbb{R}\\). A scalar \\(\\lambda\\in\\mathbb{R}\\) is an eigenvalue of \\(A\\) if there exists a non-zero vector \\(\\mathbf{x}\\in\\mathbb{R}^n\\) such that \\(A\\mathbf{x}=\\lambda\\mathbf{x}\\).\n\n\n\n\nExercise: Suppose that \\(A=\\begin{bmatrix}7&-2\\\\5&0\\end{bmatrix}\\), \\(\\mathbf{x}_1=\\begin{bmatrix}2\\\\1\\end{bmatrix}\\) and \\(\\mathbf{x}_2=\\begin{bmatrix}1\\\\1\\end{bmatrix}\\). Answer the following problems and click to check the answers\n\nAre \\(\\mathbf{x}_1\\) or \\(\\mathbf{x}_2\\) eigenvectors of \\(A\\)?\nIs \\(\\lambda=2\\) an eigenvalue of \\(A\\)?\nIs \\(\\lambda=3\\) an eigenvalue of \\(A\\)?\n\n\n\n\n\n\n\n\nClick to see the answer of 1\n\n\n\n\n\nRemember that we need to check that \\(A\\mathbf{x}\\) is a multiple of \\(\\mathbf{x}\\).\nWe compute \\(A\\mathbf{x}_1=\\begin{bmatrix}7&-2\\\\5&0\\end{bmatrix}\\begin{bmatrix}2\\\\1\\end{bmatrix}=\\begin{bmatrix}12\\\\10\\end{bmatrix}\\) and see that it is not a multiple of \\(\\begin{bmatrix}2\\\\1\\end{bmatrix}\\). Therefore, \\(\\mathbf{x}_1\\) is not an eigenvector of \\(A\\).\nNow we compute \\(A\\mathbf{x}_2=\\begin{bmatrix}7&-2\\\\5&0\\end{bmatrix}\\begin{bmatrix}1\\\\1\\end{bmatrix}=\\begin{bmatrix}5\\\\5\\end{bmatrix}\\) and notice that this is \\(5\\begin{bmatrix}1\\\\1\\end{bmatrix}\\). Therefore \\(\\mathbf{x}_2\\) is an eigenvector of \\(A\\) with eigenvalue \\(5\\).\n\n\n\n\n\n\n\n\n\nClick to see the answer of 2\n\n\n\n\n\nFrom Equation 10.1, we find \\(A-2 I=\\begin{bmatrix}5&-2\\\\5&-2\\end{bmatrix}\\) and solve the augmented system \\(\\left[\\begin{array}{cc|c}5&-2&0\\\\5&-2&0\\end{array}\\right]\\). This system has infinitely many solutions and we conclude that \\(\\lambda=2\\) is an eigenvalue of \\(A\\).\nThe solutions are multiples of \\(\\begin{bmatrix}\\frac{2}{5}\\\\1\\end{bmatrix}\\).\n\n\n\n\n\n\n\n\n\nClick to see the answer of 3\n\n\n\n\n\nFrom Equation 10.1, we find \\(A-3 I=\\begin{bmatrix}4&-2\\\\5&-3\\end{bmatrix}\\) and solve the augmented system \\(\\left[\\begin{array}{cc|c}4&-2&0\\\\5&-3&0\\end{array}\\right]\\). This system has a unique solution and we conclude that \\(\\lambda=3\\) is not an eigenvalue of \\(A\\).",
    "crumbs": [
      "Eigenvalues and Eigenvectors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Eigenvalues and Eigenvectors</span>"
    ]
  },
  {
    "objectID": "chapters/eigenvalues.html#finding-eigenvalues",
    "href": "chapters/eigenvalues.html#finding-eigenvalues",
    "title": "10  Eigenvalues and Eigenvectors",
    "section": "10.2 Finding Eigenvalues",
    "text": "10.2 Finding Eigenvalues\nTo find the eigenvalues of a matrix \\(A\\), we can use the following chain of equivalent statements:\n\\[\\begin{align}\n\\lambda\\text{ is an eigenvalue of }A\n&\\Leftrightarrow A\\mathbf{x}=\\lambda\\mathbf{x}\\text{ for some }\\mathbf{x}\\not=0\\\\\n&\\Leftrightarrow A\\mathbf{x}-\\lambda\\mathbf{x}=\\mathbf{0}\\text{ for some }\\mathbf{x}\\not=0\\\\\n&\\Leftrightarrow (A-\\lambda I)\\mathbf{x}=\\mathbf{0}\\text{ for some }\\mathbf{x}\\not=0\\\\\n&\\Leftrightarrow (A-\\lambda I) \\text{ is not invertible }\\\\\n&\\Leftrightarrow \\det(A-\\lambda I)=0 \\\\\n\\end{align}\n\\]\nWe compute \\(\\det(A-\\lambda I)\\), which for an \\(n\\times n\\) matrix is a polynomial of degree \\(n\\) named the characteristic polynomial of \\(A\\), and then we find the roots.\n\nSince the characteristic polynomial has degree \\(n\\), it has at most \\(n\\) roots. This implies that an \\(n\\times n\\) matrix has at most \\(n\\) eigenvalues.\n\nIf \\(A\\) is an upper triangular matrix \\[A=\\begin{bmatrix}\na_{11} & a_{12} & a_{13} & \\cdots & a_{1n} \\\\\n0 & a_{22} & a_{23} & \\cdots & a_{2n} \\\\\n0 & 0 & a_{33} & \\cdots & a_{3n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & a_{nn}\n\\end{bmatrix}\\] then \\[\\det(A-\\lambda I)=(a_{11}-\\lambda)(a_{22}-\\lambda)\\cdots (a_{nn}-\\lambda)\\] and the eigenvalues of \\(A\\) are the diagonal elements. The same is true if \\(A\\) is lower triangular.\nHowever, in general, it is not so easy to find the roots of the characteristic polynomial, especially as the size of the matrix increases.\nFor a \\(2 \\times 2\\) matrix, we can find the roots using the quadratic formula. For a \\(3 \\times 3\\) matrix, we could potentially use the cubic formula, though it’s rather complicated. For a \\(4 \\times 4\\) matrix, there is a quartic formula, but it’s extremely complex and rarely used in practice.\nFor matrices of size \\(5 \\times 5\\) or larger, the Abel-Ruffini theorem from abstract algebra proves that there is no general algebraic formula for the roots of polynomials of degree five or higher. Therefore, for larger matrices, we typically rely on numerical methods such the power method.\nWe can also graph the characteristic polynomial to get an idea where the eigenvalues are. Consider\n\\[A=\\begin{bmatrix}0.325 & -0.075 & 0.075 & -0.075\\\\\n0.025 & 0.225 & -0.025 & -0.275\\\\0.15 & -0.05 & 0.25 & -0.05\\\\\n-0.1 & -0.1 & 0.1 & 0.4\\end{bmatrix}\n\\]\nWe use sympy to find the characteristic polynomial \\[\\det(A-\\lambda I)=\nx^{4} - \\frac{6 x^{3}}{5} + \\frac{49 x^{2}}{100} - \\frac{39 x}{500} + \\frac{1}{250}\\] and then we plot it:\n\n\n                                                \n\n\n\nIn most cases, we use software like NumPy to find eigenvalues. The command np.linalg.eig(A) returns two objects:\n\nAn array containing the eigenvalues of matrix \\(A\\)\nA 2D array where each column represents the eigenvector corresponding to the eigenvalue at the same index\n\nThe eigenvectors are normalized to have unit length. If you need only the eigenvalues, you can use the more efficient np.linalg.eigvals(A) function.\nLet’s look at the eigenvalues of the matrix \\(A\\) defined above\n\n# Calculate eigenvalues and eigenvectors of matrix A\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Display the eigenvalues\nprint(\"Eigenvalues of A:\",eigenvalues)\n\nEigenvalues of A: [0.1 0.2 0.4 0.5]\n\n\nNow let’s enhance the eigenvector visualization with better formatting:\n\nfrom sympy import *\ninit_printing()\nMatrix(np.round(eigenvectors,4))\n\n\\(\\displaystyle \\left[\\begin{matrix}0.4082 & 0.0 & 0.7071 & 0.0\\\\0.8165 & -0.7071 & 0.0 & -0.7071\\\\0.0 & -0.7071 & 0.7071 & 0.0\\\\0.4082 & 0.0 & 0.0 & 0.7071\\end{matrix}\\right]\\)\n\n\n\nExercise: Check that the columns of eigenvectors are eigenvectors of \\(A\\) corresponding to the eigenvalues of eigenvalues\n\n\nA Note of Caution About Numerical Calculations\nWhen using NumPy to calculate eigenvalues and eigenvectors, it’s good to double-check the results, especially for certain types of matrices. For example, with a matrix like \\[\\begin{bmatrix}1&1\\\\0&1\\end{bmatrix},\\] we know that it has only one eigenvalue which is 1, but it has only one eigenvector up to a non-zero multiple Example 10.2. However, NumPy will try to give us two eigenvectors anyway, and the second one isn’t really an new eigenvector.\n\neigenvalues, eigenvectors = np.linalg.eig(np.array([[1,1],[0,1]]))\n\nprint(\"The eigenvalues are:\",eigenvalues)\n\nprint(\"The eigenvectors are:\",\"\\n\", eigenvectors)\n\nThe eigenvalues are: [1. 1.]\nThe eigenvectors are: \n [[ 1.00000000e+00 -1.00000000e+00]\n [ 0.00000000e+00  2.22044605e-16]]\n\n\nThis happens because NumPy uses numerical methods that sometimes have to make approximations. To check if the results make sense, you can always verify whether \\(A\\mathbf{v} = \\lambda\\mathbf{v}\\) holds true for each eigenvector and if the eigenvectors are linearly independent. In this example, the second eigenvector is \\((-1,0)\\) which is a multiple of the first eigenvector \\((1,0)\\).",
    "crumbs": [
      "Eigenvalues and Eigenvectors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Eigenvalues and Eigenvectors</span>"
    ]
  },
  {
    "objectID": "chapters/eigenvalues.html#examples",
    "href": "chapters/eigenvalues.html#examples",
    "title": "10  Eigenvalues and Eigenvectors",
    "section": "10.3 Examples",
    "text": "10.3 Examples\nIn this subsection we find several examples of eigenvalues and eigenvectors\n\n10.3.1 Do eigenvalues always exist?\nThe answer is no when working in the real number system. However, in the complex number system, the answer is yes. This is guaranteed by the fundamental theorem of algebra, which states that all non-constant polynomials with complex coefficients have at least one complex root.\n\nExample 10.1 A simple example of a matrix without real eigenvalues is the rotation matrix in \\(\\mathbb{R}^2\\) that rotates vectors by \\(\\theta\\) degrees. This matrix is represented as: \\[\\begin{bmatrix}\\cos(\\theta) & -\\sin(\\theta)\\\\\\sin(\\theta)&\\cos(\\theta) \\end{bmatrix}\\]\nWhen \\(\\theta\\) is not a multiple of \\(180°\\), this matrix has no real eigenvalues, only complex ones.\nThe characteristic polynomial of this matrix is \\[\\lambda^2-2\\cos(\\theta)+1\\] and the quadratic formula tells us that the eigenavalues are \\[\\lambda = \\cos(\\theta)\\pm i\\sin(\\theta),\\] which is a real number only if \\(\\sin(\\theta)=0\\), which happens precisely when \\(\\theta\\) is a multiple of \\(180°\\).\n\n\n\n10.3.2 Algebraic vs. Geometric Multiplicity of Eigenvalues\nThe following example is instructive.\n\nExample 10.2 Let \\(A=\\begin{bmatrix}1&1\\\\0&1\\end{bmatrix}\\). We see that \\(A\\) has only one eigenvalue \\(\\lambda=1\\) (appearing with algebraic multiplicity 2, since the characteristic polynomial is \\((1-\\lambda)^2\\)).\nWhen we find the eigenvectors associated with \\(\\lambda=1\\) by solving \\((A-\\lambda I)\\mathbf{v} = \\mathbf{0}\\):\n\\[(A-I)\\mathbf{v} = \\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\end{bmatrix}\\]\nWe get \\(v_2 = 0\\) with \\(v_1\\) free to take any value. Thus, all eigenvectors are scalar multiples of \\(\\begin{bmatrix}1\\\\0\\end{bmatrix}\\).\n\nDespite having an eigenvalue with algebraic multiplicity 2, we can only find one linearly independent eigenvector. The geometric multiplicity (dimension of the eigenspace) is only 1, which is less than the algebraic multiplicity.\nLet’s look at a couple of examples\n\nExample 10.3 Consider the 3×3 matrix: \\[A=\\begin{bmatrix}3 & -1 & 3\\\\1 & 1 & 3\\\\1 & -1 & 5\\end{bmatrix}\\]\nTo diagonalize \\(A\\), we first need to find its eigenvalues and eigenvectors. We’ll use SymPy to calculate the characteristic polynomial:\n\n# import required functions from SymPy\nfrom sympy import Matrix, Symbol, eye, det, factor\n\n# Define the matrix A\nA = Matrix([[3,-1,3],[1,1,3],[1,-1,5]]) \n\n# Define the symbol for eigenvalues \nx = Symbol('x') \n\n# Calculate the characteristic polynomial and factored form\nprint(\"The characteristic polynomial of A and its factored version are:\")\nchar_poly = det(A-x*eye(3)) #eye(3) is the 3x3 identity matrix\nchar_poly,factor(char_poly)\n\nThe characteristic polynomial of A and its factored version are:\n\n\n\\(\\displaystyle \\left( - x^{3} + 9 x^{2} - 24 x + 20, \\  - \\left(x - 5\\right) \\left(x - 2\\right)^{2}\\right)\\)\n\n\nThe output tells us the eigenvalues are \\(\\lambda = 5\\) (with multiplicity 1) and \\(\\lambda = 2\\) (with multiplicity 2).\nNext we find the eigenvectors associated with each eigenvalue. For \\(\\lambda = 2\\), we find a basis for the null space of \\((A-2I)\\):\n\n# Find eigenvectors for λ = 2\nprint(\"Eigenvectors for λ = 2:\")\n(A-2*eye(3)).nullspace()\n\nEigenvectors for λ = 2:\n\n\n\\(\\displaystyle \\left[ \\left[\\begin{matrix}1\\\\1\\\\0\\end{matrix}\\right], \\  \\left[\\begin{matrix}-3\\\\0\\\\1\\end{matrix}\\right]\\right]\\)\n\n\nFor \\(\\lambda = 5\\), we find a basis of \\((A- 5I)\\):\n\n# Find eigenvectors for λ = 5\nprint(\"Eigenvectors for λ = 5:\")\n(A-5*eye(3)).nullspace()\n\nEigenvectors for λ = 5:\n\n\n\\(\\displaystyle \\left[ \\left[\\begin{matrix}1\\\\1\\\\1\\end{matrix}\\right]\\right]\\)\n\n\nFrom these calculations, we obtain three linearly independent eigenvectors: \\[\\left\\{\\begin{bmatrix}1\\\\1\\\\0\\end{bmatrix}, \\begin{bmatrix}-3\\\\0\\\\1\\end{bmatrix}, \\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\\right\\}\\] that form a basis for \\(\\mathbb{R}^3\\).\n\nLet’s look at another example with two eigenvalues\n\nExample 10.4 Consider the 3×3 matrix: \\[A = \\begin{bmatrix}2 & 4 & 3\\\\-4 & -6 & -3\\\\3 & 3 & 1\\end{bmatrix}\\]\nUsing sympy, we find that the characteristic polynomial of \\(A\\) is: \\[\\det(A-\\lambda I)=-\\lambda^3-3\\lambda^2+4=-(\\lambda-1)(\\lambda+2)^2\\]\nThis tells us the eigenvalues are \\(\\lambda = 1\\) (with multiplicity 1) and \\(\\lambda = -2\\) (with multiplicity 2).\nTo find the eigenvectors for \\(\\lambda = -2\\), we find a basis for \\(\\text{nul}(A+2I)\\): \\(\\begin{bmatrix}-1\\\\1\\\\0\\end{bmatrix}\\).\nFor the eigenvalue \\(\\lambda = 1\\), a basis for \\(\\text{nul}(A-1I)\\): \\(\\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}\\).\n\nIn this case we only got two linearly independent eigenvectors and we didn’t have enough eigenvectors to get a basis of \\(\\mathbb{R}^3\\).",
    "crumbs": [
      "Eigenvalues and Eigenvectors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Eigenvalues and Eigenvectors</span>"
    ]
  },
  {
    "objectID": "chapters/eigenvalues.html#diagonalization",
    "href": "chapters/eigenvalues.html#diagonalization",
    "title": "10  Eigenvalues and Eigenvectors",
    "section": "10.4 Diagonalization",
    "text": "10.4 Diagonalization\nWe now explore one of the most significant applications of eigenvalues and eigenvectors: determining when a matrix has enough eigenvectors to form a basis of \\(\\mathbb{R}^n\\).\n\nDefinition 10.2 An \\(n\\times n\\) matrix \\(A\\) is said to be diagonalizable if there exists a basis of \\(\\mathbb{R}^n\\) consisting of eigenvector of \\(A\\).\n\nA key result supporting this definition is the following theorem:\n\nTheorem 10.1 Let \\(A\\) be an \\(n\\times n\\) matrix. Suppose that \\(A\\) has distinct eigenvalues \\(\\lambda_1,\\lambda_2,\\dots,\\lambda_k\\) with corresponding eigenvectors \\(\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\). Then the set of eigenvectors \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\}\\) is linearly independent.\n\n\nProof. We proceed by contradiction. Suppose that the eigenvalues are distinct but the set of eigenvectors \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\}\\) is linearly dependent.\nLet \\(j\\) be the smallest index such that \\(\\mathbf{v}_j\\) can be expressed as a linear combination of the previous eigenvectors. This means \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_{j-1}\\}\\) is linearly independent, but \\(\\mathbf{v}_j\\) can be written as: \\[\\mathbf{v}_j = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_{j-1}\\mathbf{v}_{j-1}\\] for some scalars \\(c_1, c_2, \\dots, c_{j-1}\\).\nNow, applying \\(A\\) to both sides of this equation: \\[\\begin{align}\nA\\mathbf{v}_j &= A(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_{j-1}\\mathbf{v}_{j-1}) \\\\\n&= c_1 A\\mathbf{v}_1 + c_2 A\\mathbf{v}_2 + \\cdots + c_{j-1} A\\mathbf{v}_{j-1} \\\\\n&= c_1 \\lambda_1 \\mathbf{v}_1 + c_2 \\lambda_2 \\mathbf{v}_2 + \\cdots + c_{j-1} \\lambda_{j-1} \\mathbf{v}_{j-1}\n\\end{align}\\]\nSince \\(\\mathbf{v}_j\\) is an eigenvector of \\(A\\) with eigenvalue \\(\\lambda_j\\), we also have: \\[A\\mathbf{v}_j = \\lambda_j \\mathbf{v}_j = \\lambda_j(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_{j-1}\\mathbf{v}_{j-1})\\]\nComparing these two expressions for \\(A\\mathbf{v}_j\\) and subtracting: \\[\\begin{align}\n\\mathbf{0} &= c_1\\lambda_1\\mathbf{v}_1 + c_2\\lambda_2\\mathbf{v}_2 + \\cdots + c_{j-1}\\lambda_{j-1}\\mathbf{v}_{j-1} - \\lambda_j(c_1\\mathbf{v}_1 + \\cdots + c_{j-1}\\mathbf{v}_{j-1}) \\\\\n&= c_1(\\lambda_1 - \\lambda_j)\\mathbf{v}_1 + c_2(\\lambda_2 - \\lambda_j)\\mathbf{v}_2 + \\cdots + c_{j-1}(\\lambda_{j-1} - \\lambda_j)\\mathbf{v}_{j-1}\n\\end{align}\\]\nSince \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_{j-1}\\}\\) is linearly independent, each coefficient must be zero: \\[c_i(\\lambda_i - \\lambda_j) = 0 \\quad \\text{for all } i \\in \\{1, 2, \\dots, j-1\\}\\]\nBut the eigenvalues are distinct, so \\(\\lambda_i - \\lambda_j \\neq 0\\) for all \\(i \\neq j\\). This forces \\(c_i = 0\\) for all \\(i \\in \\{1, 2, \\dots, j-1\\}\\). Therefore, \\(\\mathbf{v}_j = \\mathbf{0}\\), which contradicts the definition of an eigenvector. Hence, the set of eigenvectors must be linearly independent. \\(\\square\\)\n\nThis leads to an immediate corollary\n\nCorollary: An \\(n\\times n\\) matrix with \\(n\\) different eigenvalues is diagonalizable.\n\n\n10.4.1 The Diagonalization Formula\nSuppose that the \\(n\\times n\\) matrix \\(A\\) has a basis of eigenvectors \\(S=\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) with corresponding eigenvalues \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\), such that \\(A\\mathbf{v}_i=\\lambda_i\\mathbf{v}_i\\) for each \\(i\\).\nLet \\(\\mathbf{x}\\in\\mathbb{R}^n\\). We can express \\(\\mathbf{x}\\) as a linear combination of the eigenvectors in \\(S\\): \\[\\begin{align}\n\\mathbf{x}&=c_1\\mathbf{v}_1+c_2\\mathbf{v}_2+\\cdots+c_n\\mathbf{v}_n\\\\\n&= \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{v}_1 & \\mathbf{v}_2 & \\cdots & \\mathbf{v}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\\begin{bmatrix}c_1\\\\c_2\\\\\\vdots\\\\c_n\\end{bmatrix}=P\\mathbf{c}\n\\end{align}\\]\nwhere \\(P\\) is the \\(n\\times n\\) matrix with the eigenvectors as columns and \\(\\mathbf{c}=[\\mathbf{x}]_S\\) is the coordinate vector of \\(\\mathbf{x}\\) with respect to basis \\(S\\). Since the columns of \\(P\\) are linearly independent, then \\(P\\) is invertible and \\(\\mathbf{c}=[\\mathbf{x}]_S=P^{-1}\\mathbf{x}\\).\nWhen we apply \\(A\\) to \\(\\mathbf{x}\\), we get:\n\\[\\begin{align}\nA\\mathbf{x}&=A(c_1\\mathbf{v}_1+c_2\\mathbf{v}_2+\\cdots+c_n\\mathbf{v}_n)\\\\\n&=c_1A\\mathbf{v}_1+c_2A\\mathbf{v}_2+\\cdots+c_nA\\mathbf{v}_n\\\\\n&=c_1\\lambda_1\\mathbf{v}_1+c_2\\lambda_2\\mathbf{v}_2+\\cdots+c_n\\lambda_n\\mathbf{v}_n\\\\\n&= \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{v}_1 & \\mathbf{v}_2 & \\cdots & \\mathbf{v}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\\begin{bmatrix}\\lambda_1c_1\\\\\\lambda_2c_2\\\\\\vdots\\\\\\lambda_nc_n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{v}_1 & \\mathbf{v}_2 & \\cdots & \\mathbf{v}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\n\\begin{bmatrix}\\lambda_1&0&\\cdots&0\\\\ 0&\\lambda_2&\\cdots&0\\\\\\vdots&\\vdots&\\ddots&\\vdots\\\\0&0&\\cdots&\\lambda_n\\end{bmatrix}\n\\begin{bmatrix}c_1\\\\c_2\\\\\\vdots\\\\c_n\\end{bmatrix}\\\\\n&= P\\Lambda\\mathbf{c} = P\\Lambda[\\mathbf{x}]_S\\\\\n\\end{align}\\] where \\(\\Lambda\\) is the \\(n\\times n\\) diagonal matrix of eigenvalues.\nCombining both formulas, we get that for any \\(\\mathbf{x}\\in\\mathbb{R}^n\\): \\[AP[\\mathbf{x}]_S=A\\mathbf{x}=P\\Lambda[\\mathbf{x}]_S.\\]\nSince this holds for any vector \\(\\mathbf{x}\\), and \\([\\mathbf{x}]_S\\) represents any vector in \\(\\mathbb{R}^n\\) (as \\(S\\) is a basis), we conclude that: \\[AP=P\\Lambda.\\]\nThis important formula can be rewritten as: \\[A=P\\Lambda P^{-1}\\quad\\text{or equivalently}\\quad P^{-1}AP=\\Lambda.\\]\nThis formula provides remarkable computational advantages when working with powers of \\(A\\). For example, computing the cube of \\(A\\): \\[\\begin{align}\nA^3 &= A \\cdot A \\cdot A \\\\\n&= (P\\Lambda P^{-1})(P\\Lambda P^{-1})(P\\Lambda P^{-1}) \\\\\n&= P\\Lambda P^{-1}P\\Lambda P^{-1}P\\Lambda P^{-1} \\\\\n&= P\\Lambda \\underbrace{P^{-1}P}_{=I} \\Lambda \\underbrace{P^{-1}P}_{=I} \\Lambda P^{-1} \\\\\n&= P\\Lambda \\cdot I \\cdot \\Lambda \\cdot I \\cdot \\Lambda P^{-1} \\\\\n&= P\\Lambda \\Lambda \\Lambda P^{-1} \\\\\n&= P\\Lambda^3 P^{-1}\n\\end{align}\\]\nMore generally, for any positive integer \\(k\\): \\[A^k = P\\Lambda^k P^{-1}\\]\nwhere \\(\\Lambda^k\\) is the diagonal matrix with entries \\(\\lambda_i^k\\): \\[\\Lambda^k = \\begin{bmatrix}\n\\lambda_1^k & 0 & \\cdots & 0 \\\\\n0 & \\lambda_2^k & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\lambda_n^k\n\\end{bmatrix}\\]\nThis result dramatically simplifies calculations involving matrix powers, making diagonalization a powerful technique in linear algebra.\n\n\n10.4.2 The Spectral Theorem\nWhile the diagonalization formula \\(A = P\\Lambda P^{-1}\\) applies when a matrix has a complete set of eigenvectors, the spectral theorem provides an even more powerful result for a special class of matrices: symmetric matrices.\nRecall that a matrix \\(A\\) is symmetric if \\(A = A^T\\). The spectral theorem states that every symmetric matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) is diagonalizable in a very strong sense:\n\n\\(A\\) has \\(n\\) real eigenvalues (counting multiplicities)\n\\(A\\) has an orthonormal basis of eigenvectors\n\\(A\\) can be orthogonally diagonalized as \\(A = QDQ^T\\) where \\(Q\\) is an orthogonal matrix (\\(Q^T = Q^{-1}\\))\n\n\nEigenvectors of symmetric matrices corresponding to distinct eigenvalues are orthogonal, which helps explain why the spectral theorem works.\nTheorem: Let \\(A\\) be a symmetric matrix. If \\(\\lambda_i\\) and \\(\\lambda_j\\) are distinct eigenvalues with corresponding eigenvectors \\(\\mathbf{v}_i\\) and \\(\\mathbf{v}_j\\), then \\(\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0\\).\nProof: \\[\\begin{align}\n\\lambda_i(\\mathbf{v}_i \\cdot \\mathbf{v}_j) &= \\lambda_i\\mathbf{v}_i^T\\mathbf{v}_j = (A\\mathbf{v}_i)^T\\mathbf{v}_j = \\mathbf{v}_i^TA^T\\mathbf{v}_j\\\\\n&= \\mathbf{v}_i^TA\\mathbf{v}_j = \\mathbf{v}_i^T(\\lambda_j\\mathbf{v}_j) = \\lambda_j(\\mathbf{v}_i \\cdot \\mathbf{v}_j)\n\\end{align}\\]\nSince \\(\\lambda_i \\neq \\lambda_j\\), we must have \\(\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0\\). \\(\\square\\)\nIf we select orthogonal eigenvectors of a symmetric matrix \\(A\\), and we normalize them we obtain an orthonormal basis \\(\\{\\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_n\\}\\) where:\n\n\\(\\|\\mathbf{q}_i\\| = 1\\) for all \\(i\\)\n\\(\\mathbf{q}_i \\cdot \\mathbf{q}_j = 0\\) for all \\(i \\neq j\\)\n\\(A\\mathbf{q}_i = \\lambda_i\\mathbf{q}_i\\) for all \\(i\\)\n\nLet \\(Q = [\\mathbf{q}_1 \\; \\mathbf{q}_2 \\; \\cdots \\; \\mathbf{q}_n]\\) be the matrix with these orthonormal eigenvectors as columns. Then \\(Q\\) is an orthogonal matrix, meaning \\(Q^TQ = QQ^T = I\\), and thus \\(Q^{-1} = Q^T\\).\nThe spectral decomposition of \\(A\\) is then: \\[A = QDQ^T \\tag{10.2}\\]\nwhere \\(D\\) is the diagonal matrix with eigenvalues \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\) on the diagonal.\nWe can also write the spectral decomposition in the following form: \\[A = \\lambda_1\\mathbf{q}_1\\mathbf{q}_1^T + \\lambda_2\\mathbf{q}_2\\mathbf{q}_2^T + \\cdots + \\lambda_n\\mathbf{q}_n\\mathbf{q}_n^T = \\sum_{i=1}^n \\lambda_i\\mathbf{q}_i\\mathbf{q}_i^T\\]\nThis representation expresses \\(A\\) as a sum of rank-one matrices, each scaled by an eigenvalue. This form is particularly useful for applications in principal component analysis, computer graphics, and quantum mechanics.\n\n\n10.4.3 Finding the spectral decomposition in NumPy\nThe spectral theorem guarantees that every symmetric matrix can be orthogonally diagonalized. In practical applications, we need efficient and reliable computational methods to find this decomposition. NumPy provides exactly what we need through its linear algebra module.\nWe use the command np.linalg.eigh() in NumPy to find the eigenvalues and eigenvectors of symmetric matrices. Symmetric matrices are also called Hermitian (in the real case), and the “h” in “eigh” stands for that. This function is specifically optimized for symmetric matrices and guarantees real eigenvalues and orthogonal eigenvectors, while np.linalg.eig() is the general function for non-symmetric matrices that doesn’t ensure these properties.\nLet’s implement the spectral decomposition step by step:\n\nimport numpy as np\n\n# Create a symmetric matrix\nA = np.array([\n    [4, 1, 1],\n    [1, 3, 2],\n    [1, 2, 6]\n])\n\nprint(\"Original symmetric matrix A:\")\nprint(A)\nprint()\n\n# Check that A is symmetric\nprint(\"Is A symmetric?\", np.allclose(A, A.T))\nprint()\n\nOriginal symmetric matrix A:\n[[4 1 1]\n [1 3 2]\n [1 2 6]]\n\nIs A symmetric? True\n\n\n\nThe spectral theorem only applies to symmetric matrices, so it’s always good practice to verify symmetry before proceeding. The np.allclose() function accounts for potential floating-point errors when checking equality.\nNow we find the eigenvalues and eigenvectors of \\(A\\). The spectral theorem guarantees we’ll get real eigenvalues and a complete set of orthogonal eigenvectors:\n\n# Find eigenvalues and eigenvectors using NumPy\neigenvalues, eigenvectors = np.linalg.eigh(A)  # eigh is for symmetric matrices\n\nprint(\"Eigenvalues:\", eigenvalues)\nprint(\"\\nEigenvectors (as columns):\")\nprint(eigenvectors)\n\nEigenvalues: [1.88646239 3.59642438 7.51711323]\n\nEigenvectors (as columns):\n[[ 0.24556026 -0.90025898 -0.35949122]\n [-0.89393687 -0.06686651 -0.44317687]\n [ 0.37493604  0.43018908 -0.82119445]]\n\n\nNotice that in NumPy, the eigenvectors are returned as columns of the matrix. The eigh function automatically normalizes the eigenvectors to have unit length, forming an orthonormal basis. By the spectral theorem, these eigenvectors form an orthogonal matrix \\(Q\\).\nA key property of orthogonal matrices is that their transpose equals their inverse: \\(Q^T = Q^{-1}\\). Let’s verify this property:\n\n# Verify orthogonality of eigenvectors\nQ = eigenvectors\nprint(\"Q^T Q = (should be identity)\")\nprint(np.round(Q.T @ Q, 10))  # Should be identity\n\nQ^T Q = (should be identity)\n[[ 1.  0. -0.]\n [ 0.  1.  0.]\n [-0.  0.  1.]]\n\n\nThe result should be the identity matrix (within numerical precision), confirming the orthogonality of our eigenvectors.\nFor the spectral decomposition, we need a diagonal matrix \\(D\\) containing the eigenvalues:\n\n# Create diagonal matrix of eigenvalues\nD = np.diag(eigenvalues)\nprint(\"Diagonal matrix of eigenvalues D:\")\nprint(D)\n\nDiagonal matrix of eigenvalues D:\n[[1.88646239 0.         0.        ]\n [0.         3.59642438 0.        ]\n [0.         0.         7.51711323]]\n\n\nNow we can reconstruct the original matrix using the spectral decomposition formula \\(A = QDQ^T\\):\n\n# Reconstruct A using spectral decomposition: A = Q D Q^T\nA_reconstructed = Q @ D @ Q.T\nprint(\"Reconstructed A from spectral decomposition:\")\nprint(np.round(A_reconstructed, 10))  # Round to handle floating-point errors\nprint()\n\n# Verify reconstruction error\nerror = np.max(A - A_reconstructed)\nprint(f\"Reconstruction error: {error:.2e}\")\nprint()\n\nReconstructed A from spectral decomposition:\n[[4. 1. 1.]\n [1. 3. 2.]\n [1. 2. 6.]]\n\nReconstruction error: 5.33e-15\n\n\n\nThe reconstruction error should be extremely small (near machine precision), confirming that our spectral decomposition is correct.",
    "crumbs": [
      "Eigenvalues and Eigenvectors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Eigenvalues and Eigenvectors</span>"
    ]
  },
  {
    "objectID": "worksheets/quadratic_forms.html",
    "href": "worksheets/quadratic_forms.html",
    "title": "Lab",
    "section": "",
    "text": "Visualizing Quadratic Forms in \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\)\nThis lab explores quadratic forms and their geometric interpretations in 2D and 3D spaces. Quadratic forms are fundamental mathematical structures that appear in many contexts, including linear algebra, optimization, statistics, and computer graphics. In this lab, we will:\nThis lab will help solidify concepts from our Linear Algebra course, particularly:",
    "crumbs": [
      "Eigenvalues and Eigenvectors",
      "Lab"
    ]
  },
  {
    "objectID": "worksheets/quadratic_forms.html#visualizing-quadratic-forms-in-mathbbr2-and-mathbbr3",
    "href": "worksheets/quadratic_forms.html#visualizing-quadratic-forms-in-mathbbr2-and-mathbbr3",
    "title": "Lab",
    "section": "",
    "text": "Understand the connection between symmetric matrices and quadratic forms\nVisualize conic sections in \\(\\mathbb{R}^2\\) as level sets of quadratic forms\nApply the Spectral Theorem to analyze and transform quadratic forms\nExtend these concepts to quadratic surfaces in \\(\\mathbb{R}^3\\)\n\n\n\nSymmetric matrices and their properties\nEigenvalues and eigenvectors\nThe Spectral Theorem\nLinear transformations and change of basis\n\n\nRequired Libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport sympy as sp\nfrom IPython.display import display, Markdown",
    "crumbs": [
      "Eigenvalues and Eigenvectors",
      "Lab"
    ]
  },
  {
    "objectID": "worksheets/quadratic_forms.html#part-1-quadratic-forms-in-mathbbr2",
    "href": "worksheets/quadratic_forms.html#part-1-quadratic-forms-in-mathbbr2",
    "title": "Lab",
    "section": "Part 1: Quadratic Forms in \\(\\mathbb{R}^2\\)",
    "text": "Part 1: Quadratic Forms in \\(\\mathbb{R}^2\\)\nA quadratic form in \\(\\mathbb{R}^2\\) can be written as:\n\\(q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}\\)\nwhere \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\) and \\(A\\) is a \\(2 \\times 2\\) symmetric matrix.\nWhen expanded, this gives:\n\\[q(x) = a_{11}x_1^2 + 2a_{12}x_1x_2 + a_{22}x_2^2\\]\nMake sure you understand the following helper functions:\n\nThe first function creates a random rotation, which is easy in \\(\\mathbb{R}^2\\) and harder in \\(\\mathbb{R}^3\\), and it uses the eigenvalues and the spectral theorem to create a \\(2\\times 2\\) symmetric matrix.\nThe second function looks at the eigenvalues to determine the conic type\nThe last function returns the quadratic forms in sympy for easy visualization\n\ndef create_symmetric_matrix_2d(eigenvalues, random_seed=42):\n    \"\"\"Create a 2×2 symmetric matrix with specified eigenvalues and random orientation\"\"\"\n    np.random.seed(random_seed)\n    \n    # Create a random rotation matrix (for eigenvectors)\n    theta = np.random.uniform(0, 2*np.pi)\n    Q = np.array([\n        [np.cos(theta), -np.sin(theta)],\n        [np.sin(theta), np.cos(theta)]\n    ])\n    \n    # Create the diagonal matrix of eigenvalues\n    D = np.diag(eigenvalues)\n    \n    # Apply the spectral theorem formula: A = QDQ^T\n    A = Q @ D @ Q.T\n    \n    return A, Q, D, theta\n\ndef get_conic_type(eigenvalues):\n    \"\"\"Determine the type of conic section based on eigenvalue signs\"\"\"\n    pos_count = sum(1 for ev in eigenvalues if ev &gt; 1e-10)\n    neg_count = sum(1 for ev in eigenvalues if ev &lt; -1e-10)\n    zero_count = sum(1 for ev in eigenvalues if abs(ev) &lt;= 1e-10)\n    \n    if pos_count == 2:\n        return \"ellipse\"\n    elif pos_count == 1 and neg_count == 1:\n        return \"hyperbola\"\n    elif pos_count == 1 and zero_count == 1:\n        return \"(degenerate as two parallel lines)\"\n    elif neg_count == 2:\n        return \"empty set (imaginary ellipse)\"\n    else:\n        return \"unknown\"\n\ndef display_equation(A, c=1):\n    \"\"\"Display the equation of the conic in both original and canonical form\"\"\"\n    # Define symbolic variables\n    x, y = sp.symbols('x y')\n    \n    # Extract matrix elements\n    a, b, d = A[0, 0], A[0, 1], A[1, 1]\n    \n    # Create the quadratic form expression\n    quadratic_form = a*x**2 + 2*b*x*y + d*y**2\n    \n    # Get eigenvalues for canonical form\n    eigenvalues = np.linalg.eigvalsh(A)\n    \n    # Create canonical form expression\n    x_prime, y_prime = sp.symbols(\"x' y'\")\n    canonical_form = eigenvalues[0] * x_prime**2 + eigenvalues[1] * y_prime**2\n    \n    # Return forms for display\n    return (quadratic_form,canonical_form)\nGraph the conic sections\nThis function graphs the quadratic forms and you don’t need to understand what it does, but you can.\ndef plot_conic(A, c=1, title=None, grid_size=2, num_points=1000):\n    \"\"\"Plot the conic section \\mathbf{x}^T A \\mathbf{x} = c\"\"\"\n    # Extract matrix elements\n    a, b, d = A[0, 0], A[0, 1], A[1, 1]\n    \n    # Get eigenvalues and eigenvectors to determine conic type\n    eigenvalues, eigenvectors = np.linalg.eigh(A)\n    conic_type = get_conic_type(eigenvalues)\n    \n    # Create a grid of points\n    x = np.linspace(-grid_size, grid_size, num_points)\n    y = np.linspace(-grid_size, grid_size, num_points)\n    X, Y = np.meshgrid(x, y)\n    \n    # Calculate the quadratic form\n    Z = a*X**2 + 2*b*X*Y + d*Y**2\n    \n    # Create figure\n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    # Plot the conic section\n    ax.contour(X, Y, Z, levels=[c], colors=['blue'], linewidths=3)\n    \n    # Draw the eigenvectors (principal axes)\n    colors = ['red', 'green']\n    for i in range(2):\n        # Scale eigenvector for better visualization\n        scale = 1.0 / np.sqrt(abs(eigenvalues[i]) if abs(eigenvalues[i]) &gt; 1e-10 else 1)\n        vector = eigenvectors[:, i] * scale\n        \n        eigenvalue_sign = \"+\" if eigenvalues[i] &gt; 0 else (\"-\" if eigenvalues[i] &lt; 0 else \"0\")\n        \n        ax.arrow(0, 0, vector[0], vector[1], \n                 head_width=0.1, head_length=0.1, \n                 fc=colors[i], ec=colors[i],\n                 length_includes_head=True)\n        \n        ax.text(vector[0]*1.1, vector[1]*1.1, \n                f\"λ = {eigenvalues[i]:.2f} {eigenvalue_sign}\",\n                color=colors[i])\n    \n    # Add a marker at the origin\n    ax.plot(0, 0, 'ko', markersize=8)\n    \n    # Set labels and title\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    if not title:\n        title = f\"{conic_type.replace('_', ' ').title()}\"\n    ax.set_title(title)\n    \n    # Set equal aspect ratio\n    ax.set_aspect('equal')\n    \n    # Set grid and limits\n    ax.grid(True, alpha=0.3)\n    ax.set_xlim(-grid_size, grid_size)\n    ax.set_ylim(-grid_size, grid_size)\n    \n    return fig, ax\n\nExercise 1: Create Examples\nThe signs and values of the eigenvalues determine the type of conic graph:\n\n\n\nEigenvalue Signs\nSurface Type\n\n\n\n\nAll positive\nEllipse\n\n\nOne positive and one negative\nHyperbola\n\n\nOne positive and one zero\nTwo parallel lines\n\n\nTwo negatives\nEmpty set\n\n\n\nChange the eigenvalues to obtain different shapes\n# Create examples by changing the eigenvalues\neigenvalues = [4, 1] \nA, Q, D, theta = create_symmetric_matrix_2d(eigenvalues, random_seed=42)\n\nprint(\"Symmetric Matrix A:\")\nprint(A)\nprint(\"\\nEigenvectors Q (columns):\")\nprint(Q)\nprint(\"\\nEigenvalues D:\")\nprint(D)\nprint(f\"\\nRotation angle: {np.degrees(theta):.2f} degrees\")\n\n# Verify spectral decomposition\nprint(\"\\nVerify A = QDQ^T:\")\nprint(np.round(Q @ D @ Q.T, 10))\n\n# Plot\nfig, ax = plot_conic(A)\n\n# Display the equation\noriginal, canonical = display_equation(A)\n#\ndisplay(Markdown(f\"\"\"Original form: ${sp.latex(original)} = 1$\"\"\"))\ndisplay(Markdown(f\"\"\"Canonical form: ${sp.latex(canonical)} = 1$\"\"\"))\n\n\nExercise 2:\n\nUse the functions to plot the quadratic function \\(x^2+xy+y^2=1\\). What is the rotation angle?\nCreate a quadratic form in \\(\\mathbb{R}^2\\) representing a hyperbola with axes aligned to the coordinate axes. How would you modify the matrix to rotate the hyperbola by 45 degrees counter clockwise?",
    "crumbs": [
      "Eigenvalues and Eigenvectors",
      "Lab"
    ]
  },
  {
    "objectID": "worksheets/quadratic_forms.html#part-2-quadratic-forms-in-mathbbr3",
    "href": "worksheets/quadratic_forms.html#part-2-quadratic-forms-in-mathbbr3",
    "title": "Lab",
    "section": "Part 2: Quadratic Forms in \\(\\mathbb{R}^3\\)",
    "text": "Part 2: Quadratic Forms in \\(\\mathbb{R}^3\\)\nWe can extend these concepts to 3D space, where quadratic forms create quadric surfaces.\nA quadratic form in \\(\\mathbb{R}^3\\) is: \\(q(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}\\)\nwhere \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\) and \\(A\\) is a \\(3 \\times 3\\) symmetric matrix.\nLet’s create similar helper functions for 3D.\n\nNotice how the random orthogonal matrix is created. First by creating a \\(3\\times 3\\) random matrix with independent normally distributed values and then applying the QR decomposition.\nThe second and third functions are similar to the ones in \\(\\mathbb{R}^2\\).\nThe fourth function simply applies the Spectral Theorem\n\n# Helper functions\n\ndef create_symmetric_matrix_3d(eigenvalues, random_seed=42):\n    \"\"\"Create a 3×3 symmetric matrix with specified eigenvalues and random orientation\"\"\"\n    np.random.seed(random_seed)\n    \n    # Create a random orthogonal matrix (for eigenvectors)\n    # Using QR decomposition of a random matrix\n    H = np.random.randn(3, 3)\n    Q, R = np.linalg.qr(H)\n    \n    # Make sure Q is orthogonal (det(Q) = 1)\n    if np.linalg.det(Q) &lt; 0:\n        Q[:, 0] = -Q[:, 0]\n    \n    # Create the diagonal matrix of eigenvalues\n    D = np.diag(eigenvalues)\n    \n    # Apply the spectral theorem formula: A = QDQ^T\n    A = Q @ D @ Q.T\n    \n    return A, Q, D\n\ndef get_surface_type(eigenvalues):\n    \"\"\"Determine the type of quadratic surface based on eigenvalue signs\"\"\"\n    pos_count = sum(1 for ev in eigenvalues if ev &gt; 1e-10)\n    neg_count = sum(1 for ev in eigenvalues if ev &lt; -1e-10)\n    zero_count = sum(1 for ev in eigenvalues if abs(ev) &lt;= 1e-10)\n    \n    if pos_count == 3:\n        return \"ellipsoid\"\n    elif pos_count == 2 and neg_count == 1:\n        return \"hyperboloid_one_sheet\"\n    elif pos_count == 1 and neg_count == 2:\n        return \"hyperboloid_two_sheets\"\n    elif pos_count == 2 and zero_count == 1:\n        return \"elliptic_cylinder\"\n    elif pos_count == 1 and neg_count == 1 and zero_count == 1:\n        return \"hyperbolic_cylinder\"\n    elif pos_count == 1 and zero_count == 2:\n        return \"parabolic_cylinder\"\n    else:\n        return \"unknown\"\n\ndef display_3d_equation(A, c=1):\n    \"\"\"Display the equation of the quadric in both original and canonical form\"\"\"\n    # Define symbolic variables\n    x, y, z = sp.symbols('x y z')\n    \n    # Extract matrix elements\n    a, b, c_val, d, e, f = A[0,0], A[0,1], A[0,2], A[1,1], A[1,2], A[2,2]\n    \n    # Create the quadratic form expression\n    quadratic_form = a*x**2 + 2*b*x*y + 2*c_val*x*z + d*y**2 + 2*e*y*z + f*z**2\n    \n    # Get eigenvalues for canonical form\n    eigenvalues = np.linalg.eigvalsh(A)\n    \n    # Create canonical form expression\n    x_prime, y_prime, z_prime = sp.symbols(\"x' y' z'\")\n    canonical_form = eigenvalues[0] * x_prime**2 + eigenvalues[1] * y_prime**2 + eigenvalues[2] * z_prime**2\n    \n    # Display the equations\n    return (quadratic_form,canonical_form)\n\n\ndef apply_spectral_theorem(A):\n    \"\"\"Apply the spectral theorem to decompose the matrix\"\"\"\n    eigenvalues, eigenvectors = np.linalg.eigh(A)\n    return eigenvalues, eigenvectors\nGraph the surfaces in plotly\nThese are complicated functions that we are just using.\ndef generate_surface_points(A, c=1, num_points=50, range_scale=2.0):\n    \"\"\"Generate points for the quadratic surface x^T A x = c\"\"\"\n    eigenvalues, eigenvectors = apply_spectral_theorem(A)\n    surface_type = get_surface_type(eigenvalues)\n    \n    if surface_type == \"ellipsoid\":\n        # Parametrization in spherical coordinates\n        u = np.linspace(0, 2 * np.pi, num_points)\n        v = np.linspace(0, np.pi, num_points)\n        \n        # Generate points on unit sphere\n        x_mesh = np.outer(np.cos(u), np.sin(v))\n        y_mesh = np.outer(np.sin(u), np.sin(v))\n        z_mesh = np.outer(np.ones_like(u), np.cos(v))\n        \n        # Transform and scale\n        points = np.zeros((num_points, num_points, 3))\n        for i in range(num_points):\n            for j in range(num_points):\n                # Unit sphere point\n                point = np.array([x_mesh[i, j], y_mesh[i, j], z_mesh[i, j]])\n                # Scale by 1/sqrt(eigenvalues)\n                scaled_point = point / np.sqrt(np.abs(eigenvalues))\n                # Transform to original coordinates\n                points[i, j, :] = eigenvectors @ scaled_point * np.sqrt(c)\n                \n    elif surface_type == \"hyperboloid_one_sheet\":\n        # For hyperboloid of one sheet: x²/a² + y²/b² - z²/c² = 1\n        u = np.linspace(0, 2 * np.pi, num_points)\n        v = np.linspace(-range_scale, range_scale, num_points)\n        \n        U, V = np.meshgrid(u, v)\n        \n        # Find which eigenvalue is negative\n        neg_index = np.argmin(eigenvalues)\n        pos_indices = [i for i in range(3) if i != neg_index]\n        \n        # Create array mapping eigen-indices to parameter equations\n        param_array = np.zeros((num_points, num_points, 3))\n        \n        # Set the two positive eigenvalue dimensions\n        param_array[:, :, pos_indices[0]] = np.sqrt(c / eigenvalues[pos_indices[0]]) * np.cos(U) * np.cosh(V)\n        param_array[:, :, pos_indices[1]] = np.sqrt(c / eigenvalues[pos_indices[1]]) * np.sin(U) * np.cosh(V)\n        \n        # Set the negative eigenvalue dimension\n        param_array[:, :, neg_index] = np.sqrt(c / -eigenvalues[neg_index]) * np.sinh(V)\n        \n        # Transform back to original coordinates\n        points = np.zeros((num_points, num_points, 3))\n        for i in range(num_points):\n            for j in range(num_points):\n                points[i, j, :] = eigenvectors @ param_array[i, j, :]\n        \n    elif surface_type == \"hyperboloid_two_sheets\":\n        # For hyperboloid of two sheets: x²/a² - y²/b² - z²/c² = 1\n        u = np.linspace(0, 2 * np.pi, num_points)\n        v = np.linspace(0.1, range_scale, num_points)  # Start slightly above 0 to avoid singularity\n        \n        U, V = np.meshgrid(u, v)\n        \n        # Find which eigenvalue is positive\n        pos_index = np.argmax(eigenvalues)\n        neg_indices = [i for i in range(3) if i != pos_index]\n        \n        # Create points for both sheets\n        points = np.zeros((2, num_points, num_points, 3))\n        \n        for sheet in range(2):\n            sign = 1 if sheet == 0 else -1\n            \n            # Create array mapping eigen-indices to parameter equations\n            param_array = np.zeros((num_points, num_points, 3))\n            \n            # Set the positive eigenvalue dimension (the axis of the hyperboloid)\n            param_array[:, :, pos_index] = sign * np.sqrt(c / eigenvalues[pos_index]) * np.cosh(V)\n            \n            # Set the negative eigenvalue dimensions\n            param_array[:, :, neg_indices[0]] = np.sqrt(c / -eigenvalues[neg_indices[0]]) * np.cos(U) * np.sinh(V)\n            param_array[:, :, neg_indices[1]] = np.sqrt(c / -eigenvalues[neg_indices[1]]) * np.sin(U) * np.sinh(V)\n            \n            # Transform back to original coordinates\n            for i in range(num_points):\n                for j in range(num_points):\n                    points[sheet, i, j, :] = eigenvectors @ param_array[i, j, :]\n    \n    elif surface_type == \"elliptic_cylinder\":\n        # Find which eigenvalue is zero (or close to zero)\n        zero_index = np.argmin(np.abs(eigenvalues))\n        nonzero_indices = [i for i in range(3) if i != zero_index]\n        \n        # The cylinder extends along the eigenvector of the zero eigenvalue\n        # We'll parametrize as a cylinder with elliptical cross-section\n        u = np.linspace(0, 2 * np.pi, num_points)\n        v = np.linspace(-range_scale, range_scale, num_points)\n        \n        U, V = np.meshgrid(u, v)\n        \n        # Create array mapping eigen-indices to parameter equations\n        param_array = np.zeros((num_points, num_points, 3))\n        \n        # Set the nonzero eigenvalue dimensions (the elliptical cross-section)\n        param_array[:, :, nonzero_indices[0]] = np.sqrt(c / eigenvalues[nonzero_indices[0]]) * np.cos(U)\n        param_array[:, :, nonzero_indices[1]] = np.sqrt(c / eigenvalues[nonzero_indices[1]]) * np.sin(U)\n        \n        # Set the zero eigenvalue dimension (the axis of the cylinder)\n        param_array[:, :, zero_index] = V\n        \n        # Transform back to original coordinates\n        points = np.zeros((num_points, num_points, 3))\n        for i in range(num_points):\n            for j in range(num_points):\n                points[i, j, :] = eigenvectors @ param_array[i, j, :]\n    \n    elif surface_type == \"hyperbolic_cylinder\":\n        # Find indices of positive, negative, and zero eigenvalues\n        pos_index = np.argmax(eigenvalues)\n        neg_index = np.argmin(eigenvalues)\n        zero_index = 3 - pos_index - neg_index  # The remaining index\n        \n        if abs(eigenvalues[zero_index]) &gt; 1e-10:  # Check it's actually zero\n            zero_index = neg_index if abs(eigenvalues[neg_index]) &lt; abs(eigenvalues[pos_index]) else pos_index\n            neg_index = 0 if zero_index != 0 and pos_index != 0 else 1 if zero_index != 1 and pos_index != 1 else 2\n            pos_index = 3 - zero_index - neg_index\n        \n        # The cylinder extends along the eigenvector of the zero eigenvalue\n        u = np.linspace(-range_scale, range_scale, num_points)\n        v = np.linspace(-range_scale, range_scale, num_points)\n        \n        U, V = np.meshgrid(u, v)\n        \n        # Create points for both sheets of the hyperbola\n        points = np.zeros((2, num_points, num_points, 3))\n        \n        for sheet in range(2):\n            sign = 1 if sheet == 0 else -1\n            \n            # Create array mapping eigen-indices to parameter equations\n            param_array = np.zeros((num_points, num_points, 3))\n            \n            # Set the positive eigenvalue dimension\n            param_array[:, :, pos_index] = sign * np.sqrt(c / eigenvalues[pos_index]) * np.cosh(U)\n            \n            # Set the negative eigenvalue dimension\n            param_array[:, :, neg_index] = np.sqrt(c / -eigenvalues[neg_index]) * np.sinh(U)\n            \n            # Set the zero eigenvalue dimension (the axis of the cylinder)\n            param_array[:, :, zero_index] = V\n            \n            # Transform back to original coordinates\n            for i in range(num_points):\n                for j in range(num_points):\n                    points[sheet, i, j, :] = eigenvectors @ param_array[i, j, :]\n    \n    else:\n        # For other surfaces or unknown types, return a placeholder\n        points = None\n    \n    return points, eigenvalues, eigenvectors, surface_type\n\ndef plot_quadratic_surface(A, c=1, title=None):\n    \"\"\"Plot the quadratic surface x^T A x = c\"\"\"\n    points, eigenvalues, eigenvectors, surface_type = generate_surface_points(A, c)\n    \n    if points is None:\n        print(f\"Surface type '{surface_type}' visualization not implemented\")\n        return None\n    \n    fig = go.Figure()\n    \n    if surface_type == \"ellipsoid\":\n        x = points[:, :, 0]\n        y = points[:, :, 1]\n        z = points[:, :, 2]\n        \n        fig.add_trace(go.Surface(\n            x=x, y=y, z=z,\n            colorscale='Viridis',\n            opacity=0.8,\n            showscale=False\n        ))\n        \n    elif surface_type == \"hyperboloid_one_sheet\":\n        x = points[:, :, 0]\n        y = points[:, :, 1]\n        z = points[:, :, 2]\n        \n        fig.add_trace(go.Surface(\n            x=x, y=y, z=z,\n            colorscale='Plasma',\n            opacity=0.8,\n            showscale=False\n        ))\n        \n    elif surface_type == \"hyperboloid_two_sheets\":\n        # Add both sheets\n        for sheet in range(2):\n            x = points[sheet, :, :, 0]\n            y = points[sheet, :, :, 1]\n            z = points[sheet, :, :, 2]\n            \n            fig.add_trace(go.Surface(\n                x=x, y=y, z=z,\n                colorscale='Cividis',\n                opacity=0.8,\n                showscale=False\n            ))\n    \n    elif surface_type == \"elliptic_cylinder\":\n        x = points[:, :, 0]\n        y = points[:, :, 1]\n        z = points[:, :, 2]\n        \n        fig.add_trace(go.Surface(\n            x=x, y=y, z=z,\n            colorscale='Blues',\n            opacity=0.8,\n            showscale=False\n        ))\n    \n    elif surface_type == \"hyperbolic_cylinder\":\n        # Add both sheets\n        for sheet in range(2):\n            x = points[sheet, :, :, 0]\n            y = points[sheet, :, :, 1]\n            z = points[sheet, :, :, 2]\n            \n            fig.add_trace(go.Surface(\n                x=x, y=y, z=z,\n                colorscale='Reds',\n                opacity=0.8,\n                showscale=False\n            ))\n    \n    # Add principal axes (eigenvectors)\n    colors = ['red', 'green', 'blue']\n    for i in range(3):\n        # Scale eigenvector for visualization\n        scale = 2.5 / max(abs(e) for e in eigenvalues if abs(e) &gt; 1e-10)\n        vector = eigenvectors[:, i] * scale\n        \n        eigenvalue_sign = \"+\" if eigenvalues[i] &gt; 1e-10 else (\"-\" if eigenvalues[i] &lt; -1e-10 else \"0\")\n        \n        fig.add_trace(go.Scatter3d(\n            x=[0, vector[0]], y=[0, vector[1]], z=[0, vector[2]],\n            mode='lines+markers',\n            line=dict(color=colors[i], width=6),\n            marker=dict(size=5, color=colors[i]),\n            name=f\"Axis {i+1} (λ = {eigenvalues[i]:.2f} {eigenvalue_sign})\"\n        ))\n    \n    # Add a marker at the origin\n    fig.add_trace(go.Scatter3d(\n        x=[0], y=[0], z=[0],\n        mode='markers',\n        marker=dict(size=8, color='black'),\n        name=\"Origin\"\n    ))\n    \n    if not title:\n        title = f\"{surface_type.replace('_', ' ').title()}\"\n    \n    # Set plot layout\n    fig.update_layout(\n        title=title,\n        scene=dict(\n            xaxis_title='X',\n            yaxis_title='Y',\n            zaxis_title='Z',\n            aspectmode='data'\n        ),\n        width=800,\n        height=800,\n        showlegend=True\n    )\n    \n    return fig\n\nExercise 3: Create Examples\nTypes of Quadratic Surfaces\nThe signs and values of the eigenvalues determine the type of quadratic surface:\n\n\n\nEigenvalue Signs\nSurface Type\n\n\n\n\nAll positive\nEllipsoid\n\n\nTwo positive, one negative\nHyperboloid of one sheet\n\n\nOne positive, two negative\nHyperboloid of two sheets\n\n\nTwo positive, one zero\nElliptic cylinder\n\n\nOne positive, one negative, one zero\nHyperbolic cylinder\n\n\nOne positive, two zero\nTwo parallel planes\n\n\n\nChange the eigenvalues to generate the different objects\n# Create \neigenvalues = [1 , 2, 3]  \nA, Q, D = create_symmetric_matrix_3d(eigenvalues)\n\nprint(\"Symmetric Matrix A:\")\nprint(A)\n\nprint(\"\\nEigenvectors Q (columns):\")\nprint(Q)\n\nprint(\"\\nEigenvalues D:\")\nprint(D)\n\n# Verify spectral decomposition\nprint(\"\\nVerify A = QDQ^T:\")\nprint(np.round(Q @ D @ Q.T, 10))\n\n# Display the equation\noriginal, canonical = display_3d_equation(A)\n#\ndisplay(Markdown(f\"\"\"Original form: ${sp.latex(original)} = 1$\"\"\"))\ndisplay(Markdown(f\"\"\"Canonical form: ${sp.latex(canonical)} = 1$\"\"\"))\n\n# Plot the ellipsoid using Plotly for interactive visualization\n#fig = plot_quadric_surface(A)\nfig = plot_quadratic_surface(A)\nfig.show()  # In Jupyter, this displays an interactive 3D plot\n\n\nExercise 4\n\nUse the functions to plot the quadratic surface \\[2x^2+xy+xz+3y^2-yz-z^2=1\\]",
    "crumbs": [
      "Eigenvalues and Eigenvectors",
      "Lab"
    ]
  },
  {
    "objectID": "parts/linear_maps2.html",
    "href": "parts/linear_maps2.html",
    "title": "Linear Transformation II",
    "section": "",
    "text": "This section covers more topics of Linear Transformations.\n\nLinear Transformations II\nProblems",
    "crumbs": [
      "Linear Transformation II"
    ]
  },
  {
    "objectID": "chapters/linear_maps2.html",
    "href": "chapters/linear_maps2.html",
    "title": "11  Advanced Topics of Linear Maps",
    "section": "",
    "text": "11.1 Representation of Vectors using Bases\nThe diagonalization process we explored previously—expressing matrices as \\(P^{-1}DP\\) with eigenvector matrix \\(P\\) and diagonal eigenvalue matrix \\(D\\)—takes on deeper meaning when viewed through the lens of linear transformations.\nThis perspective reveals that the same linear transformation can look different depending on our coordinate system. By changing basis, we gain the flexibility to choose the most convenient representation for a given problem, bridging the gap between abstract properties and concrete matrix representations. This powerful viewpoint is essential in both theoretical contexts and applications ranging from computer graphics to quantum mechanics.\nWhen working with a vector space \\(V\\) (often \\(\\mathbb{R}^n\\)), we can represent vectors using different bases. Let’s explore how the coordinate representation of a vector changes when we switch between bases.\nConsider a vector space \\(V\\) with two different bases:\nFor any vector \\(\\mathbf{v} \\in V\\), we can express it as a linear combination using either basis:\nUsing basis \\(S_1\\): \\[\\mathbf{v} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\ldots + c_n\\mathbf{v}_n\\]\nThe coordinate vector with respect to \\(S_1\\) is: \\[[\\mathbf{v}]_{S_1} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}\\]\nSimilarly, using basis \\(S_2\\): \\[\\mathbf{v} = d_1\\mathbf{w}_1 + d_2\\mathbf{w}_2 + \\ldots + d_n\\mathbf{w}_n\\]\nThe coordinate vector with respect to \\(S_2\\) is: \\[[\\mathbf{v}]_{S_2} = \\begin{bmatrix} d_1 \\\\ d_2 \\\\ \\vdots \\\\ d_n \\end{bmatrix}\\]",
    "crumbs": [
      "Linear Transformation II",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Topics of Linear Maps</span>"
    ]
  },
  {
    "objectID": "chapters/linear_maps2.html#sec-change-of-basis",
    "href": "chapters/linear_maps2.html#sec-change-of-basis",
    "title": "11  Advanced Topics of Linear Maps",
    "section": "",
    "text": "\\(S_1 = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\}\\)\n\\(S_2 = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_n\\}\\)\n\n\n\n\n\n\n\nThe fundamental question is: What is the relationship between \\([\\mathbf{v}]_{S_1}\\) and \\([\\mathbf{v}]_{S_2}\\)?\n\n\nTheorem 11.1 Suppose that \\(V\\) is an \\(n\\)-dimensional vector space with bases \\(S_1\\) and \\(S_2\\). Then there exists a unique invertible matrix \\(P=P_{S_1 \\leftarrow S_2}\\) such that for every vector \\(\\mathbf{v} \\in V\\) \\[[\\mathbf{v}]_{S_1} = P[\\mathbf{v}]_{S_2}\\]\nMoreover, this change of basis matrix can be constructed as: \\[P_{S_1 \\leftarrow S_2} =\n\\begin{bmatrix} \\uparrow & \\uparrow & \\cdots & \\uparrow\\\\\n[\\mathbf{w}_1]_{S_1} & [\\mathbf{w}_2]_{S_1} & \\cdots & [\\mathbf{w}_n]_{S_1} \\\\\n\\downarrow & \\downarrow & \\cdots&\\downarrow \\end{bmatrix}, \\tag{11.1}\\] where \\(S_2=\\{\\mathbf{w}_1,\\dots,\\mathbf{w}_n\\}\\)\n\n\nProof. Let \\(\\mathbf{v} \\in V\\) with representation in basis \\(S_2\\):\n\\[\\mathbf{v} = d_1\\mathbf{w}_1 + d_2\\mathbf{w}_2 + \\ldots + d_n\\mathbf{w}_n \\quad \\text{where} \\quad [\\mathbf{v}]_{S_2} = \\begin{bmatrix} d_1 \\\\ d_2 \\\\ \\vdots \\\\ d_n \\end{bmatrix}\\]\nNow, we need to find \\([\\mathbf{v}]_{S_1}\\). To do that we use Theorem 6.1, that states that finding coordinates is a linear map, and Equation 3.1, that allows to express a linear combination in \\(\\mathbb{R}^n\\) as a product of a matrix and a vector: \\[\\begin{align}\n[\\mathbf{v}]_{S_1} & = [d_1\\mathbf{w}_1 + \\ldots + d_n\\mathbf{w}_n]_{S_1}\\\\\n  & = d_1[\\mathbf{w}_1]_{S_1} + \\ldots + d_n[\\mathbf{w}_n]_{S_1}\\\\\n  & = \\begin{bmatrix} \\uparrow & \\uparrow & \\cdots & \\uparrow\\\\\n[\\mathbf{w}_1]_{S_1} & [\\mathbf{w}_2]_{S_1} & \\cdots & [\\mathbf{w}_n]_{S_1} \\\\\n\\downarrow & \\downarrow & \\cdots&\\downarrow \\end{bmatrix} \\begin{bmatrix} d_1 \\\\ d_2 \\\\ \\vdots \\\\ d_n \\end{bmatrix}\n\\end{align}\\] Therefore: \\[[\\mathbf{v}]_{S_1} = P_{S_1 \\leftarrow S_2} [\\mathbf{v}]_{S_2},\\] and \\(P_{S_1\\leftarrow S_2}\\) is given by Equation 11.1\n\n\nIf \\(P=P_{S_1\\leftarrow S_2}\\) is the change of basis matrix from \\(S_2\\) to \\(S_1\\), then \\(P^{-1}=P_{S_2\\leftarrow S_1}\\) is the change of basis matrix from \\(S_1\\) to \\(S_2\\). We see this easily: from the equation \\([\\mathbf{v}]_{S_1} = P[\\mathbf{v}]_{S_2}\\), we deduce that \\[[\\mathbf{v}]_{S_2} = P^{-1}[\\mathbf{v}]_{S_1}\\]\n\n\n11.1.1 Computational Problems\nTheorem 11.1 provides not just a theoretical connection but a concrete algorithm for computing the change of basis matrix \\(P\\). The formula \\([\\mathbf{v}]_{S_1}=P[\\mathbf{v}]_{S_2}\\) looks simple but contains all the steps needed to convert coordinates between different bases. Students should pay close attention to this formula as it shows how to solve many types of coordinate conversion problems in one compact expression.\n\n\nFind the change of basis matrix\n\n\nSuppose \\(S_1\\) and \\(S_2\\) are given. Find the change of basis matrix from \\(S_2\\) to \\(S_1\\), \\(P_{S_1\\leftarrow S_2}\\).\n\n\nExample: Suppose \\(S_1 = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}\\) and \\(S_2 = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3\\}\\) are bases of \\(\\mathbb{R}^3\\) and we are asked to find \\(P_{S_1\\leftarrow S_2}\\).\nFrom formula Equation 11.1, we know that we need to find \\([\\mathbf{w}_1]_{S_1}\\), \\([\\mathbf{w}_2]_{S_1}\\), and \\([\\mathbf{w}_3]_{S_1}\\).\nTo find \\([\\mathbf{w}_i]_{S_1}\\), we solve the vector equation: \\(x_1 \\mathbf{v}_1 + x_2 \\mathbf{v}_2 + x_3 \\mathbf{v}_3 = \\mathbf{w}_i\\). We look at the augmented system and we row reduce it: \\[\n\\left[\n    \\begin{array}{ccc|c}\n\\uparrow & \\uparrow & \\uparrow & \\uparrow \\\\\n\\mathbf{v}_1 & \\mathbf{v}_2 & \\mathbf{v}_3 & \\mathbf{w}_i \\\\\n\\downarrow & \\downarrow & \\downarrow & \\downarrow\n\\end{array}\\right]\n\\xrightarrow{\\text{RREF}}\n\\left[\\begin{array}{ccc|c}\n1&0&0&\\uparrow \\\\\n0&1&0 & [\\mathbf{w}_i]_{S_1} \\\\\n0&0&1 &\\downarrow\n\\end{array}\\right].\n\\] Since \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}\\) is a basis, the first three columns of the reduced matrix is the identity and the last column gives us the coordinates. We can combine all of them:\n\\[\n\\left[\n    \\begin{array}{ccc|ccc}\n\\uparrow & \\uparrow & \\uparrow & \\uparrow & \\uparrow & \\uparrow\\\\\n\\mathbf{v}_1 & \\mathbf{v}_2 & \\mathbf{v}_3 & \\mathbf{w}_1 & \\mathbf{w}_2 & \\mathbf{w}_3 \\\\\n\\downarrow & \\downarrow & \\downarrow & \\downarrow & \\downarrow & \\downarrow\n\\end{array}\\right]\n\\xrightarrow{\\text{RREF}}\n\\left[\\begin{array}{ccc|ccc}\n1&0&0&\\uparrow &\\uparrow & \\uparrow\\\\\n0&1&0 & [\\mathbf{w}_1]_{S_1} & [\\mathbf{w}_2]_{S_1} & [\\mathbf{w}_3]_{S_1} \\\\\n0&0&1 &\\downarrow & \\downarrow & \\downarrow\n\\end{array}\\right]\n\\] Therefore, in general, to find \\(P_{S_1\\leftarrow S_2}\\) we row reduce the augmented matrix \\([S_1|S_2]\\) to get \\([I|P_{S_1\\leftarrow S_2}]\\):\n\\[[S_1|S_2]\\xrightarrow{\\text{RREF}} [I|P_{S_1\\leftarrow S_2}]\\]\nWhere:\n\n\\(S_1\\) is the matrix with columns being the vectors of the first basis\n\\(S_2\\) is the matrix with columns being the vectors of the second basis\n\\(P_{S_1\\leftarrow S_2}\\) is the change of basis matrix from \\(S_2\\) to \\(S_1\\)\n\n\n\nFind coordinates\n\n\nSuppose \\(S_1\\), \\(S_2\\) and \\(P=P_{S_1\\leftarrow S_2}\\) are given. If we we know the coordinates of a vector \\(\\mathbf{v}\\in V\\) with respect to one basis, find the coordinates with respect to the other basis.\n\n\nThese type of problems are simpler. We just need to pay attention if we multiply by \\(P\\) or by \\(P^{-1}\\). Let \\(\\mathbf{v}\\in V\\). If we know \\([\\mathbf{v}]_{S_2}\\), then we find \\([\\mathbf{v}]_{S_1}\\) using: \\[[\\mathbf{v}]_{S_1}=P[\\mathbf{v}]_{S_2}\\] On the other hand, if we know \\([\\mathbf{v}]_{S_1}\\) then we find \\([\\mathbf{v}]_{S_2}\\) using: \\[[\\mathbf{v}]_{S_2}=P^{-1}[\\mathbf{v}]_{S_1}\\]\n\n\nFind elements of a basis\n\n\nSuppose \\(S_1\\) and \\(P=P_{S_1\\leftarrow S_2}\\) are given. Find \\(S_2\\).\n\n\nExample: Suppose that\n\n\\(S_1 =\\left\\{\n\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix},\n\\begin{bmatrix}1\\\\-1\\\\0\\end{bmatrix},\n\\begin{bmatrix}1\\\\1\\\\-2\\end{bmatrix}\n  \\right\\}\\)\n\\(S_2 =\\{\\mathbf{w}_1,\\mathbf{w}_2, \\mathbf{w}_3\\}\\), and are bases of \\(\\mathbf{R^3}\\) and that \\[ P=P_{S_1\\leftarrow S_2}=\\begin{bmatrix}1&0&0\\\\1&1&0\\\\1&1&1\\end{bmatrix}\n\\] is the change of basis matrix from \\(S_2\\) to \\(S_1\\). Our task is to find the elements of \\(S_2\\).\n\nFrom Equation 11.1 we know that \\([\\mathbf{w}_1]_{S_1}=(1,1,1)\\). Then \\[\\mathbf{w}_1=1\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}+1\\begin{bmatrix}1\\\\-1\\\\0\\end{bmatrix}+1\\begin{bmatrix}1\\\\1\\\\-2\n\\end{bmatrix}=\\begin{bmatrix}3\\\\1\\\\-1\\end{bmatrix}\\] Notice that we can write this as \\[\\begin{align}\\mathbf{w}_1&=1\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}+1\\begin{bmatrix}1\\\\-1\\\\0\\end{bmatrix}+1\\begin{bmatrix}1\\\\1\\\\-2\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}1&1&1\\\\1&-1&1\\\\1&0&2\\end{bmatrix}\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\n=[S_1][\\mathbf{w}_1]_{S_1}\n\\end{align}\\] where \\([S_1]\\) is the \\(3\\times 3\\) matrix with columns being the vectors of \\(S_1\\).\nWe can find all the vectors at once \\[[S_1]P=[S_1]\n\\begin{bmatrix}\n\\uparrow&\\uparrow&\\uparrow\\\\\n[\\mathbf{w}_1]_{S_1}&[\\mathbf{w}_2]_{S_1}&[\\mathbf{w}_3]_{S_1}\\\\\n\\downarrow&\\downarrow&\\downarrow\\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\n\\uparrow &\\uparrow &\\uparrow\\\\\n\\mathbf{w}_1&\\mathbf{w}_2&\\mathbf{w}_3\\\\\n\\downarrow &\\downarrow &\\downarrow\\\\\n\\end{bmatrix}=[S_2]\\] and this works in general.",
    "crumbs": [
      "Linear Transformation II",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Topics of Linear Maps</span>"
    ]
  },
  {
    "objectID": "chapters/linear_maps2.html#matrix-representations-of-linear-maps",
    "href": "chapters/linear_maps2.html#matrix-representations-of-linear-maps",
    "title": "11  Advanced Topics of Linear Maps",
    "section": "11.2 Matrix Representations of Linear Maps",
    "text": "11.2 Matrix Representations of Linear Maps\nIn Theorem 4.1 we showed that a linear map \\(T:\\mathbf{R}^n\\to\\mathbf{R}^n\\) can be written as \\(T(\\mathbf{x})=A\\mathbf{x}\\), where \\(A\\) is the matrix \\[A=\\begin{bmatrix}\n   \\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n   T(\\mathbf{e}_1) & T(\\mathbf{e}_2) & \\cdots & T(\\mathbf{e}_n) \\\\\n   \\downarrow & \\downarrow & \\cdots & \\downarrow \\\\\n   \\end{bmatrix}\\] and \\(\\{\\mathbf{e}_1,\\dots\\mathbf{e}_n\\}\\) is the canonical basis for \\(\\mathbb{R}^n\\).\nIn this section we show that a similar result works for general linear maps on general vector spaces with a fixed basis. The idea of the proof is the same but the result is presented in terms of coordinates.\n\nTheorem 11.2 Suppose that \\(V\\) is an \\(n\\) dimensional vector space with basis \\(S=\\{\\mathbf{e}_1,\\dots\\mathbf{e}_n\\}\\). If \\(T:V\\to V\\) is a linear map, there exists a unique matrix \\(A\\) such that for every \\(\\mathbf{v}\\in V\\), \\[[T(\\mathbf{v})]_S=A[\\mathbf{v}]_S\\] Moreover, the matrix representation of \\(T\\) with respect to the basis \\(S\\) is given by \\[A =\n\\begin{bmatrix} \\uparrow & \\uparrow & \\cdots & \\uparrow\\\\\n[T(\\mathbf{v}_1)]_S & [T(\\mathbf{v}_2)]_{S} & \\cdots & [T(\\mathbf{v}_n)]_{S} \\\\\n\\downarrow & \\downarrow & \\cdots&\\downarrow \\end{bmatrix}. \\tag{11.2}\\]\n\n\nProof. Let \\(\\mathbf{v} \\in V\\) with representation in basis \\(S\\): \\[\\mathbf{v} = x_1\\mathbf{v}_1 + x_2\\mathbf{v}_2 + \\ldots + x_n\\mathbf{v}_n \\quad \\text{where} \\quad [\\mathbf{v}]_{S} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\\] Since \\(T\\) is linear \\[\n\\begin{align}T(\\mathbf{v}) &=T(x_1\\mathbf{v}_1 + x_2\\mathbf{v}_2 + \\ldots + x_n\\mathbf{v}_n)\\\\\n&= x_1T(\\mathbf{v}_1) + x_2T(\\mathbf{v}_2) + \\ldots + x_nT(\\mathbf{v}_n).\n\\end{align}\n\\] Then we find the coordinates of \\(T(\\mathbf{v})\\) using Theorem 6.1 (finding coordinates is a linear map) and Equation 3.1 to find the matrix representation: \\[\n\\begin{align}\n[T(\\mathbf{v})]_S &= [x_1T(\\mathbf{v}_1) + x_2T(\\mathbf{v}_2) + \\ldots + x_nT(\\mathbf{v}_n)]_S\\\\\n&=x_1[T(\\mathbf{v}_1)]_S + x_2[T(\\mathbf{v}_2)]_S + \\ldots + x_n[T(\\mathbf{v}_n)]_S\\\\\n&= \\begin{bmatrix} \\uparrow & \\uparrow & \\cdots & \\uparrow\\\\\n[T(\\mathbf{v}_1)]_{S} & [T(\\mathbf{v}_2)]_{S} & \\cdots & [T(\\mathbf{v}_n)]_{S} \\\\\n\\downarrow & \\downarrow & \\cdots&\\downarrow \\end{bmatrix}\n\\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{bmatrix}=A[\\mathbf{v}]_S.\n\\end{align}\n\\] This concludes the proof \\(\\square\\)\n\n\n11.2.1 Computational Problems\nLike before, Theorem 11.2 provides not just a theoretical connection but a concrete algorithm for computing the matrix representation \\(A\\). The formula \\([T(\\mathbf{v})]_{S}=A[\\mathbf{v}]_{S}\\) contains a lot of information. Students should pay close attention to it.\n\n\nFind the matrix representation\n\n\nSuppose that a linear map \\(T:\\mathbb{R}^n\\to\\mathbb{R}^n\\) and a basis \\(S=\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) of \\(\\mathbf{R}^n\\) are given. Find the matrix representation \\(A\\).\n\n\nExample: Suppose \\(S = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}\\) is a basis of \\(\\mathbb{R}^3\\) and that \\(T:\\mathbb{R}^3\\to\\mathbb{R}^3\\) is a linear map. We are asked to find the matrix representation of \\(T\\) with respect to \\(S\\), which we call \\(A\\).\nFrom Theorem 11.2, we know that we need to find \\([T(\\mathbf{v}_1)]_{S}\\), \\([T(\\mathbf{v}_2)]_{S}\\), and \\([T(\\mathbf{v}_3)]_{S}\\).\nTo find \\([T(\\mathbf{v}_i)]_{S}\\), we solve the vector equation: \\(x_1 \\mathbf{v}_1 + x_2 \\mathbf{v}_2 + x_3 \\mathbf{v}_3 = T(\\mathbf{v}_i)\\). We look at the augmented system and we row reduce it: \\[\n\\left[\n    \\begin{array}{ccc|c}\n\\uparrow & \\uparrow & \\uparrow & \\uparrow \\\\\n\\mathbf{v}_1 & \\mathbf{v}_2 & \\mathbf{v}_3 & T(\\mathbf{v}_i) \\\\\n\\downarrow & \\downarrow & \\downarrow & \\downarrow\n\\end{array}\\right]\n\\xrightarrow{\\text{RREF}}\n\\left[\\begin{array}{ccc|c}\n1&0&0&\\uparrow \\\\\n0&1&0 & [T(\\mathbf{v}_i)]_{S} \\\\\n0&0&1 &\\downarrow\n\\end{array}\\right].\n\\] Since \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}\\) is a basis, the first three columns of the reduced matrix is the identity and the last column gives us the coordinates of \\(T(\\mathbf{v}_i)\\). We can combine all of them:\n\\[\n\\left[\n    \\begin{array}{ccc|ccc}\n\\uparrow & \\uparrow & \\uparrow & \\uparrow & \\uparrow & \\uparrow\\\\\n\\mathbf{v}_1 & \\mathbf{v}_2 & \\mathbf{v}_3 & T(\\mathbf{v}_1) & T(\\mathbf{v}_2) & T(\\mathbf{v}_3) \\\\\n\\downarrow & \\downarrow & \\downarrow & \\downarrow & \\downarrow & \\downarrow\n\\end{array}\\right]\n\\xrightarrow{\\text{RREF}}\n\\left[\\begin{array}{ccc|ccc}\n1&0&0&\\uparrow &\\uparrow & \\uparrow\\\\\n0&1&0 & [T(\\mathbf{v}_1)]_{S} & [T(\\mathbf{v}_2)]_{S} & [T(\\mathbf{v}_3)]_{S} \\\\\n0&0&1 &\\downarrow & \\downarrow & \\downarrow\n\\end{array}\\right]\n\\] Therefore, in general, to find \\(A\\) we row reduce the augmented matrix \\([S|T(S)]\\) to get \\([I|A]\\):\n\\[[S|T(S)]\\xrightarrow{\\text{RREF}} [I|A]\\]\nWhere:\n\n\\(S\\) is the matrix with columns being the vectors of the basis\n\\(T(S)\\) is the matrix with columns being \\(T\\) applied to the vectors of the basis\n\\(A\\) is the matrix representation of \\(T\\) with respect to \\(S\\)\n\n\n\nFind \\(T(\\mathbf{v})\\)\n\n\nSuppose \\(S\\) is a basis of \\(\\mathbb{R}^n\\) and that \\(A\\) is the matrix representation of a linear map \\(T:\\mathbb{R}^n\\to\\mathbb{R}^n\\). If \\(\\mathbf{v}\\in\\mathbb{R}^n\\), we need to find \\(T(\\mathbf{v})\\)\n\n\nExample: Suppose that \\[S =\\left\\{\n  \\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix},\n  \\begin{bmatrix}1\\\\-1\\\\0\\end{bmatrix},\n  \\begin{bmatrix}1\\\\1\\\\-2\\end{bmatrix}\n    \\right\\}\\] is a basis of \\(\\mathbf{R^3}\\) and that \\(T:\\mathbb{R}^3\\to\\mathbb{R}^3\\) is a linear map with matrix represenation with respect to \\(S\\) given by \\[ A=\\begin{bmatrix}1&0&0\\\\1&1&0\\\\1&1&1\\end{bmatrix}.\\] Let \\(\\mathbf{v}=(1,1,0)\\). We need to find \\(T(1,1,0)\\).\nFrom Theorem 11.2 we know that \\([T(\\mathbf{v})]_S=A[\\mathbf{v}]_S\\). This formula outlines the steps that we need to take. We should not memorize them, we should read them from the formula. We have \\(\\mathbf{v}\\)\n\nFirst, we need to find \\([\\mathbf{v}]_S\\)\nThen, multiplying \\([\\mathbf{v}]_S\\) by \\(A\\) we get \\([T(\\mathbf{v})]_S\\)\nFinally we find \\(T(\\mathbf{v})\\)\n\nTo find \\([\\mathbf{v}]_S\\), we look at the augmented system and row reduce it \\[\\left[\\begin{array}{ccc|c}\n1&1&1&1\\\\1&-1&1&1\\\\1&0&-2&0\n\\end{array}\\right]\n\\xrightarrow{\\text{RREF}}\n\\left[\\begin{array}{ccc|c}\n1&0&0&2\\\\0&1&0&0\\\\0&0&1&-1\n\\end{array}\\right]\n\\] and we get that \\([(1,1,0)]_S=(2,0,-1)\\). Now we multiply the coordinates by \\(A\\) to find the coordinates of \\(T(\\mathbf{v})\\) \\[\n[T(\\mathbf{v})]_S=A[\\mathbf{v}]_S\n=A\\begin{bmatrix}1&0&0\\\\1&1&0\\\\1&1&1\\end{bmatrix}\n\\begin{bmatrix}2\\\\0\\\\-1\\end{bmatrix}=\n\\begin{bmatrix}2\\\\2\\\\1\\end{bmatrix}\n\\] Then \\[T(\\mathbf{v})=\n2 \\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}+\n2 \\begin{bmatrix}1\\\\-1\\\\0\\end{bmatrix}+\n  \\begin{bmatrix}1\\\\1\\\\-2\\end{bmatrix}\n  =\\begin{bmatrix}5\\\\1\\\\0\\end{bmatrix}\n\\]\n\n\n11.2.2 Diagonalization of Linear Transformations\nWhen studying linear transformations on vector spaces, we often seek simple matrix representations. In some cases, we can find bases that yield diagonal matrices, which significantly simplifies calculations. The following Theorem characterizes the transformations that allow for such representations:\nI’ll clean up the proof while maintaining its logical structure.\n\nTheorem: A linear transformation \\(T: V \\to V\\) is diagonalizable if and only if there exists a basis for \\(V\\) consisting entirely of eigenvectors of \\(T\\).\n\n\nProof. (\\(\\Rightarrow\\)) Suppose that the matrix representation of \\(T:V\\to V\\) with respect to the basis \\(S=\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) is diagonal: \\[\\begin{bmatrix}\nλ_1 & 0 & \\cdots & 0 \\\\\n0 & λ_2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & λ_n\n\\end{bmatrix}\\]\nFrom Equation 11.2, we know that the \\(j\\)th column of this matrix gives the coordinates of \\(T(\\mathbf{v}_j)\\) in terms of the basis \\(S\\). Thus, \\([T(\\mathbf{v}_j)]_S\\) has \\(λ_j\\) in the \\(j\\)th position and zeros elsewhere, which means \\(T(\\mathbf{v}_j)=λ_j\\mathbf{v}_j\\) for each \\(j=1,2,\\ldots,n\\). Therefore, each basis vector \\(\\mathbf{v}_j\\) is an eigenvector of \\(T\\) with corresponding eigenvalue \\(λ_j\\).\n(\\(\\Leftarrow\\)) Conversely, if \\(S=\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) is a basis of eigenvectors with \\(T(\\mathbf{v}_j)=λ_j\\mathbf{v}_j\\), then the matrix representation with respect to this basis must be diagonal with the eigenvalues on the main diagonal.",
    "crumbs": [
      "Linear Transformation II",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Topics of Linear Maps</span>"
    ]
  },
  {
    "objectID": "chapters/linear_maps2.html#matrices-that-represent-the-same-linear-map",
    "href": "chapters/linear_maps2.html#matrices-that-represent-the-same-linear-map",
    "title": "11  Advanced Topics of Linear Maps",
    "section": "11.3 Matrices that Represent the Same Linear Map",
    "text": "11.3 Matrices that Represent the Same Linear Map\nLet \\(V\\) be a vector space with bases \\(S_1 = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\}\\) and \\(S_2 = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_n\\}\\) and let \\(T:V\\to V\\) be a linear map. From Theorem 11.2 there exist unique matrices \\(A\\) and \\(B\\) such that for every \\(\\mathbf{v}\\in V\\) \\[[T(\\mathbf{v})]_{S_1}=A[\\mathbf{v}]_{S_1}\\quad\\text{ and }\\quad [T(\\mathbf{v})]_{S_2}=B[\\mathbf{v}]_{S_2}.\\]\n\nSince \\(A\\) and \\(B\\) represent the same linear map with respect to a different set of bases, they have to be related. In this section we study this relationship.\n\nFrom Theorem 11.1 there exists a unique matrix \\(P=P_{S_1\\leftarrow S_2}\\) such that for every \\(\\mathbf{z}\\in V\\) \\[[\\mathbf{z}]_{S_1}=P[\\mathbf{z}].\\] Applying this formula to \\(\\mathbf{v}\\) and \\(T(\\mathbf{v})\\) and multiplying by \\(P^{-1}\\) we get: \\[\\begin{align}\n[T(\\mathbf{v})]_{S_1} &= A[\\mathbf{v}]_{S_1}\\\\\nP[T(\\mathbf{v})]_{S_2} &= AP[\\mathbf{v}]_{S_2}\\\\\n[T(\\mathbf{v})]_{S_2} &= P^{-1}AP[\\mathbf{v}]_{S_2}.\n\\end{align}\\] Since \\(B\\) is unique we obtain that \\[B=P^{-1}AP\\quad\\text{ or equivalently }\\quad A=PBP^{-1} \\tag{11.3}\\]\nThis motivates the following definition:\n\nDefinition 11.1 Let \\(A\\) and \\(B\\) be \\(n\\times n\\) matrices. We say that \\(A\\) is similar to \\(B\\) if there exists an invertible matrix \\(P\\) such that \\(B=P^{-1}AP\\).\n\nSimilarity forms an equivalence relation between square matrices. First, the relation is symmetric: if \\(B=P^{-1}AP\\), then by multiplying both sides by \\(P\\) on the right and \\(P^{-1}\\) on the left, we get \\(A=PBP^{-1}\\), showing that \\(B\\) is also similar to \\(A\\).\nThe relation is reflexive because every square matrix \\(A\\) is similar to itself via the identity matrix: \\(A=I^{-1}AI\\).\nFinally, similarity is transitive: if \\(A\\) is similar to \\(B\\) with \\(B=P^{-1}AP\\), and \\(B\\) is similar to \\(C\\) with \\(C=Q^{-1}BQ\\), then substituting gives \\(C=Q^{-1}(P^{-1}AP)Q=(PQ)^{-1}A(PQ)\\). This shows that \\(A\\) is similar to \\(C\\) via the invertible matrix \\(PQ\\), thereby establishing similarity as an equivalence relation.\nOur explanation also provided a proof of the following theorem:\n\nTheorem 11.3 Let \\(V\\) be a vector space with bases \\(S_1\\) and \\(S_2\\) and let \\(T:V\\to V\\) be a linear map. Then the matrix representations of \\(T\\) with respect to \\(S_1\\) and \\(S_2\\) are similar. More precisely, if \\(P\\) is the change of basis matrix from \\(S_2\\) to \\(S_1\\), \\(A\\) is the matrix representation of \\(T\\) with respect to \\(S_1\\) and \\(B\\) is the matrix representation of \\(T\\) with respect to \\(S_2\\), then \\(B=P^{-1}AP\\).\n\n\n11.3.1 Rotations in \\(\\mathbb{R}^3\\)\nWe will illustrate Theorem 11.3 finding matrix representations of 3D rotations.\nLet’s start with a simpler example in \\(\\mathbb{R}^2\\) that we have discussed before\nExample 1: Let \\(T\\) be the rotation by \\(\\theta\\) degrees in \\(\\mathbb{R}^2\\). The matrix representation with respect to the canonical basis is \\[\\begin{bmatrix}\\cos(\\theta) &-\\sin(\\theta)\\\\\\sin(\\theta)&\\cos(\\theta)\\end{bmatrix}\\]\nThis matrix represents a counterclockwise rotation by \\(\\theta\\) degrees around the origin in a two-dimensional space. When we multiply this matrix by any vector in \\(\\mathbb{R}^2\\), it rotates the vector by the angle \\(\\theta\\) while preserving its length.\nThe canonical basis in \\(\\mathbb{R}^2\\) consists of the unit vectors \\(\\mathbf{e}_1 = (1,0)\\) and \\(\\mathbf{e}_2 = (0,1)\\). When rotated by \\(\\theta\\) degrees:\n\n\\(\\mathbf{e}_1\\) becomes \\((\\cos(\\theta), \\sin(\\theta))\\), which forms the first column of the matrix\n\\(\\mathbf{e}_2\\) becomes \\((-\\sin(\\theta), \\cos(\\theta))\\), which forms the second column\n\nExample 2: Rotation by \\(\\theta\\) degrees about the x-axis in \\(\\mathbb{R}^3\\) has matrix representation: \\[\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & \\cos(\\theta) & -\\sin(\\theta) \\\\ 0 & \\sin(\\theta) & \\cos(\\theta)\\end{bmatrix}\\]\nThis matrix keeps the x-coordinate unchanged while rotating the yz-plane by angle \\(\\theta\\).\nExample 3: Rotation by \\(\\theta\\) degrees about the y-axis in \\(\\mathbb{R}^3\\) has matrix representation: \\[\\begin{bmatrix}\\cos(\\theta) & 0 & \\sin(\\theta) \\\\ 0 & 1 & 0 \\\\ -\\sin(\\theta) & 0 & \\cos(\\theta)\\end{bmatrix}\\]\nThis transformation preserves the y-coordinate while rotating the xz-plane by angle \\(\\theta\\).\nExample 4: Let \\(T\\) be the 3D-rotation by \\(\\theta\\) degrees about the axis \\((1,1,1)\\). Find the matrix representation of \\(T\\) with respect to the standard basis.\nThis is a more challenging problem because we can’t immediately determine how the rotation affects the standard basis vectors. A better approach is to use a change of basis strategy: first select a new coordinate system where one axis aligns with the rotation axis \\((1,1,1)\\), compute the rotation matrix in this convenient basis, and then transform it back to the standard basis using a change of basis matrix.\nIn the aligned coordinate system, the rotation will have a simple form since the vector \\((1,1,1)\\) remains fixed while vectors in the perpendicular plane rotate. We can use the similarity transformation \\(A = PBP^{-1}\\) where \\(P\\) is the change of basis matrix, \\(B\\) represents the rotation in the aligned coordinates, and \\(A\\) is our desired rotation matrix in the standard basis.\nThis technique is widely used in computer graphics, where rotations around arbitrary axes are essential for realistic 3D animations, camera movements, and object manipulations. While modern graphics engines often implement these operations using quaternions for better computational efficiency and to avoid issues, the underlying mathematical foundation remains the same.\nStep 1: Find the new coordinate system: We first find an orthonormal basis where one vector aligns with the rotation vector \\((1,1,1)\\): \\[S_2=\\left\\{\n    \\begin{bmatrix}\\frac{1}{\\sqrt{3}}\\\\\\frac{1}{\\sqrt{3}}\\\\\\frac{1}{\\sqrt{3}}\\end{bmatrix}\\\\\n    \\begin{bmatrix}\\frac{1}{\\sqrt{2}}\\\\\\frac{-1}{\\sqrt{2}}\\\\0\\end{bmatrix}\\\\\n    \\begin{bmatrix}\\frac{1}{\\sqrt{6}}\\\\\\frac{1}{\\sqrt{6}}\\\\\\frac{-2}{\\sqrt{6}}\\end{bmatrix}\\\\\n\\right\\}\\]\nIn this case we see that the matrix representation of \\(T\\) with respect \\(S_2\\) is \\[B=\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & \\cos(\\theta) & -\\sin(\\theta) \\\\ 0 & \\sin(\\theta) & \\cos(\\theta)\\end{bmatrix}\\] This matrix keeps the first vector fixed and it rotates the perpendicular plane by \\(\\theta\\) degrees, which is exactly what we want for a rotation around the \\((1,1,1)\\) axis.\nThis approach works precisely because \\(S_2\\)​ is an orthonormal basis. The orthonormality ensures that the transformation preserves angles and distances in the new coordinate system, which is essential for a proper rotation. Without orthonormality, the transformation would introduce distortion, and we could no longer interpret it as a pure rotation.\nStep 2: Find the change of basis matrix: We now find \\(Q=P_{S_1\\leftarrow S_2}\\), the change of basis matrix from \\(S_2\\) to the canonical basis \\(S_1=\\{\\mathbf{e}_1,\\mathbf{e}_2,\\mathbf{e}_3\\}\\). Since we are transitioning to the canonical basis, for every element of \\(S_2\\), \\([\\mathbf{w}_i]_{S_1}=\\mathbf{w}_i\\) and from Equation 11.1 we see that \\[Q=\\begin{bmatrix}\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}}\\\\\n\\frac{1}{\\sqrt{3}} & \\frac{-1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}}\\\\\n\\frac{1}{\\sqrt{3}} & 0 & \\frac{-2}{\\sqrt{6}}.\\\\\n\\end{bmatrix}\\]\nThe orthonormality of \\(S_2\\) makes the change of basis matrix orthogonal, since all the columns are orthonormal. This means that the inverse of \\(Q\\) is transpose, which significantly simplifies the calculations when transforming back to the standard basis.\nStep 3: Transform back to the standard basis: Theorem 11.3 says that the matrix representation of \\(T\\) with respect to the canonical basis \\(S_1\\) is \\(A=QBQ^T\\): \\[A=\\begin{bmatrix}\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}}\\\\\n\\frac{1}{\\sqrt{3}} & \\frac{-1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}}\\\\\n\\frac{1}{\\sqrt{3}} & 0 & \\frac{-2}{\\sqrt{6}}.\\\\\n\\end{bmatrix}\n\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & \\cos(\\theta) & -\\sin(\\theta) \\\\ 0 & \\sin(\\theta) & \\cos(\\theta)\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\\\\\n\\frac{1}{\\sqrt{2}} & \\frac{-1}{\\sqrt{2}} & 0\\\\\n\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{6}} & \\frac{-2}{\\sqrt{6}}\n\\end{bmatrix}\n\\] When we simplify this expression using SymPy we get that \\[A= \\begin{bmatrix}\\frac{2 \\cos{\\left(θ \\right)}}{3} + \\frac{1}{3} & \\frac{1}{3} - \\frac{2 \\sin{\\left(θ + \\frac{\\pi}{6} \\right)}}{3} & \\frac{1}{3} - \\frac{2 \\cos{\\left(θ + \\frac{\\pi}{3} \\right)}}{3}\\\\\\frac{1}{3} - \\frac{2 \\cos{\\left(θ + \\frac{\\pi}{3} \\right)}}{3} & \\frac{2 \\cos{\\left(θ \\right)}}{3} + \\frac{1}{3} & \\frac{1}{3} - \\frac{2 \\sin{\\left(θ + \\frac{\\pi}{6} \\right)}}{3}\\\\\\frac{1}{3} - \\frac{2 \\sin{\\left(θ + \\frac{\\pi}{6} \\right)}}{3} & \\frac{1}{3} - \\frac{2 \\cos{\\left(θ + \\frac{\\pi}{3} \\right)}}{3} & \\frac{2 \\cos{\\left(θ \\right)}}{3} + \\frac{1}{3}\\end{bmatrix}.\\]\nThis is the SymPy code:\n\n# Import libraries\nfrom sympy import *\ninit_printing()\n\n# Define the orthonormal vectors of S2\nV1=Matrix([1,1,1])/sqrt(3)\nV2=Matrix([1,-1,0])/sqrt(2)\nV3=Matrix([1,1,-2])/sqrt(6)\n\n# Define the change of basis mstrix Q\nQ = Matrix.hstack(V1,V2,V3)\n\n# Define B, matrix representation of T with respect to S2\nθ = Symbol('θ')\nB = Matrix([[1,0,0],[0,cos(θ),-sin(θ)],[0,sin(θ),cos(θ)]])\n\n# Find the matrix representation with respect to the standard basis and simplify\nsimplify(Q*B*Q.T)\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{2 \\cos{\\left(θ \\right)}}{3} + \\frac{1}{3} & \\frac{1}{3} - \\frac{2 \\sin{\\left(θ + \\frac{\\pi}{6} \\right)}}{3} & \\frac{1}{3} - \\frac{2 \\cos{\\left(θ + \\frac{\\pi}{3} \\right)}}{3}\\\\\\frac{1}{3} - \\frac{2 \\cos{\\left(θ + \\frac{\\pi}{3} \\right)}}{3} & \\frac{2 \\cos{\\left(θ \\right)}}{3} + \\frac{1}{3} & \\frac{1}{3} - \\frac{2 \\sin{\\left(θ + \\frac{\\pi}{6} \\right)}}{3}\\\\\\frac{1}{3} - \\frac{2 \\sin{\\left(θ + \\frac{\\pi}{6} \\right)}}{3} & \\frac{1}{3} - \\frac{2 \\cos{\\left(θ + \\frac{\\pi}{3} \\right)}}{3} & \\frac{2 \\cos{\\left(θ \\right)}}{3} + \\frac{1}{3}\\end{matrix}\\right]\\)",
    "crumbs": [
      "Linear Transformation II",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Topics of Linear Maps</span>"
    ]
  },
  {
    "objectID": "worksheets/matrix_representations2.html",
    "href": "worksheets/matrix_representations2.html",
    "title": "Problems",
    "section": "",
    "text": "Bases, Linear Maps, Matrix Representations, and Eigenvalues and Eigenvectors\nFor the problems in the section we consider the following bases of \\(\\mathbb{R}^3\\):\n\\(S_1 = \\left\\{\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\\right\\}\\)\n\\(S_2 = \\left\\{\\begin{pmatrix} 1 \\\\ 2 \\\\ -4 \\end{pmatrix}, \\begin{pmatrix} -3 \\\\ -3 \\\\ 4 \\end{pmatrix}, \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix}\\right\\}\\)\n\\(S_3 = \\left\\{\\begin{pmatrix} \\sqrt{3}/3 \\\\ \\sqrt{3}/3 \\\\ \\sqrt{3}/3 \\end{pmatrix}, \\begin{pmatrix} \\sqrt{2}/2 \\\\ 0 \\\\ -\\sqrt{2}/2 \\end{pmatrix}, \\begin{pmatrix} -\\sqrt{6}/6 \\\\ \\sqrt{6}/3 \\\\ -\\sqrt{6}/6 \\end{pmatrix}\\right\\}\\)\nAnd the following bases of \\(\\mathbb{R}^4\\):\n\\(S_4 = \\left\\{\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\\right\\}\\)\n\\(S_5 = \\left\\{\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ -2 \\\\ -1 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 3 \\\\ 1 \\\\ 0 \\\\ -2 \\end{pmatrix}, \\begin{pmatrix} 4 \\\\ -4 \\\\ -1 \\\\ -2 \\end{pmatrix}\\right\\}\\)\n\\(S_6 = \\left\\{\\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix}, \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1/2 \\\\ -1/2 \\end{pmatrix}, \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 1/2 \\\\ -1/2 \\end{pmatrix}, \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ -1/2 \\\\ 1/2 \\end{pmatrix}\\right\\}\\)",
    "crumbs": [
      "Linear Transformation II",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_representations2.html#bases-linear-maps-matrix-representations-and-eigenvalues-and-eigenvectors",
    "href": "worksheets/matrix_representations2.html#bases-linear-maps-matrix-representations-and-eigenvalues-and-eigenvectors",
    "title": "Problems",
    "section": "",
    "text": "Bases in NumPy\nimport numpy as np\n\n# Bases for R^3\nS1 = [\n    np.array([1, 0, 0]),\n    np.array([0, 1, 0]),\n    np.array([0, 0, 1])\n]\n\nS2 = [\n    np.array([1, 2, -4]),\n    np.array([-3, -3, 4]),\n    np.array([1, 1, -1])\n]\n\nS3 = [\n    np.array([np.sqrt(3)/3, np.sqrt(3)/3, np.sqrt(3)/3]),\n    np.array([np.sqrt(2)/2, 0, -np.sqrt(2)/2]),\n    np.array([-np.sqrt(6)/6, np.sqrt(6)/3, -np.sqrt(6)/6])\n]\n\n# Bases for R^4\nS4 = [\n    np.array([1, 0, 0, 0]),\n    np.array([0, 1, 0, 0]),\n    np.array([0, 0, 1, 0]),\n    np.array([0, 0, 0, 1])\n]\n\nS5 = [\n    np.array([1, 0, 0, -1]),\n    np.array([2, -2, -1, 1]),\n    np.array([3, 1, 0, -2]),\n    np.array([4, -4, -1, -2])\n]\n\nS6 = [\n    np.array([1/2, 1/2, 1/2, 1/2]),\n    np.array([1/2, 1/2, -1/2, -1/2]),\n    np.array([1/2, -1/2, 1/2, -1/2]),\n    np.array([1/2, -1/2, -1/2, 1/2])\n]\n\n# You can also define these as matrices (each column is a basis vector)\nS1_matrix = np.column_stack(S1)  # Standard basis as a matrix\nS2_matrix = np.column_stack(S2)  # S2 as a matrix\nS3_matrix = np.column_stack(S3)  # S3 as a matrix\nS4_matrix = np.column_stack(S4)  # Standard basis for R^4 as a matrix\nS5_matrix = np.column_stack(S5)  # S5 as a matrix\nS6_matrix = np.column_stack(S6)  # S6 as a matrix\n\n\nBases in SymPy\nfrom sympy import *\ninit_printing()\n\n# Bases for R^3\nS1_sp = [\n    Matrix([1, 0, 0]),\n    Matrix([0, 1, 0]),\n    Matrix([0, 0, 1])\n]\nS2_sp = [\n    Matrix([1, 2, -4]),\n    Matrix([-3, -3, 4]),\n    Matrix([1, 1, -1])\n]\nS3_sp = [\n    Matrix([sqrt(3)/3, sqrt(3)/3, sqrt(3)/3]),\n    Matrix([sqrt(2)/2, 0, -sqrt(2)/2]),\n    Matrix([-sqrt(6)/6, sqrt(6)/3, -sqrt(6)/6])\n]\n# Bases for R^4\nS4_sp = [\n    Matrix([1, 0, 0, 0]),\n    Matrix([0, 1, 0, 0]),\n    Matrix([0, 0, 1, 0]),\n    Matrix([0, 0, 0, 1])\n]\nS5_sp = [\n    Matrix([1, 0, 0, -1]),\n    Matrix([2, -2, -1, 1]),\n    Matrix([3, 1, 0, -2]),\n    Matrix([4, -4, -1, -2])\n]\nS6_sp = [\n    Matrix([1/2, 1/2, 1/2, 1/2]),\n    Matrix([1/2, 1/2, -1/2, -1/2]),\n    Matrix([1/2, -1/2, 1/2, -1/2]),\n    Matrix([1/2, -1/2, -1/2, 1/2])\n]\n\n# Converting lists of vectors to matrices (each column is a basis vector)\nS1_matrix_sp = Matrix.hstack(*S1_sp)  \nS2_matrix_sp = Matrix.hstack(*S2_sp)  \nS3_matrix_sp = Matrix.hstack(*S3_sp)  \nS4_matrix_sp = Matrix.hstack(*S4_sp)  \nS5_matrix_sp = Matrix.hstack(*S5_sp)  \nS6_matrix_sp = Matrix.hstack(*S6_sp)",
    "crumbs": [
      "Linear Transformation II",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_representations2.html#part-i-bases-and-coordinates",
    "href": "worksheets/matrix_representations2.html#part-i-bases-and-coordinates",
    "title": "Problems",
    "section": "Part I: Bases and Coordinates",
    "text": "Part I: Bases and Coordinates\n\nIf \\(S = \\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}\\) is a basis of \\(\\mathbb{R}^n\\) and \\(\\mathbf{v} \\in \\mathbb{R}^n\\), then there exists a unique vector \\(\\mathbf{x} = (x_1, \\ldots, x_n) \\in \\mathbb{R}^n\\) such that \\(x_1\\mathbf{v}_1 + x_2\\mathbf{v}_2 + \\cdots + x_n\\mathbf{v}_n = \\mathbf{v}\\). The vector \\(\\mathbf{x}\\) is denoted by \\([\\mathbf{v}]_S\\) and represents the vector of coordinates of \\(\\mathbf{v}\\) with respect to \\(S\\).\n\n\nProblem 1\nSkip Problem 1 if you know how to do its problems\nFind the vector \\(\\mathbf{v}\\) if:\n\n\\([\\mathbf{v}]_{S_2} = (1,0,-1)\\).\n\\([\\mathbf{v}]_{S_3} = (0,-1,0)\\).\n\\([\\mathbf{v}]_{S_5} = (1,0,-1,0)\\).\n\\([\\mathbf{v}]_{S_5} = (1,1,1,1)\\).\n\n\n\nProblem 2\nIf you know how to do Problem 2, do only one of its problems\nFor each of the following, find \\([\\mathbf{v}]_S\\) by:\n\nWriting the vector equation \\(\\mathbf{v} = x_1\\mathbf{v}_1 + x_2\\mathbf{v}_2 + \\cdots + x_k\\mathbf{v}_k\\)\nWriting it as a matrix equation or augmented matrix\nSolve it (if it is too complicated, use SymPy)\nFinding \\([\\mathbf{v}]_S\\)\n\n\n\\(\\mathbf{v} = (1,1,1)\\) with basis \\(S_2\\)\n\\(\\mathbf{v} = (1,1,-1)\\) with basis \\(S_3\\)\n\\(\\mathbf{v} = (1,1,1,1)\\) with basis \\(S_5\\)\n\\(\\mathbf{v} = (1,1,0,0)\\) with basis \\(S_6\\)\n\n\n\nProblem 3\nFind coordinates of more than one vector in one step:\n\nFor \\(S = S_2\\), find \\([\\mathbf{v}_1]_S\\) and \\([\\mathbf{v}_2]_S\\) where \\(\\mathbf{v}_1 = (1,1,0)\\) and \\(\\mathbf{v}_2 = (0,1,1)\\)\nFor \\(S = S_6\\), find coordinates of \\(\\mathbf{v}_1 = (1,1,0,-1)\\), \\(\\mathbf{v}_2 = (0,1,1,-2)\\), and \\(\\mathbf{v}_3 = (1,0,1,0)\\)",
    "crumbs": [
      "Linear Transformation II",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_representations2.html#part-ii-change-of-basis-matrices",
    "href": "worksheets/matrix_representations2.html#part-ii-change-of-basis-matrices",
    "title": "Problems",
    "section": "Part II: Change of Basis Matrices",
    "text": "Part II: Change of Basis Matrices\n\nIf \\(B_1 = \\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}\\) and \\(B_2 = \\{\\mathbf{w}_1, \\ldots, \\mathbf{w}_n\\}\\) are bases of \\(\\mathbb{R}^n\\), then there exists an invertible \\(n \\times n\\) matrix \\(P = P_{B_1\\leftarrow B_2}\\) such that for every \\(\\mathbf{v} \\in \\mathbb{R}^n\\): \\[[\\mathbf{v}]_{B_1} = P[\\mathbf{v}]_{B_2}.\\] Moreover, \\(P = \\begin{bmatrix} [\\mathbf{w}_1]_{B_1} & [\\mathbf{w}_2]_{B_1} & \\cdots & [\\mathbf{w}_n]_{B_1} \\end{bmatrix}\\)\n\n\nProblem 4\n\nFind the change of basis matrix from \\(S_2\\) to \\(S_1\\)\nFind the change of basis matrix from \\(S_1\\) to \\(S_3\\)\nFind the change of basis matrix from \\(S_2\\) to \\(S_3\\)\nSuppose that \\(S=\\{\\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3\\}\\) is a basis for \\(\\mathbb{R}^3\\) and that \\(P = \\begin{bmatrix} 1 & 1 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}\\) is the change of basis matrix from \\(S\\) to \\(S_2\\). Find \\(\\mathbf{w}_1\\), \\(\\mathbf{w}_2\\), and \\(\\mathbf{w}_3\\).\nSuppose that \\(S=\\{\\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3,\\mathbf{w}_4\\}\\) is a basis for \\(\\mathbb{R}^4\\) and that \\(P = \\begin{bmatrix} 1 & -1 & 1 & 0 \\\\ 0 & -2 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 0 & -2 \\end{bmatrix}\\) is the change of basis matrix from \\(S_5\\) to \\(S\\). Find \\(\\mathbf{w}_1\\), \\(\\mathbf{w}_2\\), \\(\\mathbf{w}_3\\), and \\(\\mathbf{w}_4\\).",
    "crumbs": [
      "Linear Transformation II",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_representations2.html#part-iii-matrix-representation-of-linear-maps",
    "href": "worksheets/matrix_representations2.html#part-iii-matrix-representation-of-linear-maps",
    "title": "Problems",
    "section": "Part III: Matrix Representation of Linear Maps",
    "text": "Part III: Matrix Representation of Linear Maps\n\nRecall that for a linear map \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) and a basis \\(S = \\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_n\\}\\), there exists a unique matrix \\(A\\) such that \\[[T(\\mathbf{v})]_S = A[\\mathbf{v}]_S.\\] Moreover \\(A = \\begin{bmatrix} [T(\\mathbf{v}_1)]_S & [T(\\mathbf{v}_2)]_S & \\cdots & [T(\\mathbf{v}_n)]_S \\end{bmatrix}\\)\n\n\nProblem 5\n\nSuppose that \\(S=\\{\\mathbf{v}_1,\\mathbf{v}_2,\\mathbf{v}_3\\}\\) is a basis of \\(\\mathbb{R}^3\\) and that \\(T:\\mathbb{R}^3\\to\\mathbb{R}^3\\) is a linear map satisfying \\(T(\\mathbf{v}_1) = -2\\mathbf{v}_1\\), \\(T(\\mathbf{v}_2) = 8\\mathbf{v}_2\\), \\(T(\\mathbf{v}_3) = \\mathbf{v}_3\\). Find the matrix representation of \\(T\\) with respect to \\(S\\). Notice that \\(S\\) is a basis of eigenvectors.\nSuppose that \\(S=\\{\\mathbf{v}_1,\\mathbf{v}_2\\}\\) is a basis of \\(\\mathbb{R}^2\\) and that the matrix representation of the linear map \\(T:\\mathbb{R}^2\\to\\mathbb{R}^2\\) is \\(\\begin{bmatrix} -2 & 0 \\\\ 0 & 8 \\end{bmatrix}\\). Find \\(T(\\mathbf{v}_1)\\) and \\(T(\\mathbf{v}_2)\\). Can you find the eigenvalues and eigenvectors of \\(T\\)?\nLet \\(T:\\mathbb{R}^3\\to\\mathbb{R}^3\\) be defined by \\(T(x,y,z) = (x+y, 2x-y+z, z-3x)\\):\n\nProve \\(T\\) is linear\nFind the matrix representation of \\(T\\) with respect to \\(S_1\\)\nFind the matrix representation of \\(T\\) with respect to \\(S_2\\)\n\nSuppose that the matrix representation of \\(T:\\mathbb{R}^3\\to\\mathbb{R}^3\\) with respect to \\(S_2\\) is \\(A = \\begin{bmatrix} 1 & 2 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & 0 & 1 \\end{bmatrix}\\).\n\nFind \\(T((1,1,-1))\\)\nFind \\(T((1,1,1))\\)\nFind \\(\\mathbf{v}\\) such that \\(T(\\mathbf{v}) = (-3,-3,4)\\)\nFind the matrix representation with respect to \\(S_1\\)\n\nSuppose that the matrix representation of \\(T:\\mathbb{R}^4\\to\\mathbb{R}^4\\) with respect to \\(S_6\\) is \\(A = \\begin{bmatrix} 1 & 2 & 0 & 0 \\\\ -1 & 2 & -1 & 1 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 4 & 0 & 1 \\end{bmatrix}\\).\n\nFind \\(T((1/2,1/2,1/2,1/2))\\)\nFind \\(T((1,0,0,0))\\)\nFind \\(T((0,1,0,0))\\)\nFind a basis for \\(\\text{nul}(A)\\) and use it to find a non-zero \\(\\mathbf{v}\\) such that \\(T(\\mathbf{v})=\\mathbf{0}\\)",
    "crumbs": [
      "Linear Transformation II",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_representations2.html#part-iv-matrix-representations-with-respect-to-two-bases",
    "href": "worksheets/matrix_representations2.html#part-iv-matrix-representations-with-respect-to-two-bases",
    "title": "Problems",
    "section": "Part IV: Matrix Representations with Respect to Two Bases",
    "text": "Part IV: Matrix Representations with Respect to Two Bases\n\nLet \\(\\mathcal{B}_1=\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) and \\(\\mathcal{B}_2=\\{\\mathbf{w}_1,\\dots,\\mathbf{w}_n\\}\\) be bases of \\(\\mathbb{R}^n\\) and let \\(T:\\mathbb{R}^n\\to\\mathbb{R}^n\\) be a linear map. Recall that there exist unique \\(n\\times n\\) matrices \\(A\\) and \\(B\\) such that for every \\(\\mathbf{v}\\in\\mathbb{R}^n\\), \\[\n[T\\mathbf{v}]_{\\mathcal{B}_1}=A[\\mathbf{v}]_{\\mathcal{B}_1}\\quad\\text{ and }\\quad[T\\mathbf{v}]_{\\mathcal{B}_2}=B[\\mathbf{v}]_{\\mathcal{B}_2}\n\\]\nLet \\(P=P_{\\mathcal{B}_1\\leftarrow\\mathcal{B}_2}\\) be the change of basis matrix from \\(\\mathcal{B}_2\\) to \\(\\mathcal{B}_1\\). Then for every \\(\\mathbf{v}\\in\\mathbb{R}^n\\), \\([\\mathbf{v}]_{\\mathcal{B}_1}=P[\\mathbf{v}]_{\\mathcal{B}_2}\\). Then using this on the previous equation, we get that \\[B = P^{-1}AP\\quad\\text{ and equivalently }\\quad A=PBP^{-1}\\]\n\n\nProblem 6\n\nSuppose that the matrix representation of the linear map \\(T:\\mathbb{R}^4\\to\\mathbb{R}^4\\) with respect \\(S_6\\) is a diagonal matrix with entries \\((-1,2,0,4)\\).\n\nFind the eigenvalues and eigenvectors of \\(T\\).\nFind the matrix representation of \\(T\\) with respect to \\(S_4\\).\n\nIn Problem 5 (4) you found the matrix representations of a linear map \\(T:\\mathbb{R}^3\\to\\mathbb{R}^3\\) with respect to the basis \\(S_1\\). The matrix representation of \\(T\\) with respect to the basis \\(S_2\\) was given. Call these matrics \\(B\\) and \\(A\\) respectively. Then find the change of basis matrix between the bases and verify they satisfy the formulas \\(PAP^{-1}=B\\) or \\(P^{-1}AP=B\\), depending if \\(P\\) is the change of basis from \\(S_2\\) to \\(S_1\\) or from \\(S_1\\) to \\(S_2\\).",
    "crumbs": [
      "Linear Transformation II",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_representations2.html#part-v-finding-diagonal-representations-if-they-exist",
    "href": "worksheets/matrix_representations2.html#part-v-finding-diagonal-representations-if-they-exist",
    "title": "Problems",
    "section": "Part V: Finding Diagonal Representations (if they exist)",
    "text": "Part V: Finding Diagonal Representations (if they exist)\n\nLet \\(T:\\mathbb{R}^n\\to\\mathbb{R}^n\\) be a linear map. Suppose that there exists a basis \\(S=\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) such that the matrix representation of \\(T\\) with respect to \\(S\\) is diagonal. That is, there exist \\(\\lambda_1,\\dots\\lambda_n\\in\\mathbb{R}\\) such that for every \\(\\mathbf{v}\\in\\mathbb{R}^n\\), \\[\n   [T\\mathbf{v}]_S=\\begin{bmatrix}\\lambda_1&0&\\cdots&0\\\\0&\\lambda_2&\\cdots&0\\\\\\vdots&\\vdots&\\ddots&0\\\\0&0&\\cdots&\\lambda_n\\end{bmatrix}[\\mathbf{v}]_S\n\\] Then we can check that for every \\(i\\leq n\\), \\(T\\mathbf{v}_i=\\lambda_i\\mathbf{v}_i\\), showing that \\(\\mathbf{v}_i\\) is an eigenvector with eigenvalue \\(\\lambda_i.\\)\nOn the other hand, suppose that \\(S=\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) is a basis of eigenvectors of \\(T:\\mathbb{R}^n\\to\\mathbb{R}^n\\). That is, for every \\(i\\leq n\\), there exists \\(\\lambda_i\\in\\mathbb{R}\\) such that \\(T\\mathbf{v}_i=\\lambda_i\\mathbf{v}_i\\). When we find the matrix representation of \\(T\\) with respect to \\(S\\) we obtain the diagonal matrix.\nWe now look at the same problem using matrices.\nLet \\(A\\) be an \\(n\\times n\\) matrix. And suppose that \\(A\\) has \\(n\\) linearly independent eigenvectors \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_n.\\) That is, for every \\(i\\leq n\\), there exists \\(\\lambda_i\\in\\mathbb{R}\\) such that \\(A\\mathbf{v}_i=\\lambda_i\\mathbf{v}_i\\). Let \\(P=\\begin{bmatrix}\\mathbf{v}_1&\\cdots&\\mathbf{v}_n\\end{bmatrix}\\). Since \\(P\\) is \\(n\\times n\\) and the columns are linearly independent, \\(P\\) is invertible. Then \\[AP=A\\begin{bmatrix}\\mathbf{v}_1&\\cdots&\\mathbf{v}_n\\end{bmatrix}\n=\\begin{bmatrix}A\\mathbf{v}_1&\\cdots&A\\mathbf{v}_n\\end{bmatrix}\n=\\begin{bmatrix}\\lambda_1\\mathbf{v}_1&\\cdots&\\lambda_n\\mathbf{v}_n\\end{bmatrix}\n=\\begin{bmatrix}\\mathbf{v}_1&\\cdots&\\mathbf{v}_n\\end{bmatrix}\\Lambda=P\\Lambda,\\] where \\(\\Lambda = \\begin{bmatrix}\\lambda_1&0&\\cdots&0\\\\0&\\lambda_2&\\cdots&0\\\\\\vdots&\\vdots&\\ddots&0\\\\0&0&\\cdots&\\lambda_n\\end{bmatrix}\\). Since \\(P\\) is invertible, we can solve for \\(A\\) and for \\(\\Lambda\\) to get the following equations:\n\n\n\\[AP=P\\Lambda,\\quad P^{-1}AP=\\Lambda,\\quad A=P\\Lambda P^{-1} \\quad\\text{ where }\\quad P=\\begin{bmatrix}\\mathbf{v}_1&\\cdots&\\mathbf{v}_n\\end{bmatrix}.\\]\nConversely, if the matrix \\(A\\), \\(P\\) and \\(\\Lambda\\) satisfy the previous equations, the columns of \\(P\\) are the eigenvectors of \\(A\\) and the diagonal terms of \\(\\Lambda\\) are the eigenvalues. Finally, if \\(A\\) is symmetric, one can choose \\(P\\) to be orthogonal.\n\n\nProblem 7\n\nThe matrix \\(A\\) is factored in the following way: \\(A = \\begin{bmatrix} -2 & 12 \\\\ -1 & 5 \\end{bmatrix} = \\begin{bmatrix} 3 & 4 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} -1 & 4 \\\\ 1 & -3 \\end{bmatrix}\\). Read the eigenvalues and eigenvectors of \\(A\\) from the factorization and check that they satisfy \\(A\\mathbf{v}=\\lambda\\mathbf{v}\\). Find \\(P\\), compute \\(P^{-1}\\) and verify the product.\nLet \\(A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 4 & 5 \\\\ 0 & 0 & -2 \\end{bmatrix}\\):\n\nWithout doing any computation find the eigenvalues of \\(A\\) and explain why \\(A\\) can de diagonalized.\nFind the eigenvectors of \\(A\\), write them in the form of \\(P\\) and \\(\\Lambda\\) and check that \\(P\\) is invertible and that \\(AP=P\\Lambda\\).\n\nLet \\(A = \\begin{bmatrix} -3 & -5 & -2 \\\\ -5 & 0 & -5 \\\\ -2 & -5 & -3 \\end{bmatrix}\\):\n\nUse sympy to find the eigenvalues and eigenvectors of \\(A\\), using the command A.eigenvects(). Then find \\(P\\) and \\(\\Lambda\\) and factor \\(A\\) as before. Check that the product gives you \\(A\\) and check that the eigenvectors are orthogonal.\nUse numpy to find the eigenvalues and eigenvectors of \\(A\\). Since \\(A\\) is symmetric, use the command np.linalg.eigh(A). Make sure that \\(A\\) is a numpy array and that the type is float (A.astype(float)converts the entries of \\(A\\) to floats). Numpy returns a array of eigenvalues and an orthogonal matrix. Check that the matrix is orthogonal, find \\(P\\) and \\(\\Lambda\\) and factor \\(A\\) as before.\n\nRepeat for \\(A = \\begin{bmatrix} -2 & 1 & 2 \\\\ 1 & 2 & 0 \\\\ 2 & 0 & 2 \\end{bmatrix}\\)\nFind the eigenvalues and eigenvectors of \\(A=\\begin{bmatrix} 0 & 1 &-1\\\\  1 & 0 & 1\\\\  1 & 1 & -1\\end{bmatrix}\\). Do we enough linearly independent eigenvectors to diagonalize \\(A\\)? If you do, find \\(P\\), \\(\\Lambda\\) and factor \\(A\\) as before.\nRepeat for \\(A = \\begin{bmatrix} -1 & 0 & 0 \\\\ -2 & -1 & 2 \\\\ -2 & 0 & 1 \\end{bmatrix}\\)",
    "crumbs": [
      "Linear Transformation II",
      "Problems"
    ]
  }
]