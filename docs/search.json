[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "Introduction to Linear Algebra\nThese are the companion notes to the Winter 2025 Linear Algebra class. The course is designed to bridge theoretical understanding with practical applications, recognizing two fundamental aspects of linear algebra:\n\nThe geometric/visual interpretation of linear algebra concepts, helping students move beyond purely algorithmic approaches to understand systems as relationships between vectors and transformations that can be visualized.\nThe integration of computational tools to handle routine calculations, allowing focus on interpretation and real-world applications rather than manual computation.\n\nLinear algebra serves as the mathematical foundation for numerous real-world applications that shape our modern world. You’ll encounter it in:\n\nSciences & Engineering: Including quantum computing, computational biology, signal processing, robotics and autonomous systems\nComputing, Data & Statistics: Including machine learning, computer graphics, statistical analysis, and network modeling\nBusiness & Economics: Including market modeling, optimization, and equilibrium analysis\n\nYou’ll learn to view linear algebra through three lenses: theoretical foundations, geometric intuition, and practical applications - preparing you to apply these concepts across mathematics, sciences, engineering, and beyond.",
    "crumbs": [
      "Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "parts/vectors.html",
    "href": "parts/vectors.html",
    "title": "Vectors",
    "section": "",
    "text": "This section covers vectors.\n\nVectors in \\(\\mathbb{R}^n\\)\nAbstract Vector Spaces\nProblems: Vector Operations and Dot Products",
    "crumbs": [
      "Vectors"
    ]
  },
  {
    "objectID": "chapters/vectors.html",
    "href": "chapters/vectors.html",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "1.1 Addition of vectors and scalar product of vectors\nThe following videos from the Essence of Linear Algebra, from 3Blue1Brown, are exceptionally good. Watch them carefully\nLet \\(\\mathbf{v}=(v_1,\\dots,v_n)\\) and \\(\\mathbf{w}=(w_1,\\dots,w_n)\\) be two vectors in \\(\\mathbb{R}^n\\) and let \\(c\\in\\mathbb{R}\\) be a scalar. The sum of \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) is defined by: \\[\\mathbf{v}+\\mathbf{w}=\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots \\\\ v_n\\end{bmatrix}+\\begin{bmatrix}w_1\\\\w_2\\\\\\vdots\\\\ w_n\\end{bmatrix}=\\begin{bmatrix}v_1+w_1\\\\v_2+w_2\\\\\\vdots\\\\ v_n+w_n\\end{bmatrix}.\\]\nThe scalar product of \\(c\\) and \\(\\mathbf{v}\\) is defined by: \\[c\\mathbf{v}=c\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots\\\\ v_n\\end{bmatrix}=\\begin{bmatrix}cv_1\\\\cv_2\\\\\\vdots\\\\ cv_n\\end{bmatrix}.\\]\nIn \\(\\mathbb{R}^n\\), we represent vectors as column vectors. For convenience in inline text, we sometimes write \\(\\mathbf{v}=(v_1,\\dots,v_n)\\), but this notation should be understood to represent a column vector. This is distinct from a row vector, which we explicitly denote as \\(\\mathbf{v}=\\begin{bmatrix}v_1& v_2 &\\cdots &v_n\\end{bmatrix}\\). The distinction is important because row and column vectors behave differently under matrix operations.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vectors.html#addition-of-vectors-and-scalar-product-of-vectors",
    "href": "chapters/vectors.html#addition-of-vectors-and-scalar-product-of-vectors",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "Properties of addition and scalar product\nLet \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{w}\\) be vectors in \\(\\mathbb{R}^n\\), and let \\(c\\) and \\(d\\) be scalars.\n\n\\(\\mathbf{u} + \\mathbf{v} \\in \\mathbb{R}^n\\) (Closed under addition)\n\\(c\\mathbf{u} \\in \\mathbb{R}^n\\) (Closed under scalar multiplication)\n\\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\) (Commutative property of addition)\n\\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\) (Associative property of addition)\n\\(\\exists \\, \\mathbf{0} \\in \\mathbb{R}^n\\) such that \\(\\mathbf{u} + \\mathbf{0} = \\mathbf{u}\\) (Existence of an additive identity)\n\\(\\forall \\, \\mathbf{u} \\in \\mathbb{R}^n, \\, \\exists \\, -\\mathbf{u}\\) such that \\(\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}\\) (Existence of additive inverses)\n\\(1\\mathbf{u} = \\mathbf{u}\\) (Identity element of scalar multiplication)\n\\((cd)\\mathbf{u} = c(d\\mathbf{u})\\) (Associative property of scalar multiplication)\n\\(c(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}\\) (Distributive property)\n\\((c + d)\\mathbf{u} = c\\mathbf{u} + d\\mathbf{u}\\) (Distributive property)\n\nThese properties characterize sets equipped with addition and scalar multiplication that satisfy the axioms of a vector space. In particular, they define the structure not only of \\(\\mathbb{R}^n\\) but also of more abstract vector spaces, where elements need not be geometric vectors, and scalars may belong to fields other than \\(\\mathbb{R}\\), such as \\(\\mathbb{C}\\) or finite fields.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vectors.html#linear-combinations",
    "href": "chapters/vectors.html#linear-combinations",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "1.2 Linear Combinations",
    "text": "1.2 Linear Combinations\nA linear combination of vectors \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\) in \\(\\mathbb{R}^n\\) is a sum of scalar multiples of these vectors:\n\\[a_1\\mathbf{v}_1 + a_2\\mathbf{v}_2 + \\cdots + a_k\\mathbf{v}_k\\]\nwhere \\(a_1, a_2, \\ldots, a_k\\) are scalars (real numbers).\nLet’s illustrate this with two vectors in \\(\\mathbb{R}^2\\): \\(\\mathbf{v}_1=\\begin{bmatrix}1\\\\1\\end{bmatrix}\\) and \\(\\mathbf{v}_2=\\begin{bmatrix}-1\\\\1\\end{bmatrix}\\). We can create different vectors through linear combinations of \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\):\n\nCombining with positive coefficients: \\[2\\mathbf{v}_1 + \\mathbf{v}_2 = 2\\begin{bmatrix}1\\\\1\\end{bmatrix} + \\begin{bmatrix}-1\\\\1\\end{bmatrix} = \\begin{bmatrix}1\\\\3\\end{bmatrix}\\]\nUsing a negative coefficient: \\[\\mathbf{v}_1 - 3\\mathbf{v}_2 = \\begin{bmatrix}1\\\\1\\end{bmatrix} - 3\\begin{bmatrix}-1\\\\1\\end{bmatrix} = \\begin{bmatrix}4\\\\-2\\end{bmatrix}\\]\nWorking with fractions: \\[\\frac{1}{2}\\mathbf{v}_1 + \\frac{1}{2}\\mathbf{v}_2 = \\frac{1}{2}\\begin{bmatrix}1\\\\1\\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix}-1\\\\1\\end{bmatrix} = \\begin{bmatrix}0\\\\1\\end{bmatrix}\\]\n\nMany questions in linear algebra reduce to solving systems of linear equations. Questions about linear combinations are a prime example, as we’ll see in the following exercise:\n\nExercise: Can we write \\(\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}\\) as a linear combination of the vectors \\(\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}\\), \\(\\begin{bmatrix}4\\\\5\\\\6\\end{bmatrix}\\), \\(\\begin{bmatrix}7\\\\8\\\\9\\end{bmatrix}\\)?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nWe want scalars \\(c_1\\), \\(c_2\\), \\(c_3\\) such that: \\[c_1\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix} + c_2\\begin{bmatrix}4\\\\5\\\\6\\end{bmatrix} + c_3\\begin{bmatrix}7\\\\8\\\\9\\end{bmatrix} = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}\\] This gives the system of equations: \\[\\begin{align*}\nc_1 + 4c_2 + 7c_3 &= 0\\\\\n2c_1 + 5c_2 + 8c_3 &= 1\\\\\n3c_1 + 6c_2 + 9c_3 &= 0.\n\\end{align*}\\] We can use substitution, or elimination, to show that this system has no solution, so \\(\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}\\) is not a linear combination of the given vectors. We’ll cover solutions to such systems extensively later in the course.\n\n\n\n\nLinear combinations are fundamental in linear algebra and have numerous applications, such as:\n\nExpressing a vector in terms of other vectors\nSolving systems of linear equations\nDescribing lines, planes, and hyperplanes in \\(\\mathbb{R}^n\\)\nAnalyzing linear transformations and matrices",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vectors.html#sec-span",
    "href": "chapters/vectors.html#sec-span",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "1.3 Span",
    "text": "1.3 Span\nThe set of all possible linear combinations of a given set of vectors is known as the span of those vectors, and it has important properties.\nLet’s start with a precise definition. If \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\) are vectors in \\(\\mathbb{R}^n\\), then \\[\\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right) =\\{ c_1\\mathbf{v}_1+\\cdots+c_k\\mathbf{v}_k:c_1,\\dots,c_k\\in\\mathbb{R} \\}\\]\nNotice that \\(\\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\) is a subset of \\(\\mathbb{R}^n\\). When working with sets, we typically focus on two key questions:\n\nHow do we verify if an element belongs to the set?\nWhat properties can we deduce when we know an element belongs to the set?\n\nFor the span of vectors \\(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\):\nVerification: To check if \\(\\mathbf{v}\\) is in the span, we solve a system of equations. Consider:\nIf \\(\\mathbf{v}_1 = \\begin{bmatrix}1\\\\2\\\\1\\end{bmatrix}\\), \\(\\mathbf{v}_2 = \\begin{bmatrix}0\\\\1\\\\1\\end{bmatrix}\\), and \\(\\mathbf{v} = \\begin{bmatrix}2\\\\5\\\\3\\end{bmatrix}\\)\nTo check if \\(\\mathbf{v}\\) is in span\\((\\{\\mathbf{v}_1,\\mathbf{v}_2\\})\\), we ask: do there exist \\(c_1,c_2\\) such that \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 = \\mathbf{v}\\)?\nThis gives us: \\[\\begin{align*}\nc_1(1) + c_2(0) &= 2\\\\\nc_1(2) + c_2(1) &= 5\\\\\nc_1(1) + c_2(1) &= 3\n\\end{align*}\\]\nIf we find values for \\(c_1,c_2\\) satisfying all equations, then \\(\\mathbf{v}\\) is in the span. If no such values exist, \\(\\mathbf{v}\\) is not in the span.\nProperties: If a vector \\(\\mathbf{w}\\) is in \\(\\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\), we know that there exist constants \\(c_1,\\dots,c_n\\in\\mathbb{R}\\) such that \\(\\mathbf{w}=c_1\\mathbf{v}_1+\\cdots+c_k\\mathbf{v}_k\\).\nWe use these properties to deduce important results:\n\nTheorem 1.1 Suppose that \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\) are vectors in \\(\\mathbb{R}^n\\). Then\n\nIf \\(\\mathbf{w}_1\\) and \\(\\mathbf{w}_2\\) are in \\(\\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\), then \\(\\mathbf{w}_1+\\mathbf{w}_2\\) is also in \\(\\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\).\nIf \\(\\mathbf{w}\\in \\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\), and \\(c\\in\\mathbb{R}\\), then \\(c\\mathbf{w}\\in \\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\).\n\n\n\nProof. We only check (1). The proof of (2) is similar. Since \\(\\mathbf{w}_1\\) is in the span, there exist \\(a_1,\\dots,a_k\\) with \\(\\mathbf{w}_1 = a_1\\mathbf{v}_1+\\cdots+a_k\\mathbf{v}_k\\). Similarly, there exist \\(b_1,\\dots,b_k\\) with \\(\\mathbf{w}_2 = b_1\\mathbf{v}_1+\\cdots+b_k\\mathbf{v}_k\\). Then: \\[\\mathbf{w}_1 + \\mathbf{w}_2 = (a_1+b_1)\\mathbf{v}_1+\\cdots+(a_k+b_k)\\mathbf{v}_k\\] showing \\(\\mathbf{w}_1 + \\mathbf{w}_2\\) is in the span. \\(\\square\\)\n\nVisualizing Vector Spans in \\(\\mathbb{R}^3\\)\n\nSpan of a Single Vector Given \\(\\mathbf{v} \\in \\mathbb{R}^3\\), the \\(\\text{span}({\\mathbf{v}})\\) is:\n\n\nA point at origin if \\(\\mathbf{v} = \\mathbf{0}\\)\nA line through the origin if \\(\\mathbf{v} \\neq \\mathbf{0}\\), containing all scalar multiples of \\(\\mathbf{v}\\)\n\n\nSpan of Two Vectors For nonzero vectors \\(\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^3\\), the \\(\\text{span}({\\mathbf{v}, \\mathbf{w}})\\) is:\n\n\nA line through origin if the vectors are parallel (one is a scalar multiple of the other)\nA plane through origin otherwise, containing all linear combinations \\(s\\mathbf{v} + t\\mathbf{w}\\) where \\(s,t \\in \\mathbb{R}\\)\n\n\nSpan of Multiple Vectors Consider the set of vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3, \\mathbf{v}_4\\}\\): \\[\n\\mathbf{v}_1 = \\begin{bmatrix}1\\\\1\\\\3\\end{bmatrix}, \\;\n\\mathbf{v}_2 = \\begin{bmatrix}-2\\\\-2\\\\-6\\end{bmatrix}, \\;\n\\mathbf{v}_3 = \\begin{bmatrix}1\\\\-2\\\\5\\end{bmatrix}, \\;\n\\mathbf{v}_4 = \\begin{bmatrix}0\\\\3\\\\-2\\end{bmatrix}\n\\]\n\nThe span of these vectors is the set of all possible linear combinations: \\[\\text{span}(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3, \\mathbf{v}_4\\}) = \\{t_1\\mathbf{v}_1 + t_2\\mathbf{v}_2 + t_3\\mathbf{v}_3 + t_4\\mathbf{v}_4 : t_1,t_2,t_3,t_4 \\in \\mathbb{R}\\}.\\]\nTheir span is visualized by the following graph and we see that all the vectors are in one plane.\n\n\n                                                \n\n\nThe span of a set of vectors in \\(\\mathbb{R}^3\\) must be one of exactly four geometric objects:\n\nA single point (specifically, the origin \\((0,0,0)\\))\nA line passing through the origin\nA plane containing the origin\nAll of \\(\\mathbb{R}^3\\) (the entire three-dimensional space)",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vectors.html#dot-product-in-mathbbrn",
    "href": "chapters/vectors.html#dot-product-in-mathbbrn",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "1.4 Dot Product in \\(\\mathbb{R}^n\\)",
    "text": "1.4 Dot Product in \\(\\mathbb{R}^n\\)\nLet \\(\\mathbf{v}=(v_1,\\dots,v_n)\\) and \\(\\mathbf{w}=(w_1,\\dots,w_n)\\) be two vectors in \\(\\mathbb{R}^n\\). The dot product of \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) is defined by: \\[\\mathbf{v}\\cdot\\mathbf{w}=\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots\\\\ v_n\\end{bmatrix}\\cdot \\begin{bmatrix}w_1\\\\w_2\\\\\\vdots\\\\ w_n\\end{bmatrix}= \\sum_{i=1}^nv_iw_i.\\]\n\n1.4.1 Properties of the Dot Product in \\(\\mathbb{R}^n\\)\nThe dot product has three fundamental properties that can be verified directly from its definition. These properties form the foundation for many calculations and proofs in linear algebra.\nLet \\(\\mathbf{u}\\), \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) be vectors in \\(\\mathbb{R}^n\\), and let \\(a\\in\\mathbb{R}\\) and \\(b\\in\\mathbb{R}\\) be scalars.\n\n\\(\\mathbf{v} \\cdot \\mathbf{v} \\geq 0\\) and \\(\\mathbf{v} \\cdot \\mathbf{v} = 0\\) if and only if \\(\\mathbf{v} = \\mathbf{0}\\) (Positive Definite)\n\\(\\mathbf{v} \\cdot \\mathbf{w} = \\mathbf{w} \\cdot \\mathbf{v}\\) (Symmetric)\n\\(\\mathbf{u}\\cdot(a\\mathbf{v} + b\\mathbf{w}) = a(\\mathbf{u} \\cdot \\mathbf{v}) + b(\\mathbf{u} \\cdot \\mathbf{w})\\) and \\((a\\mathbf{u} + b\\mathbf{v}) \\cdot \\mathbf{w} = a(\\mathbf{u} \\cdot \\mathbf{w}) + b(\\mathbf{v} \\cdot \\mathbf{w})\\) (Linear in each Variable)\n\nExercise: Suppose that \\(\\mathbf{v}_1,\\mathbf{v}_2,\\mathbf{w}_1,\\mathbf{w}_2\\in\\mathbb{R}^n\\) and that we know the values of \\(\\mathbf{v}_i\\cdot\\mathbf{w}_j\\). Find \\[(2\\mathbf{v}_1+3\\mathbf{v}_2)\\cdot(\\mathbf{w}_1-2\\mathbf{w}_2)\\] in terms of the \\(\\mathbf{v}_i\\cdot\\mathbf{w}_j\\)’s.\n\n\n\n\n\n\nClick to see the answer\n\n\n\n\n\n\nFirst, recall the key linearity properties:\n\nLinear in first variable: \\((c_1\\mathbf{a}_1 + c_2\\mathbf{a}_2)\\cdot\\mathbf{b} = c_1(\\mathbf{a}_1\\cdot\\mathbf{b}) + c_2(\\mathbf{a}_2\\cdot\\mathbf{b})\\)\nLinear in second variable: \\(\\mathbf{a}\\cdot(c_1\\mathbf{b}_1 + c_2\\mathbf{b}_2) = c_1(\\mathbf{a}\\cdot\\mathbf{b}_1) + c_2(\\mathbf{a}\\cdot\\mathbf{b}_2)\\)\n\nLet’s start with the first variable using linearity:\n\n\\((2\\mathbf{v}_1+3\\mathbf{v}_2)\\cdot(\\mathbf{w}_1-2\\mathbf{w}_2)\\)\n\\(= 2\\mathbf{v}_1\\cdot(\\mathbf{w}_1-2\\mathbf{w}_2) + 3\\mathbf{v}_2\\cdot(\\mathbf{w}_1-2\\mathbf{w}_2)\\)\n\nNow apply linearity in the second variable for each term:\n\n\\(= 2(\\mathbf{v}_1\\cdot\\mathbf{w}_1 - 2\\mathbf{v}_1\\cdot\\mathbf{w}_2) + 3(\\mathbf{v}_2\\cdot\\mathbf{w}_1 - 2\\mathbf{v}_2\\cdot\\mathbf{w}_2)\\)\n\nExpand this:\n\n\\(= 2\\mathbf{v}_1\\cdot\\mathbf{w}_1 - 4\\mathbf{v}_1\\cdot\\mathbf{w}_2 + 3\\mathbf{v}_2\\cdot\\mathbf{w}_1 - 6\\mathbf{v}_2\\cdot\\mathbf{w}_2\\)\n\n\nNow we can use the given values of \\(\\mathbf{v}_i\\cdot\\mathbf{w}_j\\) to compute the final result by substituting those values into this expression.\nThe final formula in terms of the known dot products is: \\[2(\\mathbf{v}_1\\cdot\\mathbf{w}_1) - 4(\\mathbf{v}_1\\cdot\\mathbf{w}_2) + 3(\\mathbf{v}_2\\cdot\\mathbf{w}_1) - 6(\\mathbf{v}_2\\cdot\\mathbf{w}_2).\\]\n\n\n\n\n\n1.4.2 Norm\nThe dot product induces a norm on \\(\\mathbb{R}^n\\). The norm of a vector \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\in\\mathbb{R}^n\\) is given by:\n\\[\\|\\mathbf{v}\\| = \\sqrt{\\mathbf{v}\\cdot\\mathbf{v}} = \\sqrt{\\sum_{i=1}^n |v_i|^2}\\]\nThe norm is also known as the magnitude or length of a vector. When we compute the norm or when we check properties, we often look at \\(\\|\\mathbf{v}\\|^2=\\mathbf{v}\\cdot\\mathbf{v}\\) to avoid the square root.\nThe norm satisfies the following properties:\n\nNon-negativity: \\(\\|\\mathbf{v}\\| \\geq 0\\) for all \\(\\mathbf{v} \\in \\mathbb{R}^n\\)\nDefiniteness: \\(\\|\\mathbf{v}\\| = 0\\) if and only if \\(\\mathbf{v} = \\mathbf{0}\\)\nHomogeneity: \\(\\|c\\mathbf{v}\\| = |c| \\|\\mathbf{v}\\|\\) for all \\(c \\in \\mathbb{R}\\) and \\(\\mathbf{v} \\in \\mathbb{R}^n\\)\nTriangle inequality: \\(\\|\\mathbf{v} + \\mathbf{w}\\| \\leq \\|\\mathbf{v}\\| + \\|\\mathbf{w}\\|\\) for all \\(\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^n\\)\n\nThe first three properties are easy to verify. The Triangle Inequality can be proved using Cauchy-Schwarz inequality that says that \\(|\\mathbf{v} \\cdot \\mathbf{w}| \\leq \\|\\mathbf{v}\\| \\|\\mathbf{w}\\|\\)\n\nExercise: Use Cauchy-Schwarz Inequality to prove the Triangle Inequality of the norm.\n\n\n\n\n\n\nClick to see the proof\n\n\n\n\n\nWe start with the squared norm of \\(\\mathbf{v} + \\mathbf{w}\\):\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 = (\\mathbf{v} + \\mathbf{w}) \\cdot (\\mathbf{v} + \\mathbf{w})\\]\nExpanding the right-hand side using the properties of the dot product, we get:\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 = \\mathbf{v} \\cdot \\mathbf{v} + 2(\\mathbf{v} \\cdot \\mathbf{w}) + \\mathbf{w} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\|^2 + 2(\\mathbf{v} \\cdot \\mathbf{w}) + \\|\\mathbf{w}\\|^2\\]\nNow, we apply the Cauchy-Schwarz Inequality to the term \\(2(\\mathbf{v} \\cdot \\mathbf{w})\\):\n\\[2(\\mathbf{v}\\cdot\\mathbf{w})\\leq2|\\mathbf{v} \\cdot \\mathbf{w}| \\leq 2\\|\\mathbf{v}\\| \\|\\mathbf{w}\\|\\]\nSubstituting this into the previous equation, we obtain:\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 \\leq \\|\\mathbf{v}\\|^2 + 2\\|\\mathbf{v}\\| \\|\\mathbf{w}\\| + \\|\\mathbf{w}\\|^2 = (\\|\\mathbf{v}\\| + \\|\\mathbf{w}\\|)^2\\]\nTaking the square root of both sides (which is valid since both sides are non-negative) yields:\n\\[\\|\\mathbf{v} + \\mathbf{w}\\| \\leq \\|\\mathbf{v}\\| + \\|\\mathbf{w}\\|\\]\nwhich is the Triangle Inequality for the norm in \\(\\mathbb{R}^n\\). \\(\\square\\)\n\n\n\n\n\n\n1.4.3 Orthogonality and Cauchy-Schwarz Inequality\nTwo vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) in \\(\\mathbb{R}^n\\) are orthogonal if \\(\\mathbf{v}\\cdot\\mathbf{w}=0\\). Orthogonality is a central topic in linear algebra and has numerous applications in various fields, such as:\n\nCoordinate systems and basis vectors\nLeast squares approximation and regression analysis\nFourier series and signal processing\nQuantum mechanics and Hilbert spaces\n\n\nTheorem 1.2 (Pythagorean Theorem) If \\(\\mathbf{v}, \\mathbf{w}\\in \\mathbb{R}^n\\) are orthogonal vectors, then \\(\\|\\mathbf{v} + \\mathbf{w}\\|^2= \\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2\\)\n\n\nProof. Let \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\) and \\(\\mathbf{w} = (w_1, w_2, \\ldots, w_n)\\) be orthogonal vectors in \\(\\mathbb{R}^n\\). We start with the squared norm of \\(\\mathbf{v} + \\mathbf{w}\\):\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 = (\\mathbf{v} + \\mathbf{w}) \\cdot (\\mathbf{v} + \\mathbf{w})\\]\nExpanding the right-hand side using the properties of the dot product, we get:\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 = \\mathbf{v} \\cdot \\mathbf{v} + 2(\\mathbf{v} \\cdot \\mathbf{w}) + \\mathbf{w} \\cdot \\mathbf{w}\\]\nSince \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) are orthogonal, \\(\\mathbf{v} \\cdot \\mathbf{w} = 0\\). Substituting this into the equation above, we obtain:\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 = \\mathbf{v} \\cdot \\mathbf{v} + \\mathbf{w} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2\\]\nwhich is the Pythagorean Theorem for orthogonal vectors in \\(\\mathbb{R}^n\\). \\(\\square\\)\n\nA similar result is the Parallelogram Law, that says that for any two vectors \\(\\mathbf{v}, \\mathbf{w}\\in \\mathbb{R}^n\\):\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 + \\|\\mathbf{v} - \\mathbf{w}\\|^2 = 2(\\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2)\\]\nWhen \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) are orthogonal, \\(\\|\\mathbf{v} + \\mathbf{w}\\|^2 = \\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2\\) and \\(\\|\\mathbf{v} - \\mathbf{w}\\|^2 = \\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2\\). Then the Parallelogram Law reduces to the Pythagorean Theorem.\n\nExercise: Prove the Parallelogram Law\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nExpand both squared norms using the dot product definition: \\(|\\mathbf{u}|^2 = \\mathbf{u} \\cdot \\mathbf{u}\\)\nFor the left side, you’ll get terms with \\(\\mathbf{v} \\cdot \\mathbf{v}\\), \\(\\mathbf{w} \\cdot \\mathbf{w}\\), and \\(\\mathbf{v} \\cdot \\mathbf{w}\\)\nPay attention to the signs of the cross terms \\(\\mathbf{v} \\cdot \\mathbf{w}\\) in both expansions\n\n\n\n\n\n\nTheorem 1.3 (Theorem: Cauchy-Schwarz Inequality) Let \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\) and \\(\\mathbf{w} = (w_1, w_2, \\ldots, w_n)\\) be two vectors in \\(\\mathbb{R}^n\\). Then \\(|\\mathbf{v} \\cdot \\mathbf{w}| \\leq \\|\\mathbf{v}\\| \\|\\mathbf{w}\\|\\)\n\n\nProof. Let \\(t \\in \\mathbb{R}\\) be a scalar. Consider the non-negative quantity \\(\\|\\mathbf{v} - t\\mathbf{w}\\|^2\\):\n\\[\\|\\mathbf{v} - t\\mathbf{w}\\|^2 \\geq 0\\]\nExpanding the left-hand side using the properties of the dot product, we get:\n\\[(\\mathbf{v} - t\\mathbf{w}) \\cdot (\\mathbf{v} - t\\mathbf{w}) = \\mathbf{v} \\cdot \\mathbf{v} - 2t(\\mathbf{v} \\cdot \\mathbf{w}) + t^2(\\mathbf{w} \\cdot \\mathbf{w}) \\geq 0\\]\nThis inequality holds for all values of \\(t\\). Let’s choose \\(t\\) to be the value that minimizes the left-hand side:\n\\[t = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{\\|\\mathbf{w}\\|^2}\\]\nSubstituting this value of \\(t\\) into the inequality, we obtain:\n\\[\\|\\mathbf{v}\\|^2 - 2\\frac{(\\mathbf{v} \\cdot \\mathbf{w})^2}{\\|\\mathbf{w}\\|^2} + \\frac{(\\mathbf{v} \\cdot \\mathbf{w})^2}{\\|\\mathbf{w}\\|^2} \\geq 0\\]\nSimplifying the left-hand side:\n\\[\\|\\mathbf{v}\\|^2 - \\frac{(\\mathbf{v} \\cdot \\mathbf{w})^2}{\\|\\mathbf{w}\\|^2} \\geq 0\\]\nMultiplying both sides by \\(\\|\\mathbf{w}\\|^2\\) yields:\n\\[\\|\\mathbf{v}\\|^2\\|\\mathbf{w}\\|^2 \\geq (\\mathbf{v} \\cdot \\mathbf{w})^2\\]\nTaking the square root of both sides (which is valid since both sides are non-negative) gives:\n\\[\\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\geq |\\mathbf{v} \\cdot \\mathbf{w}|\\]\nwhich is the Cauchy-Schwarz Inequality. \\(\\square\\)\n\n\n\n\n\n\n\nClick to see an alternative proof of Cauchy-Schawrz\n\n\n\n\n\nSuppose that neither \\(\\mathbf{v}\\) nor \\(\\mathbf{w}\\) are zero and that one is not a multiple of the other.\nFor any scalar \\(t\\in\\mathbb{R}\\), we can write \\(\\mathbf{w}\\) as the sum of two vectors: \\(\\mathbf{w} = t\\mathbf{v}+(\\mathbf{w}-t\\mathbf{v})\\). Our goal is to find \\(t\\in\\mathbb{R}\\) such that \\(t\\mathbf{v}\\) and \\(\\mathbf{w}-t\\mathbf{v}\\) are orthogonal. For such a \\(t\\), \\[\\|\\mathbf{w}\\|^2 = t^2\\|\\mathbf{v}\\|^2+\\|\\mathbf{w}-t\\mathbf{v}\\|^2.\\] In particular, \\[t^2\\|\\mathbf{v}\\|^2\\leq\\|\\mathbf{w}\\|^2.\\] From the equation \\(\\mathbf{v}\\cdot (\\mathbf{w}-t\\mathbf{v})=0\\), we find that \\(t=\\frac{\\mathbf{v}\\cdot \\mathbf{w}}{\\mathbf{v}\\cdot \\mathbf{v}}\\). When we substitute this into \\[t^2\\|\\mathbf{v}\\|^2\\leq\\|\\mathbf{w}\\|^2,\\] and simplify we get the Cauchy-Schwarz inequality.\n\n\n\n\n\n1.4.4 Geometric Interpretation of the Dot Product\nThe dot product of two vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) in \\(\\mathbb{R}^n\\) can be expressed in terms of their norms and the angle between them: \\[\\mathbf{v} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\cos(\\theta)\\] where \\(\\theta\\) is the angle between \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\). This relationship highlights the geometric interpretation of the dot product. When \\(\\theta = 0°\\), the vectors are parallel, and the dot product equals the product of their norms. When \\(\\theta = 90°\\), the vectors are orthogonal, and the dot product is zero. The Cauchy-Schwarz Inequality follows directly from this relationship, as \\(|\\cos(\\theta)| \\leq 1\\).\nWe can verify this easily in \\(\\mathbb{R}^2\\). Consider two vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) in \\(\\mathbb{R}^2\\). Let the angle that vector \\(\\mathbf{v}\\) makes with the positive x-axis be \\(\\alpha\\), and the angle that vector \\(\\mathbf{w}\\) makes with the positive x-axis be \\(\\alpha + \\beta\\), where \\(\\beta\\) is the angle between vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\).\nThe vectors can be expressed in terms of their magnitudes and angles: \\(\\mathbf{v} = (\\|\\mathbf{v}\\| \\cos(\\alpha), \\|\\mathbf{v}\\| \\sin(\\alpha))\\) and \\(\\mathbf{w} = (\\|\\mathbf{w}\\| \\cos(\\alpha + \\beta), \\|\\mathbf{w}\\| \\sin(\\alpha + \\beta))\\). The dot product of these vectors is:\n\\[\\mathbf{v} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| (\\cos(\\alpha) \\cos(\\alpha + \\beta) + \\sin(\\alpha) \\sin(\\alpha + \\beta))\\]\nUsing the angle addition formulas:\n\\[\\cos(\\alpha + \\beta) = \\cos(\\alpha) \\cos(\\beta) - \\sin(\\alpha) \\sin(\\beta)\\] \\[\\sin(\\alpha + \\beta) = \\sin(\\alpha) \\cos(\\beta) + \\cos(\\alpha) \\sin(\\beta)\\]\nWe show that \\(\\cos(\\alpha) \\cos(\\alpha + \\beta) + \\sin(\\alpha) \\sin(\\alpha + \\beta) = \\cos(\\beta)\\) and we conclude that \\[\\mathbf{v} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\cos(\\beta).\\]\n\nExercise: Prove that \\(\\cos(\\alpha) \\cos(\\alpha + \\beta) + \\sin(\\alpha) \\sin(\\alpha + \\beta) = \\cos(\\beta)\\)\n\n\n\n\n\n\nClick to see the proof\n\n\n\n\n\n\\[\\begin{aligned}\n\\cos(\\alpha) \\cos(\\alpha + \\beta) + \\sin(\\alpha) \\sin(\\alpha + \\beta) &=\n\\cos(\\alpha) (\\cos(\\alpha) \\cos(\\beta) - \\sin(\\alpha) \\sin(\\beta)) \\\\\n&\\quad + \\sin(\\alpha) (\\sin(\\alpha) \\cos(\\beta) + \\cos(\\alpha) \\sin(\\beta))) \\\\\n&= (\\cos^2(\\alpha) \\cos(\\beta) - \\cos(\\alpha) \\sin(\\alpha) \\sin(\\beta) \\\\\n&\\quad + \\sin^2(\\alpha) \\cos(\\beta) + \\cos(\\alpha) \\sin(\\alpha) \\sin(\\beta)) \\\\\n&= (\\cos^2(\\alpha) \\cos(\\beta) + \\sin^2(\\alpha) \\cos(\\beta)) \\\\\n&= (\\cos^2(\\alpha) + \\sin^2(\\alpha)) \\cos(\\beta) \\\\\n&= \\cos(\\beta)\n\\end{aligned}\\]\n\n\n\n\n\n\n1.4.5 Distance\nThe norm induces a distance (or metric) on \\(\\mathbb{R}^n\\), the distance between two vectors \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\) and \\(\\mathbf{w} = (w_1, w_2, \\ldots, w_n)\\) is given by:\n\\[d(\\mathbf{v}, \\mathbf{w}) =\\|\\mathbf{v}-\\mathbf{w}\\| = \\sqrt{\\sum_{i=1}^n |v_i - w_i|^2}\\]\nThis distance is known as the Euclidean distance. It satisfies the following properties:\n\nNon-negativity: \\(d(\\mathbf{v}, \\mathbf{w}) \\geq 0\\) for all \\(\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^n\\)\nDefiniteness: \\(d(\\mathbf{v}, \\mathbf{w}) = 0\\) if and only if \\(\\mathbf{v} = \\mathbf{w}\\)\nSymmetry: \\(d(\\mathbf{v}, \\mathbf{w}) = d(\\mathbf{w}, \\mathbf{v})\\) for all \\(\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^n\\)\nTriangle inequality: \\(d(\\mathbf{v}, \\mathbf{z}) \\leq d(\\mathbf{v}, \\mathbf{w}) + d(\\mathbf{w}, \\mathbf{z})\\) for all \\(\\mathbf{v}, \\mathbf{w}, \\mathbf{z} \\in \\mathbb{R}^n\\)\n\nThe induced distance has numerous applications in various fields, such as:\n\nClustering and classification in machine learning\nMeasuring similarity or dissimilarity between objects or data points\nOptimization problems in operations research\nError analysis and approximation theory in numerical analysis\n\nUnderstanding the relationships between dot product, norm, and distance is crucial in applications in mathematics, physics, computer science, and engineering.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vectors.html#vector-computation-in-python",
    "href": "chapters/vectors.html#vector-computation-in-python",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "1.5 Vector Computation in Python",
    "text": "1.5 Vector Computation in Python\nPython provides various ways to work with vectors and mathematical computations. Here’s an overview of the main approaches:\n\nBuilt-in Python Lists:\n\nBasic vector operations require explicit formulas or list comprehensions\nUseful for understanding the underlying computations\nNot optimized for large-scale numerical calculations\n\nNumPy (Numerical Python):\n\nIndustry-standard library for numerical computing\nProvides efficient array operations and mathematical functions\nOptimized for performance with vectorized operations\nEssential for scientific computing and data analysis\n\nSymPy (Symbolic Python):\n\nComputer algebra system for symbolic mathematics\nHandles mathematical expressions with variables and symbols\nPerfect for mathematical proofs and algebraic manipulations\nUseful for verifying theoretical results\n\n\nIn this class, we will primarily use NumPy and Sympy. For solving linear systems, NumPy uses numerically stable methods like QR decomposition rather than Gaussian elimination. While Gaussian elimination is a foundational algorithm taught in linear algebra courses for its theoretical importance and intuitive approach, it can be numerically unstable in practice. SymPy provides a direct implementation of Gaussian elimination, making it useful for understanding the algorithm and verifying theoretical results.\n\n1.5.1 Representing Vectors\nIn Python, we can represent vectors using lists, in NumPy we use arrays, and in Sympy we use vectors, that are implemented with the function Matrix. Notice that Sympy shows vectors as column vectors.\n\nimport numpy as np\nfrom sympy import Matrix\n\n# Using Python lists\nv = [1, 2, 3]\nw = [4, 5, 6]\nprint(v)  # Output: [51,2,3]\n\n# Using NumPy arrays\nv_np = np.array([1, 2, 3])\nw_np = np.array([4, 5, 6])\nprint(v_np)  # Output: [1 2 3]\n\n# Using Sympy vectors\nv_sp = Matrix([1,2,3])\nw_sp = Matrix([4,5,6])\n\n[1, 2, 3]\n[1 2 3]\n\n\n\n# Showing v_sp\nv_sp\n\n\\(\\displaystyle \\left[\\begin{matrix}1\\\\2\\\\3\\end{matrix}\\right]\\)\n\n\n\n\n1.5.2 Vector Addition\nThe + is already implemented in Numpy and Sympy.\n\n# Using Python lists\nresult = [v[i] + w[i] for i in range(len(v))]\nprint(result)  # Output: [5, 7, 9]\n\n# Using NumPy arrays\nresult_np = v_np + w_np\nprint(result_np)  # Output: [5 7 9]\n\n[5, 7, 9]\n[5 7 9]\n\n\n\n# Using Sympy vectors\nv_sp + w_sp\n\n\\(\\displaystyle \\left[\\begin{matrix}5\\\\7\\\\9\\end{matrix}\\right]\\)\n\n\n\n\n1.5.3 Scalar Multiplication\nThe scalar multiplication is already implemented in Numpy and Sympy.\n\nscalar = 2\n\n# Using Python lists\nresult = [scalar * x for x in v]\nprint(result)  # Output: [2, 4, 6]\n\n# Using NumPy arrays\nresult_np = scalar * v_np\nprint(result_np)  # Output: [2 4 6]\n\n[2, 4, 6]\n[2 4 6]\n\n\n\n# Using Sympy vectors\nscalar * v_sp\n\n\\(\\displaystyle \\left[\\begin{matrix}2\\\\4\\\\6\\end{matrix}\\right]\\)\n\n\n\n\n1.5.4 Dot Product\nThe dot product is already implemented in Numpy and Sympy.\n\n# Using Python lists\ndot_product = sum([v[i] * w[i] for i in range(len(v))])\nprint(dot_product)  # Output: 32\n\n# Using NumPy arrays\ndot_product_np = np.dot(v_np, w_np)\nprint(dot_product_np)  # Output: 32\n\n# Using Sympy vectors\ndot_product_sp = v_sp.dot(w_sp)\nprint(dot_product_sp)  # Output: 32\n\n32\n32\n32\n\n\n\n\n1.5.5 Vector Norms\nThe norm is already implemented in Numpy and Sympy, but it is inside the linalg library of Numpy.\n\nimport math\n\n# Using Python lists\nnorm = math.sqrt(sum([x**2 for x in v]))\nprint(norm)  # Output: 3.7416573867739413\n\n# Using NumPy arrays\nnorm_np = np.linalg.norm(v_np)\nprint(norm_np)  # Output: 3.7416573867739413\n\n# Using Symoy vectors\nnorm_sp = v_sp.norm()\nprint(norm_sp)  # Output: sqrt(14) which is 3.7416573867739413\n\n3.7416573867739413\n3.7416573867739413\nsqrt(14)",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vector_spaces.html",
    "href": "chapters/vector_spaces.html",
    "title": "2  Abstract Vector Spaces",
    "section": "",
    "text": "2.1 Definition\nThe step from \\(\\mathbb{R}^n\\) to abstract vector spaces reflects a fundamental principle in mathematics: identifying common patterns to unify seemingly different objects. While \\(\\mathbb{R}^n\\) provides a concrete and visualizable model, the abstract framework reveals that spaces of functions, polynomials, and solutions to some differential equations share the exact same algebraic structure. This abstraction is not just elegant—it’s immensely practical. When we prove a theorem about vector spaces in general, it automatically applies to all these examples at once.\nA vector space \\(V\\) over \\(\\mathbb{R}\\) is a set equipped with two operations: vector addition (\\(+\\)) and scalar multiplication (\\(\\cdot\\)). For \\(\\mathbf{u},\\mathbf{v},\\mathbf{w}\\in V\\) and scalars \\(a,b\\in\\mathbb{R}\\), these operations must satisfy:\nVector Addition Properties:\nScalar Multiplication Properties:\nExamples: Vector spaces appear naturally throughout mathematics and its applications. While we’ll encounter many examples throughout this course, here are a few fundamental ones to illustrate how diverse they can be.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abstract Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/vector_spaces.html#definition",
    "href": "chapters/vector_spaces.html#definition",
    "title": "2  Abstract Vector Spaces",
    "section": "",
    "text": "Commutativity: \\(\\mathbf{u}+\\mathbf{v}=\\mathbf{v}+\\mathbf{u}\\)\nAssociativity: \\((\\mathbf{u}+\\mathbf{v})+\\mathbf{w}=\\mathbf{u}+(\\mathbf{v}+\\mathbf{w})\\)\nZero vector: There exists \\(\\mathbf{0}\\in V\\) such that \\(\\mathbf{v}+\\mathbf{0}=\\mathbf{v}\\) for all \\(\\mathbf{v}\\in V\\)\nAdditive inverse: For each \\(\\mathbf{v}\\in V\\), there exists \\(-\\mathbf{v}\\in V\\) such that \\(\\mathbf{v}+(-\\mathbf{v})=\\mathbf{0}\\)\n\n\n\nDistributivity over vector addition: \\(a(\\mathbf{u}+\\mathbf{v})=a\\mathbf{u}+a\\mathbf{v}\\)\nDistributivity over scalar addition: \\((a+b)\\mathbf{v}=a\\mathbf{v}+b\\mathbf{v}\\)\nAssociativity: \\(a(b\\mathbf{v})=(ab)\\mathbf{v}\\)\nIdentity: \\(1\\mathbf{v}=\\mathbf{v}\\)\n\n\n\n\\(\\mathbb{R}^n\\): Our familiar space of n-tuples of real numbers with the standard addition and scalar multiplication.\nFunction spaces:\n\n\\(C[a,b]\\): Continuous functions on \\([a,b]\\)\n\nSum: \\((f + g)(x) = f(x) + g(x)\\)\nScalar multiplication: \\((cf)(x) = c\\cdot f(x)\\)\n\n\\(C^\\infty(\\mathbb{R})\\): Infinitely differentiable functions\n\nSum: \\((f + g)(x) = f(x) + g(x)\\)\nScalar multiplication: \\((cf)(x) = c\\cdot f(x)\\)\n\n\nPolynomial spaces:\n\n\\(\\mathbb{P}_n\\): Polynomials of degree \\(\\leq n\\)\n\nSum: \\((p + q)(x) = p(x) + q(x)\\)\nScalar multiplication: \\((cp)(x) = c\\cdot p(x)\\)\n\n\\(\\mathbb{P}\\): All polynomials\n\nSum: \\((p + q)(x) = p(x) + q(x)\\)\nScalar multiplication: \\((cp)(x) = c\\cdot p(x)\\)",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abstract Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/vector_spaces.html#subspaces-of-vector-spaces",
    "href": "chapters/vector_spaces.html#subspaces-of-vector-spaces",
    "title": "2  Abstract Vector Spaces",
    "section": "2.2 Subspaces of Vector Spaces",
    "text": "2.2 Subspaces of Vector Spaces\nWithin any vector space, certain subsets naturally inherit the vector space structure. These special subsets, called subspaces, play a fundamental role in linear algebra.\n\nDefinition: A subset \\(W\\) of the vector space \\(V\\) is called a subspace if it satisfies three conditions:\n\nThe zero vector \\(\\mathbf{0}\\) is in \\(W\\)\nFor all \\(\\mathbf{u},\\mathbf{v}\\in W\\), their sum \\(\\mathbf{u}+\\mathbf{v}\\) is also in \\(W\\) (closed under addition)\nFor all \\(\\mathbf{v}\\in W\\) and all scalars \\(c\\in\\mathbb{R}\\), the vector \\(c\\mathbf{v}\\) is in \\(W\\) (closed under scalar multiplication)\n\n\nThese conditions ensure that \\(W\\) inherits the vector space structure from \\(V\\), making it a vector space in its own right and providing us with a rich source of new examples.\n\nTheorem 2.1 Theorem: Every subspace of a vector space \\(V\\) is itself a vector space.\n\n\nProof. Let \\(W\\) be a subspace of the vector space \\(V\\). We must verify all eight vector space properties.\nVector Addition Properties:\n\nCommutativity: Let \\(\\mathbf{u},\\mathbf{v}\\in W\\). Since \\(W\\subseteq V\\), we know \\(\\mathbf{u}+\\mathbf{v}=\\mathbf{v}+\\mathbf{u}\\), as this holds in \\(V\\).\nAssociativity: Let \\(\\mathbf{u},\\mathbf{v},\\mathbf{w}\\in W\\). Since \\(W\\subseteq V\\), we know that \\((\\mathbf{u}+\\mathbf{v})+\\mathbf{w}=\\mathbf{u}+(\\mathbf{v}+\\mathbf{w})\\), as this holds in \\(V\\).\nZero vector: This is given directly by subspace property 1.\nAdditive inverse: Let \\(\\mathbf{v}\\in W\\). By subspace property 3, \\((-1)\\mathbf{v}\\in W\\). This is the additive inverse of \\(\\mathbf{v}\\) since \\(\\mathbf{v}+(-1)\\mathbf{v}=1\\mathbf{v}+(-1)\\mathbf{v}=(1-1)\\mathbf{v}=0\\mathbf{v}=\\mathbf{0}\\).\n\nScalar Multiplication Properties:\n\nDistributivity over vector addition: Let \\(a\\in\\mathbb{R}\\) and \\(\\mathbf{u},\\mathbf{v}\\in W\\). We know that \\(a(\\mathbf{u}+\\mathbf{v})=a\\mathbf{u}+a\\mathbf{v}\\), as this holds in \\(V\\).\nDistributivity over scalar addition: Let \\(a,b\\in\\mathbb{R}\\) and \\(\\mathbf{v}\\in W\\). We know that \\((a+b)\\mathbf{v}=a\\mathbf{v}+b\\mathbf{v}\\), as this holds in \\(V\\).\nAssociativity of scalar multiplication: Let \\(a,b\\in\\mathbb{R}\\) and \\(\\mathbf{v}\\in W\\). We know that \\(a(b\\mathbf{v})=(ab)\\mathbf{v}\\), as this holds in \\(V\\).\nIdentity scalar multiplication: Let \\(\\mathbf{v}\\in W\\). We know that \\(1\\mathbf{v}=\\mathbf{v}\\), as this holds in \\(V\\), and clearly \\(\\mathbf{v}\\in W\\) by assumption.\n\nTherefore, since all eight vector space properties are satisfied, \\(W\\) is indeed a vector space.\n\nNote that this proof relies heavily on two key facts:\n\nThe vector space operations in \\(W\\) are inherited from \\(V\\)\nThe subspace properties ensure that these operations are well-defined on \\(W\\) (i.e., their outputs remain in \\(W\\))\n\n\n\n\n\n\n\nSubspaces of \\(\\mathbb{R}^n\\)\n\n\n\nSubspaces of \\(\\mathbb{R}^n\\) are fundamental structures that arise naturally in many applications of linear algebra. They have intuitive geometric interpretations that help us visualize abstract concepts:\n\nIn \\(\\mathbb{R}^2\\):\n\nA line through the origin\nThe entire plane \\(\\mathbb{R}^2\\) itself\nThe zero vector \\({\\vec{0}}\\)\n\nIn \\(\\mathbb{R}^3\\):\n\nA line through the origin\nA plane through the origin\nThe entire space \\(\\mathbb{R}^3\\)\nThe zero vector \\({\\mathbf{0}}\\)\n\n\nIn Theorem 1.1, we proved that the span of a set of vectors \\({\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_k}\\) in \\(\\mathbb{R}^n\\) is a subspace.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abstract Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/vector_spaces.html#inner-products",
    "href": "chapters/vector_spaces.html#inner-products",
    "title": "2  Abstract Vector Spaces",
    "section": "2.3 Inner Products:",
    "text": "2.3 Inner Products:\nAn inner product on a vector space \\(V\\) is a function \\(\\langle\\cdot,\\cdot\\rangle:V\\times V\\to\\mathbb{R}\\) satisfying:\n\nSymmetry: \\(\\langle\\mathbf{u},\\mathbf{v}\\rangle=\\langle\\mathbf{v},\\mathbf{u}\\rangle\\)\nLinearity: \\(\\langle a\\mathbf{u}+b\\mathbf{v},\\mathbf{w}\\rangle=a\\langle\\mathbf{u},\\mathbf{w}\\rangle+b\\langle\\mathbf{v},\\mathbf{w}\\rangle\\)\nPositive definiteness: \\(\\langle\\mathbf{v},\\mathbf{v}\\rangle\\geq 0\\) with equality if and only if \\(\\mathbf{v}=\\mathbf{0}\\)\n\nThe inner product generalizes the familiar dot product of \\(\\mathbb{R}^n\\). Different fields use different notations:\n\nIn \\(\\mathbb{R}^n\\): \\(\\mathbf{u}\\cdot\\mathbf{v}\\) (dot product notation)\nIn mathematics: \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle\\) (angle bracket notation)\nIn physics: \\(\\langle \\mathbf{u} | \\mathbf{v} \\rangle\\) (Dirac or bra-ket notation)\n\nExamples of Inner Products:\n\nStandard dot product in \\(\\mathbb{R}^n\\): \\(\\langle\\mathbf{x},\\mathbf{y}\\rangle=\\sum_{i=1}^n x_iy_i\\)\nOn \\(C[a,b]\\): \\(\\langle f,g\\rangle=\\int_a^b f(x)g(x)\\,dx\\)\nOn \\(\\mathbb{P}_n\\): \\(\\langle p,q\\rangle=\\int_{-1}^1 p(x)q(x)\\,dx\\)\n\nJust as in \\(\\mathbb{R}^n\\), these inner products satisfy fundamental properties that make them powerful tools. Every inner product generates a norm through \\(\\|\\mathbf{v}\\|=\\sqrt{\\langle\\mathbf{v},\\mathbf{v}\\rangle}\\), which in turn induces a distance function \\(d(\\mathbf{u},\\mathbf{v})=\\|\\mathbf{u}-\\mathbf{v}\\|\\). The Cauchy-Schwarz inequality holds in any inner product space: \\(|\\langle\\mathbf{u},\\mathbf{v}\\rangle|\\leq\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|\\). And the norm satisfies all the properties we know from \\(\\mathbb{R}^n\\): positivity, homogeneity, and the triangle inequality. Thus, every inner product space inherits the geometric structure that makes \\(\\mathbb{R}^n\\) so useful.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abstract Vector Spaces</span>"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html",
    "href": "worksheets/computing_dot_products.html",
    "title": "Problems",
    "section": "",
    "text": "Vector Operations and Dot Products",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-1",
    "href": "worksheets/computing_dot_products.html#problem-1",
    "title": "Problems",
    "section": "Problem 1",
    "text": "Problem 1\nConsider the following vectors:\n\n\n\n\n\n\n\n\n\n\nFind the coordinates of \\(\\mathbf{u}\\), \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\)\nFind \\(\\mathbf{u} + \\mathbf{v} + \\mathbf{w}\\) (does the answer make geometric sense?)\nFind \\(\\mathbf{u} - \\mathbf{v} - \\mathbf{w}\\) (does the answer make geometric sense?)\nNormalize the vectors \\(\\mathbf{u}\\), \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\). That is, find a vector in the same direction with norm equal to one.\nCheck that \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) satisfy Cauchy-Schwartz inequality.",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-2",
    "href": "worksheets/computing_dot_products.html#problem-2",
    "title": "Problems",
    "section": "Problem 2",
    "text": "Problem 2\nLet \\(\\mathbf{v}\\), \\(\\mathbf{w}\\), \\(\\mathbf{z} \\in \\mathbb{R}^n\\). Suppose that \\(\\mathbf{v} \\cdot \\mathbf{w} = 2\\), \\(\\mathbf{v} \\cdot \\mathbf{z} = -1\\), \\(\\mathbf{z} \\cdot \\mathbf{w} = 1\\), \\(\\|\\mathbf{v}\\| = \\sqrt{3}\\), \\(\\|\\mathbf{w}\\| = 2\\), and \\(\\|\\mathbf{z}\\| = \\sqrt{5}\\). Find the following:\n\n\\((2\\mathbf{v} + 3\\mathbf{w}) \\cdot \\mathbf{z}\\)\n\\(\\mathbf{v} \\cdot (\\mathbf{v} - 2\\mathbf{w} + 3\\mathbf{z})\\)\n\\((3\\mathbf{v} - 4\\mathbf{z}) \\cdot (\\mathbf{w} + 5\\mathbf{z})\\)\n\\(\\|\\mathbf{v} + \\mathbf{w}\\|\\)\n\\(\\|\\mathbf{v} - \\mathbf{w}\\|\\)\n\\(\\|2\\mathbf{v} - 6\\mathbf{z}\\|\\)\nFind the angle between \\(\\mathbf{v}\\) and \\(\\mathbf{z}\\)\nFind \\(c \\in \\mathbb{R}\\) such that \\(\\mathbf{v}\\) is orthogonal to \\(\\mathbf{w} + c\\mathbf{z}\\)\nFind \\(c \\in \\mathbb{R}\\) such that \\(\\mathbf{w} - c\\mathbf{v}\\) is orthogonal to \\(\\mathbf{v}\\)",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-3",
    "href": "worksheets/computing_dot_products.html#problem-3",
    "title": "Problems",
    "section": "Problem 3",
    "text": "Problem 3\nLet \\(\\mathbf{v}\\), \\(\\mathbf{w} \\in \\mathbb{R}^n\\). Suppose that \\(\\|\\mathbf{v}\\| = 2\\), \\(\\|\\mathbf{v} + \\mathbf{w}\\| = \\sqrt{3}\\) and \\(\\|\\mathbf{w}\\| = \\sqrt{2}\\). Find \\(\\mathbf{v} \\cdot \\mathbf{w}\\).",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-4",
    "href": "worksheets/computing_dot_products.html#problem-4",
    "title": "Problems",
    "section": "Problem 4",
    "text": "Problem 4\nLet \\(\\mathbf{v}\\), \\(\\mathbf{w} \\in \\mathbb{R}^n\\). Suppose that \\(\\|\\mathbf{v}\\| = \\sqrt{2}\\), \\(\\|\\mathbf{v} - \\mathbf{w}\\| = \\sqrt{3}\\) and \\(\\mathbf{v} \\cdot \\mathbf{w} = \\frac{1}{2}\\). Find \\(\\|\\mathbf{w}\\|\\).",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-5",
    "href": "worksheets/computing_dot_products.html#problem-5",
    "title": "Problems",
    "section": "Problem 5",
    "text": "Problem 5\nSuppose that \\(\\mathbf{u} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\), \\(\\mathbf{v} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\\) and \\(\\mathbf{w} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix}\\)\n\nFind a constant \\(c \\in \\mathbb{R}\\) such that \\(\\mathbf{v} - c\\mathbf{u}\\) is orthogonal to \\(\\mathbf{u}\\). Name this vector \\(\\mathbf{f_2} = \\mathbf{v} - c\\mathbf{u}\\).\nFind constants \\(c, d \\in \\mathbb{R}\\) such that \\(\\mathbf{w} - c\\mathbf{u} - d\\mathbf{f_2}\\) is orthogonal to \\(\\mathbf{u}\\) and orthogonal to \\(\\mathbf{f_2}\\). Name this vector \\(\\mathbf{f_3} = \\mathbf{w} - c\\mathbf{u} - d\\mathbf{f_2}\\).\nFind constants \\(c, d, e \\in \\mathbb{R}\\) such that the vectors \\(c\\mathbf{u}\\), \\(d\\mathbf{f_2}\\), and \\(e\\mathbf{f_3}\\) have norm one. Rename these vectors \\(\\mathbf{e_1} = c\\mathbf{u_1}\\), \\(\\mathbf{e_2} = d\\mathbf{f_2}\\), and \\(\\mathbf{e_3} = e\\mathbf{f_3}\\) and write them explicitly.",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-6",
    "href": "worksheets/computing_dot_products.html#problem-6",
    "title": "Problems",
    "section": "Problem 6",
    "text": "Problem 6\nLet \\(\\mathbf{v_1}\\), \\(\\mathbf{v_2}\\), \\(\\mathbf{v_3}\\), \\(\\mathbf{v_4}\\) be vectors satisfying:\n\\(\\mathbf{v_i} \\cdot \\mathbf{v_j} = \\begin{cases} 1 & \\text{if } i = j \\\\ 0 & \\text{otherwise} \\end{cases}\\)\n\nFind \\((2\\mathbf{v_1} - 3\\mathbf{v_2}) \\cdot (2\\mathbf{v_3} + 4\\mathbf{v_4})\\)\nFind \\((\\mathbf{v_1} + \\mathbf{v_2}) \\cdot (\\mathbf{v_1} - \\mathbf{v_2})\\)\nFind \\(\\|\\mathbf{v_4}\\|\\)\nFind \\(\\|4\\mathbf{v_1} - 3\\mathbf{v_2}\\|\\)\nFind \\(\\|2\\mathbf{v_1} - 3\\mathbf{v_2} + 4\\mathbf{v_3} - 5\\mathbf{v_4}\\|\\)",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "parts/matrices.html",
    "href": "parts/matrices.html",
    "title": "Matrices",
    "section": "",
    "text": "This section covers matrices.\n\nMatrices\nProblems: Matrix Multiplication\nApplication: Image Transformation with Matrices",
    "crumbs": [
      "Matrices"
    ]
  },
  {
    "objectID": "chapters/matrices.html",
    "href": "chapters/matrices.html",
    "title": "3  Matrices",
    "section": "",
    "text": "3.1 Matrices: Definition and Different Perspectives\nThe following video from the Essence of Linear Algebra, from 3Blue1Brown, is exceptionally good. Watch it carefully\nA matrix is a rectangular array of numbers arranged in rows and columns. Formally, a matrix \\(A\\) with \\(m\\) rows and \\(n\\) columns is written as:\n\\[\nA = \\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m,1} & a_{m,2} & \\cdots & a_{m,n}\n\\end{bmatrix}.\n\\] The matrix \\(A\\) is said to have dimension \\(m\\times n\\). The entry \\(a_{i,j}\\) represents the element at the \\(i\\)-th row and \\(j\\)-th column. This entry is also sometimes denoted as \\(A_{i,j}\\) or \\((A)_{i,j}\\).\nFor example, consider the following matrix:\n\\[M=\\begin{bmatrix}8&6&0&6\\\\-4&-8&2&-7\\\\-8&4&-5&3\\end{bmatrix}\\]\nThis matrix \\(M\\) has 3 rows and 4 columns. We can interpret and view this matrix in several ways:",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrices-definition-and-different-perspectives",
    "href": "chapters/matrices.html#matrices-definition-and-different-perspectives",
    "title": "3  Matrices",
    "section": "",
    "text": "As an array of numbers: We can see \\(M\\) as a collection of numbers arranged in a rectangular grid with 3 rows and 4 columns. Each entry in the matrix is identified by its row and column index. For example, \\(M_{2,3}\\) or \\((M)_{2,3}\\), the entry in the second row and third column is 2.\nAs a collection of column vectors: We can view \\(M\\) as having 4 column vectors, each with 3 elements. The columns of \\(M\\) are:\n\\[\\left [ \\begin{bmatrix}8\\\\-4\\\\-8\\end{bmatrix}, \\begin{bmatrix}6\\\\-8\\\\4\\end{bmatrix}, \\begin{bmatrix}0\\\\2\\\\-5\\end{bmatrix}, \\begin{bmatrix}6\\\\-7\\\\3\\end{bmatrix}\\right ]\\]\nEach column vector can be treated as a separate entity, and matrix operations can be performed on these columns.\nAs a collection of row vectors: We can view \\(M\\) as having 3 row vectors, each with 4 elements. The rows of \\(M\\) are:\n\\[\\biggl [ \\begin{bmatrix}8&6&0&6\\end{bmatrix}, \\begin{bmatrix}-4&-8&2&-7\\end{bmatrix}, \\begin{bmatrix}-8&4&-5&3\\end{bmatrix}\\biggr ]\\]\nEach row vector can be treated as a separate entity, and matrix operations can be performed on these rows.",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#addition-and-scalar-multiplication-of-matrices",
    "href": "chapters/matrices.html#addition-and-scalar-multiplication-of-matrices",
    "title": "3  Matrices",
    "section": "3.2 Addition and Scalar Multiplication of Matrices",
    "text": "3.2 Addition and Scalar Multiplication of Matrices\nGiven two matrices \\(A\\) and \\(B\\) of the same size \\(m \\times n\\), the sum of \\(A\\) and \\(B\\), denoted as \\(A + B\\), is a new matrix \\(C\\) of size \\(m \\times n\\) where each element \\(c_{i,j}\\) is the sum of the corresponding elements \\(a_{i,j}\\) and \\(b_{i,j}\\) from matrices \\(A\\) and \\(B\\), respectively. In other words: \\[c_{i,j} = a_{i,j} + b_{i,j}\\] for all \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\). For example, if \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\\), then: \\[A + B = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 1+5 & 2+6 \\\\ 3+7 & 4+8 \\end{bmatrix} = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix}\\] The scalar multiplication of a matrix \\(A\\) by a scalar \\(k\\), denoted as \\(kA\\), is a new matrix \\(B\\) of the same size as \\(A\\), where each element \\(b_{i,j}\\) is the product of the scalar \\(k\\) and the corresponding element \\(a_{i,j}\\) from matrix \\(A\\). In other words: \\[b_{i,j} = k \\cdot a_{i,j}\\] for all \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\). For example, if \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(k = 2\\), then: \\[2A = 2 \\cdot \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 2 \\cdot 1 & 2 \\cdot 2 \\\\ 2 \\cdot 3 & 2 \\cdot 4 \\end{bmatrix} = \\begin{bmatrix} 2 & 4 \\\\ 6 & 8 \\end{bmatrix}\\]\n\n3.2.1 Properties of addition and scalar product\nLet \\(A\\), \\(B\\), and \\(C\\) be \\(m\\times n\\) matrices and let \\(c\\) and \\(d\\) be scalars. Then we can esily check that\n\nCommutativity of addition: \\(A + B = B + A\\)\nAssociativity of addition: \\((A + B) + C = A + (B + C)\\)\nExistence of zero matrix: There exists a matrix \\(O\\) such that \\(A + O = A\\) for all matrices \\(A\\)\nExistence of additive inverse: For every matrix \\(A\\), there exists a matrix \\(-A\\) such that \\(A + (-A) = O\\)\nDistributivity of scalar multiplication over matrix addition: \\(k(A + B) = kA + kB\\)\nDistributivity of scalar multiplication over field addition: \\((k + l)A = kA + lA\\)\nAssociativity of scalar multiplication: \\((kl)A = k(lA)\\)\nExistence of multiplicative identity: \\(1A = A\\) for all matrices \\(A\\)\n\nSince we have already shown that if \\(A\\) and \\(B\\) are \\(m\\times n\\) matrices and \\(k\\) is a scalar, then \\(A+B\\) and \\(kA\\) are also \\(m\\times n\\) matrices, we can conclude that the set of all \\(m\\times n\\) matrices forms a vector space. This set is commonly denoted using various notations, including: \\(M_{m\\times n}\\), \\(\\mathbb{M}_{m\\times n}\\), and \\(\\mathbb{R}^{m\\times n}\\).",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#the-transpose-of-a-matrix",
    "href": "chapters/matrices.html#the-transpose-of-a-matrix",
    "title": "3  Matrices",
    "section": "3.3 The Transpose of a Matrix",
    "text": "3.3 The Transpose of a Matrix\nGiven a matrix \\(A\\) with \\(m\\) rows and \\(n\\) columns, denoted as: \\[\nA = \\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m,1} & a_{m,2} & \\cdots & a_{m,n}\n\\end{bmatrix},\n\\] the transpose of matrix \\(A\\), denoted as \\(A^T\\) or \\(A'\\), is obtained by interchanging the rows and columns of \\(A\\). In other words, the first row of \\(A\\) becomes the first column of \\(A^T\\), the second row of \\(A\\) becomes the second column of \\(A^T\\), and so on. The resulting matrix \\(A^T\\) has \\(n\\) rows and \\(m\\) columns:\n\\[\nA^T = \\begin{bmatrix}\na_{1,1} & a_{2,1} & \\cdots & a_{m,1}\\\\\na_{1,2} & a_{2,2} & \\cdots & a_{m,2}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\na_{1,n} & a_{2,n} & \\cdots & a_{m,n}.\n\\end{bmatrix}\n\\]\nWhen \\(A\\) is represented by columns or by rows, we can easily determine the form of the transpose. \\[\n\\text{If}\\quad A = \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix},\\text{ then }\\quad\nA^T=\\begin{bmatrix}\n\\leftarrow & \\mathbf{c}_1^T &\\rightarrow \\\\\n\\leftarrow & \\mathbf{c}_2^T &\\rightarrow\\\\\n\\vdots & \\vdots &\\vdots\\\\\n\\leftarrow & \\mathbf{c}_n^T &\\rightarrow\n\\end{bmatrix},\n\\] and \\[\n\\text{if}\\quad A =\n\\begin{bmatrix}\n\\leftarrow & \\mathbf{r}_1 &\\rightarrow \\\\\n\\leftarrow & \\mathbf{r}_2 &\\rightarrow\\\\\n\\vdots & \\vdots &\\vdots\\\\\n\\leftarrow & \\mathbf{r}_m &\\rightarrow\n\\end{bmatrix}, \\text{ then }\\quad\nA^T = \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{r}_1^T & \\mathbf{r}_2^T & \\cdots & \\mathbf{r}_n^T \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}.\n\\]\nNotice that if we take the transpose twice returns the original matrix. In other words, \\[(A^T)^T=A.\\] To check properties of the transpose we use the definition \\((A^T)_{i,j}=A_{j,i}\\).\n\nExercise: Suppose that \\(A\\) and \\(B\\) are \\(m\\times n\\) matrices and that \\(c\\in\\mathbb{R}\\). Prove that \\((A+B)^T=A^T+B^T\\) and that \\((cA)^T=cA^T\\).\n\n\n\n\n\n\nClick to see a sketch of the proof\n\n\n\n\n\n\\[((A+B)^T)_{i,j}=(A+B)_{j,i}=A_{j,i}+B_{j,i}=(A^T)_{i,j}+(B^T)_{i,j}=(A^T+B^T)_{i,j}.\\] The other one is similar",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrix-vector-multiplication",
    "href": "chapters/matrices.html#matrix-vector-multiplication",
    "title": "3  Matrices",
    "section": "3.4 Matrix-Vector Multiplication",
    "text": "3.4 Matrix-Vector Multiplication\nWe now cover one of the most important operations in linear algebra: multiplying a matrix with a vector. Let \\(A\\) be a an \\(m\\times n\\) matrix and \\(\\mathbf{x}\\in\\mathbb{R}^n\\). The product \\(A\\mathbf{x}\\) will be a vector in \\(\\mathbb{R}^m\\). This operation is crucial because it allows us to:\n\nTransform vectors in space (like rotations, reflections, or scaling)\nSolve systems of linear equations in a compact way\nApply linear transformations in computer graphics, data science, and physics\n\nWe look at the \\(m\\times n\\) matrix \\(A\\) in three ways: \\[A = \\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m,1} & a_{m,2} & \\cdots & a_{m,n}\n\\end{bmatrix}\n=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\n=\\begin{bmatrix}\n\\leftarrow & \\mathbf{r}_1 &\\rightarrow \\\\\n\\leftarrow & \\mathbf{r}_2 &\\rightarrow \\\\\n\\vdots & \\vdots &\\vdots\\\\\n\\leftarrow & \\mathbf{r}_m &\\rightarrow\n\\end{bmatrix}.\\]\n\n3.4.1 A Linear Combination of Columns\nWe define the product \\(A\\mathbf{x}\\) as a linear combination of the columns of \\(A\\): \\[\nA\\mathbf{x} =\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\n\\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_n \\end{bmatrix}\n= x_1\\mathbf{c}_1+x_2\\mathbf{c_2}+\\cdots+x_n\\mathbf{c}_n.\n\\tag{3.1}\\]\nExample: Let \\(A=\\begin{bmatrix} 1 & 2 & 0 \\\\ -1 & 3 & 4 \\end{bmatrix}\\) be a \\(2\\times 3\\) matrix and \\(\\mathbf{x}=\\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}\\) be a vector in \\(\\mathbb{R}^3\\). Then \\[\n\\begin{aligned}\nA\\mathbf{x} &=\n\\begin{bmatrix} 1 & 2 & 0 \\\\ -1 & 3 & 4 \\end{bmatrix}\n\\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix} \\\\\n&=2\\begin{bmatrix}1\\\\-1 \\end{bmatrix}\n+ (-1)\\begin{bmatrix}2\\\\3 \\end{bmatrix}\n+3\\begin{bmatrix}0\\\\4 \\end{bmatrix}\n=\\begin{bmatrix}0\\\\6 \\end{bmatrix}\n\\end{aligned}\n\\]\nEquation (Equation 3.1) is one of the most useful formulas.\n\nIt allows us to write matrix multiplications as linear combinations,\nIt allows us write linear combinations as matrix multiplication.\n\n\n\n3.4.2 The Component-wise Formula\nFrom the definition given by (Equation 3.1), we can write \\(A\\mathbf{x}\\) in terms of the \\(a_{i,j}\\)’s: \\[\\begin{aligned}\nA\\mathbf{x} &=\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m,1} & a_{m,2} & \\cdots & a_{m,n}\n\\end{bmatrix}\n\\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix} \\\\\n&= x_1\\begin{bmatrix} a_{1,1}\\\\ a_{2,1}\\\\ \\vdots\\\\ a_{m,1} \\end{bmatrix}\n+ x_2\\begin{bmatrix} a_{1,2}\\\\ a_{2,2}\\\\ \\vdots\\\\ a_{m,2} \\end{bmatrix} +\\cdots\n+ x_n\\begin{bmatrix} a_{1,n}\\\\ a_{2,n}\\\\ \\vdots\\\\ a_{m,n} \\end{bmatrix} \\\\\n% &= \\begin{bmatrix} a_{1,1}x_1\\\\ a_{2,1}x_1\\\\ \\vdots\\\\ a_{m,1}x_1 \\end{bmatrix}\n% + \\begin{bmatrix} a_{1,2}x_2\\\\ a_{2,2}x_2\\\\ \\vdots\\\\ a_{m,2}x_2 \\end{bmatrix} +\\cdots\n% + \\begin{bmatrix} a_{1,n}x_n\\\\ a_{2,n}x_n\\\\ \\vdots\\\\ a_{m,n}x_n \\end{bmatrix} \\\\\nA\\mathbf{x} &= \\begin{bmatrix} a_{1,1}x_1+a_{1,2}x_2+\\cdots+a_{1,n}x_n\\\\ a_{2,1}x_1+a_{2,2}x_2+\\cdots+a_{2,n}x_n\\\\ \\vdots\\\\ a_{m,1}x_1+a_{m,2}x_2+\\cdots+a_{m,n}x_n\\\\ \\end{bmatrix}.\n\\end{aligned}\n\\]\nTherefore, the \\(i\\)-th component of \\(A\\mathbf{x}\\) is: \\[(A\\mathbf{x})_i=a_{i,1}x_1+a_{i,2}x_2+\\cdots+a_{i,n}x_n \\tag{3.2}\\]\n\n\n3.4.3 The Row Dot Product Formula\nFrom (Equation 3.2) we see that the \\(i\\)-th term of \\(A\\mathbf{x}\\) can be written as a dot product: \\[\n(A\\mathbf{x})_i=a_{i,1}x_1+a_{i,2}x_2+\\cdots+a_{i,n}x_n\n=\\begin{bmatrix} a_{i,1}\\\\ a_{i,2}\\\\\\vdots \\\\ a_{i,n}\\end{bmatrix}\\cdot\n\\begin{bmatrix} x_1\\\\ x_2\\\\\\vdots \\\\ x_n\\end{bmatrix}\n=\\mathbf{r}_i^T\\cdot\\mathbf{x}.\\] Recall that vectors in \\(\\mathbb{R}^n\\) are represented by column vectors, and that the first vector is the transpose of the \\(i\\)-th row of \\(A\\). Putting all the compunents togther, we get: \\[\nA\\mathbf{x}=\\begin{bmatrix}\n\\leftarrow & \\mathbf{r}_1 &\\rightarrow \\\\\n\\leftarrow & \\mathbf{r}_2 &\\rightarrow \\\\\n\\vdots & \\vdots &\\vdots\\\\\n\\leftarrow & \\mathbf{r}_m &\\rightarrow \\end{bmatrix}\n\\begin{bmatrix} x_1\\\\ x_2\\\\\\vdots \\\\ x_n\\end{bmatrix}\n=\\begin{bmatrix}\n\\mathbf{r}_1^T\\cdot\\mathbf{x} \\\\\n\\mathbf{r}_2^T\\cdot\\mathbf{x}\\\\ \\vdots \\\\\n\\mathbf{r}_n^T\\cdot\\mathbf{x}\\end{bmatrix}.\n\\]",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrix-matrix-multiplication",
    "href": "chapters/matrices.html#matrix-matrix-multiplication",
    "title": "3  Matrices",
    "section": "3.5 Matrix-Matrix Multiplication",
    "text": "3.5 Matrix-Matrix Multiplication\nMatrix-matrix multiplication, like matrix-vector multiplication, requires compatibility between the dimensions of the matrices involved. For the product \\(AB\\) to be defined, the number of columns in \\(A\\) must equal the number of rows in \\(B\\). When this condition is met, the matrices are said to be compatible for multiplication. Specifically, if \\(A\\) is an \\(m \\times n\\) matrix and \\(B\\) is an \\(n \\times p\\) matrix, their product \\(AB\\) will be an \\(m \\times p\\) matrix.\n\n3.5.1 The Product \\(AB\\): \\(A\\) Acts on the Columns of \\(B\\)\nLet \\(A\\) be an \\(m \\times n\\) matrix and \\(B\\) an \\(n \\times p\\) matrix. Since the number of columns in \\(A\\) matches the number of rows in \\(B\\), the matrices are compatible for multiplication. To define the product \\(AB\\), we express \\(B\\) in terms of its column vectors and let \\(A\\) act on each column individually. Specifically,\n\\[\nAB = A \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{b}_1 & \\mathbf{b}_2 & \\cdots & \\mathbf{b}_p \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\nA\\mathbf{b}_1 & A\\mathbf{b}_2 & \\cdots & A\\mathbf{b}_p \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}.\n\\tag{3.3}\\]\nNotice that \\(AB\\) consists of \\(p\\) columns, where each column \\(A\\mathbf{c}_i \\in \\mathbb{R}^m\\). Therefore, \\(AB\\) is an \\(m \\times p\\) matrix.\n\n\n3.5.2 The Component Formula\nWe use the previous formula to compute the individual entries of \\(AB\\). Consider \\((AB)_{i,j}\\). This is the element of \\(AB\\) in the \\(i\\)-th row and \\(j\\)-th column. Since the \\(j\\)-th column of \\(AB\\) is \\(A\\mathbf{b}_j\\), it follows from (Equation 3.2) that \\((AB)_{i,j}=(A\\mathbf{b}_j)_i= \\sum_{k=1}^na_{i,k}b_{k,j}.\\) Then we have \\[(AB)_{i,j}=\\sum_{k=1}^na_{i,k}b_{k,j}. \\tag{3.4}\\]\n\n\n3.5.3 The Row-Column Dot Product Formula\nFrom (Equation 3.4), it follows that \\((AB)_{i,j}\\) is the dot product of the \\(i\\)-th row of \\(A\\) and the \\(j\\)-th column of \\(B\\). Writing \\(A\\) in terms of its rows and \\(B\\) in terms of its columns, we have:\n\\[\nAB =\n\\begin{bmatrix}\n\\leftarrow & \\mathbf{r}_1 & \\rightarrow \\\\\n\\leftarrow & \\mathbf{r}_2 & \\rightarrow \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\leftarrow & \\mathbf{r}_m & \\rightarrow\n\\end{bmatrix}\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{b}_1 & \\mathbf{b}_2 & \\cdots & \\mathbf{b}_p \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\mathbf{r}_1^T \\cdot \\mathbf{b}_1 & \\mathbf{r}_1^T \\cdot \\mathbf{b}_2 & \\cdots & \\mathbf{r}_1^T \\cdot \\mathbf{b}_p \\\\  \n\\mathbf{r}_2^T \\cdot \\mathbf{b}_1 & \\mathbf{r}_2^T \\cdot \\mathbf{b}_2 & \\cdots & \\mathbf{r}_2^T \\cdot \\mathbf{b}_p \\\\  \n\\vdots & \\vdots & \\ddots & \\vdots \\\\  \n\\mathbf{r}_m^T \\cdot \\mathbf{b}_1 & \\mathbf{r}_m^T \\cdot \\mathbf{b}_2 & \\cdots & \\mathbf{r}_m^T \\cdot \\mathbf{b}_p  \n\\end{bmatrix}.\n\\tag{3.5}\\]\nHere, \\(\\mathbf{r}_i\\) represents the \\(i\\)-th row of \\(A\\), and \\(\\mathbf{b}_j\\) represents the \\(j\\)-th column of \\(B\\). Each entry of \\(AB\\), denoted \\((AB)_{i,j}\\), is the dot product \\(\\mathbf{r}_i^T \\cdot \\mathbf{b}_j\\).",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#the-transpose-of-the-product",
    "href": "chapters/matrices.html#the-transpose-of-the-product",
    "title": "3  Matrices",
    "section": "3.6 The Transpose of the Product",
    "text": "3.6 The Transpose of the Product\nAn important property of matrix multiplication is that the transpose of a product is the product of the transposes in reverse order. This relation is fundamental in many areas of linear algebra, from proving theoretical results about linear transformations to solving practical problems in optimization and data analysis.\n\n3.6.0.1 Theorem\nLet \\(A\\) be an \\(m \\times n\\) matrix and \\(B\\) an \\(n \\times p\\) matrix. Then:\n\\[(AB)^T = B^T A^T.\\]\n\n\nProof. The product \\(AB\\) is an \\(m \\times p\\) matrix, so its transpose \\((AB)^T\\) is a \\(p \\times m\\) matrix. Similarly, \\(B^T\\) is a \\(p \\times n\\) matrix, and \\(A^T\\) is an \\(n \\times m\\) matrix. Thus, the product \\(B^T A^T\\) also has dimensions \\(p \\times m\\), matching those of \\((AB)^T\\).\nTo prove the equality, we verify that the entries of \\((AB)^T\\) and \\(B^T A^T\\) are identical. Consider the \\((i, j)\\)-th entry of \\((AB)^T\\):\n\\[( (AB)^T )_{i,j} = (AB)_{j,i}.\\]\nUsing the definition of matrix multiplication, we expand \\((AB)_{j,i}\\):\n\\[(AB)_{j,i} = \\sum_{k=1}^n A_{j,k} B_{k,i}.\\]\nNext, observe that \\((B^T)_{i,k} = B_{k,i}\\) and \\((A^T)_{k,j} = A_{j,k}\\). Substituting these into the sum, we get:\n\\[(AB)_{j,i} = \\sum_{k=1}^n (B^T)_{i,k} (A^T)_{k,j}.\\]\nTherefore,\n\\[((AB)^T)_{i,j} = (B^T A^T)_{i,j}.\\]\nSince the \\((i, j)\\)-th entries of \\((AB)^T\\) and \\(B^T A^T\\) are equal for all \\(i\\) and \\(j\\), we conclude that:\n\\[(AB)^T = B^T A^T.\\]\n\n\n\n\n\n\n\nClick to see a proof that uses the row column dot product formula\n\n\n\n\n\nWrite \\(A\\) and \\(B\\) in terms of their rows and columns \\[A=\\begin{bmatrix}\n\\leftarrow & \\mathbf{r}_1 & \\rightarrow \\\\\n\\leftarrow & \\mathbf{r}_2 & \\rightarrow \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\leftarrow & \\mathbf{r}_m & \\rightarrow\n\\end{bmatrix} \\quad\\quad\nB=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{b}_1 & \\mathbf{b}_2 & \\cdots & \\mathbf{b}_p \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\\] Then \\[B^T = \\begin{bmatrix} \\leftarrow &\\mathbf{b}_1^T&\\rightarrow \\\\ \\leftarrow& \\mathbf{b}_2^T&\\rightarrow \\\\ \\vdots &\\vdots&\\vdots \\\\\\leftarrow & \\mathbf{b}_p^T &\\rightarrow \\end{bmatrix}\\quad\\quad\nA^T =\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{r}_1^T & \\mathbf{r}_2^T & \\cdots & \\mathbf{r}_m^T \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}.\\] Therefore it follows from the row column dot product formula that \\[(B^TA^T)_{i,j} = (\\mathbf{b}_i^T)^T\\cdot\\mathbf{r}_j^T=\\mathbf{b}_i\\cdot\\mathbf{r}_j^T\n=\\mathbf{r}_j^T\\cdot\\mathbf{b}_i=(AB)_{j,i}=((AB)^T)_{i,j}.\\]",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrix-types-and-the-inverse",
    "href": "chapters/matrices.html#matrix-types-and-the-inverse",
    "title": "3  Matrices",
    "section": "3.7 Matrix Types and the Inverse",
    "text": "3.7 Matrix Types and the Inverse\nMatrices come in various types, each with unique properties that make them fundamental to linear algebra and its applications. Among these, the following play a central role.\n\nSquare Matrices\nA square matrix is a matrix that has an equal number of rows and columns. The collection of all \\(n \\times n\\) square matrices is denoted by \\(M_n\\). This set is closed under several operations: if \\(A, B \\in M_n\\), their product \\(AB\\) also belongs to \\(M_n\\). Similarly, any power of \\(A\\), such as \\(A^k\\) for a positive integer \\(k\\), remains in \\(M_n\\), as does the transpose of \\(A\\).\n\n\nDiagonal Matrices\nA diagonal matrix is a square matrix in which all off-diagonal entries are zero. Formally, a matrix \\(D \\in M_n\\) is diagonal if \\((D)_{i,j} = 0\\) for all \\(i \\neq j\\). The only potentially nonzero entries are located along the main diagonal, from the top-left to the bottom-right. Diagonal matrices are significant because they are easy to work with: addition, multiplication, and finding powers are straightforward operations when the matrices are diagonal.\n\n\nUpper and Lower Triangular Matrices\nAn upper triangular matrix is a square matrix in which all entries below the main diagonal are zero, meaning \\((U)_{i,j} = 0\\) for all \\(i &gt; j\\). Similarly, a lower triangular matrix has all entries above the main diagonal equal to zero, i.e., \\((L)_{i,j} = 0\\) for all \\(i &lt; j\\). These matrices are commonly used in matrix factorizations, and solving systems of linear equations efficiently. Both types are particularly important in numerical methods, as their structure reduces computational complexity in many algorithms.\n\n\nSymmetric Matrices\nA square matrix is symmetric if it is equal to its transpose, meaning \\(A^T=A\\) or, equivalently, \\(A_{i,j}=A_{j,i}\\) for all \\(i,j\\). Symmetric matrices play a crucial role in various fields due to their significant orthogonal properties and frequent appearance in science and engineering applications.\n\n\nIdentity Matrices\nAn identity matrix is a special type of diagonal matrix where all the diagonal entries are 1, and all off-diagonal entries are 0. It is denoted as \\(I_n\\) for an \\(n \\times n\\) matrix. Formally, \\((I_n)_{i,j} = 1\\) if \\(i = j\\) and \\((I_n)_{i,j} = 0\\) if \\(i \\neq j\\).\nThe identity matrix serves as the multiplicative identity in matrix multiplication. Specifically, if \\(A\\) is an \\(n \\times p\\) matrix, then \\(I_n A = A\\). Similarly, if \\(B\\) is an \\(m \\times n\\) matrix, then \\(B I_n = B\\).\n\n\nInverse of a Matrix\nThe inverse of a matrix is a concept that applies to square matrices. A square matrix \\(A\\) is said to be invertible (or nonsingular) if there exists another matrix \\(A^{-1}\\) such that:\n\\[A A^{-1} = A^{-1} A = I_n,\\]\nwhere \\(I_n\\) is the identity matrix. The matrix \\(A^{-1}\\) is called the inverse of \\(A\\).\nNot all square matrices have an inverse. In practical applications, matrix inverses are used to solve systems of linear equations, analyze transformations, and compute solutions in various scientific and engineering contexts. However, for large matrices, explicit inversion is computationally expensive, and alternative methods, such as iterative techniques, are often preferred.",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#elementary-matrices-row-operations",
    "href": "chapters/matrices.html#elementary-matrices-row-operations",
    "title": "3  Matrices",
    "section": "3.8 Elementary Matrices: Row Operations",
    "text": "3.8 Elementary Matrices: Row Operations\nElementary matrices are special square matrices that perform row operations through matrix multiplication. They play an important role in linear algebra, particularly in solving systems of linear equations, characterizing invertible matrices, and understanding and computing determinants. There are three types:\n\nType 1: Switching two rows\nType 2: Multiplying a row by a non-zero constant\nType 3: Adding a multiple of a row to another row\n\nLet’s illustrate each type with 3×3 elementary matrices acting on a generic 3×5 matrix:\nLet A be a 3×5 matrix: \\(A = \\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix}\\)\n\n3.8.1 Example 1: Interchanging Rows 1 and 2\n\\(\\begin{aligned}\nE_1A &=\n\\begin{bmatrix}\n0 & 1 & 0\\\\\n1 & 0 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix} \\\\\n&=\\begin{bmatrix}\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix}\n\\end{aligned}\\)\n\n\n3.8.2 Example 2: Multiplying Row 3 by 2\n\\(\\begin{aligned}\nE_2A &=\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 2\n\\end{bmatrix}\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix} \\\\\n&=\\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\n2a_{3,1} & 2a_{3,2} & 2a_{3,3} & 2a_{3,4} & 2a_{3,5}\n\\end{bmatrix}\n\\end{aligned}\\)\n\n\n3.8.3 Example 3: Adding 3 Times Row 1 to Row 2\n\\(\\begin{aligned}\nE_3A &=\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n3 & 1 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix} \\\\\n&=\\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\n3a_{1,1}+a_{2,1} & 3a_{1,2}+a_{2,2} & 3a_{1,3}+a_{2,3} & 3a_{1,4}+a_{2,4} & 3a_{1,5}+a_{2,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix}\n\\end{aligned}\\)\nNote that each elementary matrix is invertible, and its inverse performs the opposite operation:\n\nFor \\(E_1\\): its own inverse (swapping the same rows again)\nFor \\(E_2\\): multiply the third row by 1/2\nFor \\(E_3\\): subtract 3 times row 1 from row 2\n\nWe saw in (Equation 3.1) that when we multiply a matrix \\(A\\) by a vector \\(\\mathbf{x}\\), the product \\(A\\mathbf{x}\\) is a linear combination of the columns of \\(A\\). Similarly, when we multiply by a row vector \\(\\mathbf{z}\\) from the left, the product \\(\\mathbf{z}A\\) is a linear combination of the rows of \\(A\\). This fundamental principle helps us understand elementary matrices: when we multiply a matrix \\(A\\) by an elementary matrix \\(E\\) on the left, each row of the product \\(EA\\) is a linear combination of the rows of \\(A\\), precisely implementing our desired row operation.",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrices-in-numpy",
    "href": "chapters/matrices.html#matrices-in-numpy",
    "title": "3  Matrices",
    "section": "3.9 Matrices in Numpy",
    "text": "3.9 Matrices in Numpy\nThis section covers fundamental matrix operations using NumPy’s ndarray class. We’ll explore creation, indexing, and basic mathematical operations.\n\n3.9.1 Setup\nFirst, let’s import NumPy:\n\nimport numpy as np\n\n\n\n3.9.2 Creating Matrices\nNumPy provides several ways to create matrices using ndarrays:\n\n# From a list of lists\nA = np.array([[1, 2, 3],\n              [4, 5, 6]])\nprint(\"Matrix A:\")\nprint(A)\n\n# Using special functions\nzeros = np.zeros((2, 3))    # 2x3 matrix of zeros\nones = np.ones((3, 3))      # 3x3 matrix of ones\neye = np.eye(3)             # 3x3 identity matrix\n\nprint(\"\\nZeros matrix:\")\nprint(zeros)\nprint(\"\\nIdentity matrix:\")\nprint(eye)\n\nMatrix A:\n[[1 2 3]\n [4 5 6]]\n\nZeros matrix:\n[[0. 0. 0.]\n [0. 0. 0.]]\n\nIdentity matrix:\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\n\n\n3.9.3 Matrix Properties and Shape\nThe shape attribute tells us the dimensions of the matrix:\n\nA = np.array([[1, 2, 3],\n              [4, 5, 6]])\nprint(f\"Shape: {A.shape}\")\nprint(f\"Number of dimensions: {A.ndim}\")\nprint(f\"Size: {A.size}\")\n\nShape: (2, 3)\nNumber of dimensions: 2\nSize: 6\n\n\n\n\n3.9.4 Indexing and Slicing\nNumPy provides powerful ways to access matrix elements:\n\nA = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\n# Individual elements\nprint(\"First element:\", A[0, 0])\nprint(\"Second row, third column:\", A[1, 2])\n\n# Extracting rows\nprint(\"\\nFirst row:\", A[0, :])\nprint(\"Second row:\", A[1])  # : is implicit\n\n# Extracting columns\nprint(\"\\nFirst column:\", A[:, 0])\nprint(\"Second column:\", A[:, 1])\n\n# Slicing\nprint(\"\\nSubmatrix (first two rows, second and third columns):\")\nprint(A[0:2, 1:3])\n\nFirst element: 1\nSecond row, third column: 6\n\nFirst row: [1 2 3]\nSecond row: [4 5 6]\n\nFirst column: [1 4 7]\nSecond column: [2 5 8]\n\nSubmatrix (first two rows, second and third columns):\n[[2 3]\n [5 6]]\n\n\n\n\n3.9.5 Basic Operations\n\n3.9.5.1 Addition and Subtraction\nMatrix addition and subtraction work element-wise:\n\nA = np.array([[1, 2],\n              [3, 4]])\nB = np.array([[5, 6],\n              [7, 8]])\n\nprint(\"Matrix A:\")\nprint(A)\nprint(\"\\nMatrix B:\")\nprint(B)\nprint(\"\\nA + B:\")\nprint(A + B)\nprint(\"\\nA - B:\")\nprint(A - B)\n\nMatrix A:\n[[1 2]\n [3 4]]\n\nMatrix B:\n[[5 6]\n [7 8]]\n\nA + B:\n[[ 6  8]\n [10 12]]\n\nA - B:\n[[-4 -4]\n [-4 -4]]\n\n\n\n\n3.9.5.2 Scalar Operations\nMultiply or divide a matrix by a scalar:\n\nA = np.array([[1, 2],\n              [3, 4]])\n\nprint(\"Original matrix:\")\nprint(A)\nprint(\"\\nMultiply by 2:\")\nprint(2 * A)\nprint(\"\\nDivide by 2:\")\nprint(A / 2)\n\nOriginal matrix:\n[[1 2]\n [3 4]]\n\nMultiply by 2:\n[[2 4]\n [6 8]]\n\nDivide by 2:\n[[0.5 1. ]\n [1.5 2. ]]\n\n\n\n\n3.9.5.3 Matrix Multiplication\nNumPy provides several ways to perform matrix multiplication:\n\nA = np.array([[1, 2],\n              [3, 4]])\nB = np.array([[5, 6],\n              [7, 8]])\n\nprint(\"Matrix multiplication (A @ B):\")\nprint(A @ B)          # Preferred method (Python 3.5+)\n\nprint(\"\\nElement-wise multiplication (A * B):\")\nprint(A * B)          # Hadamard product\n\nMatrix multiplication (A @ B):\n[[19 22]\n [43 50]]\n\nElement-wise multiplication (A * B):\n[[ 5 12]\n [21 32]]\n\n\n\n\n\n3.9.6 Common Matrix Operations\nHere are some frequently used matrix operations:\n\nA = np.array([[1, 2],\n              [3, 4]])\n\nprint(\"Original matrix:\")\nprint(A)\n\nprint(\"\\nTranspose:\")\nprint(A.T)\n\nprint(\"\\nMatrix trace:\")\nprint(np.trace(A))\n\nprint(\"\\nMatrix determinant:\")\nprint(np.linalg.det(A))\n\nprint(\"\\nMatrix inverse:\")\nprint(np.linalg.inv(A))\n\nOriginal matrix:\n[[1 2]\n [3 4]]\n\nTranspose:\n[[1 3]\n [2 4]]\n\nMatrix trace:\n5\n\nMatrix determinant:\n-2.0000000000000004\n\nMatrix inverse:\n[[-2.   1. ]\n [ 1.5 -0.5]]\n\n\n\n\n3.9.7 Important Notes\n\nAlways check matrix dimensions when performing operations\nUse the appropriate multiplication operator:\n\n@ or np.matmul() for matrix multiplication\n* for element-wise multiplication\n\nRemember that indexing starts at 0, not 1\nWhen extracting rows or columns:\n\nA single row: A[i] or A[i, :]\nA single column: A[:, j]",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrices-in-sympy",
    "href": "chapters/matrices.html#matrices-in-sympy",
    "title": "3  Matrices",
    "section": "3.10 Matrices in Sympy",
    "text": "3.10 Matrices in Sympy\nThis section covers fundamental matrix operations using SymPy’s Matrix class. We’ll explore creation, indexing, and both numeric and symbolic operations.\n\n3.10.1 Setup\nFirst, let’s import SymPy and set up symbolic variables:\n\nfrom sympy import Matrix, Symbol, init_printing, pprint\nimport sympy as sp\n\n# Setup pretty printing\ninit_printing()\n\n# Define some symbolic variables\nx = Symbol('x')\ny = Symbol('y')\n\n\n\n3.10.2 Creating Matrices\nSymPy provides several ways to create matrices:\n\n# From a list of lists\nA = Matrix([[1, 2, 3],\n            [4, 5, 6]])\nprint(\"Matrix A:\")\npprint(A)\n\n# Using special constructors\nzeros = Matrix.zeros(2, 3)    # 2x3 matrix of zeros\nones = Matrix.ones(3, 3)      # 3x3 matrix of ones\neye = Matrix.eye(3)           # 3x3 identity matrix\n\nprint(\"\\nZeros matrix:\")\npprint(zeros)\nprint(\"\\nIdentity matrix:\")\npprint(eye)\n\n# Symbolic matrix\nsymbolic = Matrix([[x, y],\n                  [y, x]])\nprint(\"\\nSymbolic matrix:\")\npprint(symbolic)\n\nMatrix A:\n⎡1  2  3⎤\n⎢       ⎥\n⎣4  5  6⎦\n\nZeros matrix:\n⎡0  0  0⎤\n⎢       ⎥\n⎣0  0  0⎦\n\nIdentity matrix:\n⎡1  0  0⎤\n⎢       ⎥\n⎢0  1  0⎥\n⎢       ⎥\n⎣0  0  1⎦\n\nSymbolic matrix:\n⎡x  y⎤\n⎢    ⎥\n⎣y  x⎦\n\n\n\n\n3.10.3 Matrix Properties and Shape\nSymPy matrices have several useful properties:\n\nA = Matrix([[1, 2, 3],\n            [4, 5, 6]])\nprint(f\"Shape: {A.shape}\")\nprint(f\"Number of rows: {A.rows}\")\nprint(f\"Number of columns: {A.cols}\")\n\nShape: (2, 3)\nNumber of rows: 2\nNumber of columns: 3\n\n\n\n\n3.10.4 Indexing and Slicing\nSymPy uses different indexing methods than NumPy:\n\nA = Matrix([[1, 2, 3],\n            [4, 5, 6],\n            [7, 8, 9]])\n\n# Individual elements (zero-based indexing)\nprint(\"First element:\", A[0, 0])\nprint(\"Second row, third column:\", A[1, 2])\n\n# Extracting rows\nprint(\"\\nFirst row:\")\npprint(A.row(0))\nprint(\"\\nSecond row:\")\npprint(A.row(1))\n\n# Extracting columns\nprint(\"\\nFirst column:\")\npprint(A.col(0))\nprint(\"\\nSecond column:\")\npprint(A.col(1))\n\n# Extracting submatrices\nprint(\"\\nSubmatrix:\")\npprint(A[0:2, 1:3])\n\nFirst element: 1\nSecond row, third column: 6\n\nFirst row:\n[1  2  3]\n\nSecond row:\n[4  5  6]\n\nFirst column:\n⎡1⎤\n⎢ ⎥\n⎢4⎥\n⎢ ⎥\n⎣7⎦\n\nSecond column:\n⎡2⎤\n⎢ ⎥\n⎢5⎥\n⎢ ⎥\n⎣8⎦\n\nSubmatrix:\n⎡2  3⎤\n⎢    ⎥\n⎣5  6⎦\n\n\n\n\n3.10.5 Basic Operations\n\n3.10.5.1 Addition and Subtraction\nMatrix addition and subtraction work both with numeric and symbolic matrices:\n\nA = Matrix([[1, 2],\n            [3, 4]])\nB = Matrix([[5, 6],\n            [7, 8]])\n\nprint(\"Matrix A:\")\npprint(A)\nprint(\"\\nMatrix B:\")\npprint(B)\nprint(\"\\nA + B:\")\npprint(A + B)\nprint(\"\\nA - B:\")\npprint(A - B)\n\n# Symbolic example\nC = Matrix([[x, y],\n            [y, x]])\nprint(\"\\nSymbolic addition A + C:\")\npprint(A + C)\n\nMatrix A:\n⎡1  2⎤\n⎢    ⎥\n⎣3  4⎦\n\nMatrix B:\n⎡5  6⎤\n⎢    ⎥\n⎣7  8⎦\n\nA + B:\n⎡6   8 ⎤\n⎢      ⎥\n⎣10  12⎦\n\nA - B:\n⎡-4  -4⎤\n⎢      ⎥\n⎣-4  -4⎦\n\nSymbolic addition A + C:\n⎡x + 1  y + 2⎤\n⎢            ⎥\n⎣y + 3  x + 4⎦\n\n\n\n\n3.10.5.2 Scalar Operations\nMultiply or divide a matrix by a scalar (numeric or symbolic):\n\nA = Matrix([[1, 2],\n            [3, 4]])\n\nprint(\"Original matrix:\")\npprint(A)\nprint(\"\\nMultiply by 2:\")\npprint(2 * A)\nprint(\"\\nMultiply by symbolic x:\")\npprint(x * A)\n\nOriginal matrix:\n⎡1  2⎤\n⎢    ⎥\n⎣3  4⎦\n\nMultiply by 2:\n⎡2  4⎤\n⎢    ⎥\n⎣6  8⎦\n\nMultiply by symbolic x:\n⎡ x   2⋅x⎤\n⎢        ⎥\n⎣3⋅x  4⋅x⎦\n\n\n\n\n3.10.5.3 Matrix Multiplication\nSymPy matrix multiplication works with both numeric and symbolic matrices:\n\nA = Matrix([[1, 2],\n            [3, 4]])\nB = Matrix([[5, 6],\n            [7, 8]])\n\nprint(\"Matrix multiplication (A * B):\")\npprint(A * B)\n\nMatrix multiplication (A * B):\n⎡19  22⎤\n⎢      ⎥\n⎣43  50⎦\n\n\n\n\n\n3.10.6 Other Matrix Operations\nSymPy provides powerful symbolic matrix operations:\n\nA = Matrix([[1, 2],\n            [3, 4]])\n\nprint(\"Original matrix:\")\npprint(A)\n\nprint(\"\\nTranspose:\")\npprint(A.transpose())\n\nprint(\"\\nMatrix trace:\")\npprint(A.trace())\n\nprint(\"\\nDeterminant:\")\npprint(A.det())\n\nprint(\"\\nMatrix inverse:\")\npprint(A.inv())\n\n# Symbolic example\nS = Matrix([[x, y],\n            [y, x]])\nprint(\"\\n5th power of S:\")\nS**5\n\nOriginal matrix:\n⎡1  2⎤\n⎢    ⎥\n⎣3  4⎦\n\nTranspose:\n⎡1  3⎤\n⎢    ⎥\n⎣2  4⎦\n\nMatrix trace:\n5\n\nDeterminant:\n-2\n\nMatrix inverse:\n⎡-2    1  ⎤\n⎢         ⎥\n⎣3/2  -1/2⎦\n\n5th power of S:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}x^{5} + 10 x^{3} y^{2} + 5 x y^{4} & 5 x^{4} y + 10 x^{2} y^{3} + y^{5}\\\\5 x^{4} y + 10 x^{2} y^{3} + y^{5} & x^{5} + 10 x^{3} y^{2} + 5 x y^{4}\\end{matrix}\\right]\\)\n\n\n\n\n3.10.7 Important Notes\n\nSymPy matrices use * for matrix multiplication (unlike NumPy’s @)\nIndexing is zero-based, similar to NumPy\nSymPy matrices are immutable - operations return new matrices\nRow and column extraction methods return Matrix objects\nSymPy can handle:\n\nSymbolic computations\nExact fractions\nAlgebraic expressions",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html",
    "href": "worksheets/matrix_multiplication.html",
    "title": "Problems",
    "section": "",
    "text": "Problem 1\nFor this problem let\n\\[A = \\begin{bmatrix}\n3 & 1 & -2 \\\\\n5 & -4 & 3\n\\end{bmatrix},\nB = \\begin{bmatrix}\n8 & 2\\\\\n-6 & -3\\\\\n2 & -4\n\\end{bmatrix}\\]\n\\[C = \\begin{bmatrix}\n2 & 3 \\\\\n-3 & 2\n\\end{bmatrix},\nD = \\begin{bmatrix}\n4 & 6 \\\\\n-2 & 5\n\\end{bmatrix},\nE = \\begin{bmatrix}\n-6 \\\\\n4\n\\end{bmatrix}\\]\nCompute the following. If the operation is not possible explain why",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-1",
    "href": "worksheets/matrix_multiplication.html#problem-1",
    "title": "Problems",
    "section": "",
    "text": "\\(-3A+B\\)\n\\(B - 3A^T\\)\n\\(I_3-AB\\)\n\\(AB-2I_2\\)\n\\(AC\\)\n\\(DA\\)\n\\(E^TCE\\)\n\\(BD+A^T\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-2",
    "href": "worksheets/matrix_multiplication.html#problem-2",
    "title": "Problems",
    "section": "Problem 2",
    "text": "Problem 2\nLet \\(A = \\begin{bmatrix} \\uparrow & \\uparrow & \\uparrow \\\\ \\mathbf{c}_1 & \\mathbf{c}_2 & \\mathbf{c}_3 \\\\ \\downarrow & \\downarrow & \\downarrow \\end{bmatrix} = \\begin{bmatrix} \\leftarrow \\mathbf{r}_1 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_2 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_3 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_4 \\rightarrow \\end{bmatrix}\\)\nand suppose that \\(A \\begin{bmatrix} 1 \\\\ 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 0 \\\\ 2 \\\\ -9 \\end{bmatrix}\\). For the following write the vectors in terms of the columns or rows of \\(A\\).\n\nWhat are the dimensions of \\(A\\)?\nIs any row of \\(A\\) orthogonal to \\((1,1,-1)\\)? (Actually, the question is if the transpose of any row is orthogonal to \\((1,1,-1)\\))\nWrite a vector \\(\\mathbf{v}\\) in terms of the rows of \\(A\\) that satisfies \\(\\mathbf{v} \\cdot (1,1,-1) = 3\\)\nWrite a vector \\(\\mathbf{v}\\) in terms of the rows of \\(A\\) that satisfies \\(\\mathbf{v} \\cdot (1,1,-1) = 1\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-3",
    "href": "worksheets/matrix_multiplication.html#problem-3",
    "title": "Problems",
    "section": "Problem 3",
    "text": "Problem 3\nLet \\(A = \\begin{bmatrix} \\uparrow & \\uparrow & \\uparrow \\\\ \\mathbf{c}_1 & \\mathbf{c}_2 & \\mathbf{c}_3 \\\\ \\downarrow & \\downarrow & \\downarrow \\end{bmatrix}\\), \\(B = \\begin{bmatrix} \\uparrow & \\uparrow & \\uparrow \\\\ \\mathbf{d}_1 & \\mathbf{d}_2 & \\mathbf{d}_3 \\\\ \\downarrow & \\downarrow & \\downarrow \\end{bmatrix}\\) and suppose that \\(A^tB = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\)\n\nWhat are the dimensions of \\(A\\) and \\(B\\)?\nFind the following dot products: \\(\\mathbf{c}_1 \\cdot \\mathbf{d}_1\\), \\(\\mathbf{c}_3 \\cdot \\mathbf{d}_1\\), \\(\\mathbf{c}_2 \\cdot \\mathbf{d}_3\\)\nFind a constant \\(\\alpha \\in \\mathbb{R}\\) such that \\(\\mathbf{c}_1 - \\alpha \\mathbf{c}_2\\) is orthogonal to \\(\\mathbf{d}_3\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-4",
    "href": "worksheets/matrix_multiplication.html#problem-4",
    "title": "Problems",
    "section": "Problem 4",
    "text": "Problem 4\nLet \\(A = \\begin{bmatrix} \\uparrow & \\uparrow & \\uparrow \\\\ \\mathbf{c}_1 & \\mathbf{c}_2 & \\mathbf{c}_3 \\\\ \\downarrow & \\downarrow & \\downarrow \\end{bmatrix}\\) and assume that \\(A^tA = \\begin{bmatrix} 1 & -1 & 1 \\\\ -1 & 2 & 0 \\\\ 1 & 0 & 3 \\end{bmatrix}\\)\n\nFind the following dot products: \\(\\mathbf{c}_1 \\cdot \\mathbf{c}_2\\), \\(\\mathbf{c}_1 \\cdot \\mathbf{c}_3\\), \\(\\mathbf{c}_2 \\cdot \\mathbf{c}_3\\)\nFind \\(\\|\\mathbf{c}_1\\|\\), \\(\\|\\mathbf{c}_2\\|\\), \\(\\|\\mathbf{c}_3\\|\\)\nFind \\(\\|\\mathbf{c}_1 + 3\\mathbf{c}_2\\|\\) and \\(\\|3\\mathbf{c}_2 - 4\\mathbf{c}_3\\|\\)\n\n\nFor the following problems we have:\n\\(A = \\begin{bmatrix}\\uparrow&\\uparrow&\\uparrow\\\\\n\\mathbf{c}_1&\\mathbf{c}_2 &\\mathbf{c}_3\\\\\n\\downarrow&\\downarrow&\\downarrow\\end{bmatrix} =\n\\begin{bmatrix} \\leftarrow \\mathbf{r}_1 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_2 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_3 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_4 \\rightarrow \\end{bmatrix}\\)\n\\(E_1 = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\),   \\(E_2 = \\begin{bmatrix} 1 & 0 & -2 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\),  \n\\(E_3 = \\begin{bmatrix} -1 & 0 & 0 & 0 \\\\ 1 & -1 & 0 & 0 \\\\ 0 & 1 & -1 & 0 \\\\ 0 & 0 & 1 & -1 \\end{bmatrix}\\),   \\(E_4 = \\begin{bmatrix} 1 & 0 & 1 & -10 \\\\ 2 & 2 & 1 & 0 \\\\ 0 & -2 & -1 & 0 \\\\ 4 & 0 & 0 & -1 \\end{bmatrix}\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-5",
    "href": "worksheets/matrix_multiplication.html#problem-5",
    "title": "Problems",
    "section": "Problem 5",
    "text": "Problem 5\nFor each of the following problems, indicate if the matrices can be multiplied. If they can, express the answer in terms of the rows of \\(A\\) or the columns of \\(A\\). If they cannot be multiplied, explain why.\n\n\\(AE_1\\)\n\\(E_1A\\)\n\\(AE_2\\)\n\\(E_2A\\)\n\\(AE_3\\)\n\\(E_3A\\)\n\\(AE_4\\)\n\\(E_4A\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-6",
    "href": "worksheets/matrix_multiplication.html#problem-6",
    "title": "Problems",
    "section": "Problem 6",
    "text": "Problem 6\nSuppose that \\(AB_1 = \\begin{bmatrix} \\uparrow & \\uparrow & \\uparrow & \\uparrow \\\\ (\\mathbf{c}_1-3\\mathbf{c}_2) & \\mathbf{c}_3 & \\mathbf{c}_2 & (8\\mathbf{c}_2-\\mathbf{c}_3) \\\\ \\downarrow & \\downarrow & \\downarrow & \\downarrow \\end{bmatrix}\\). Find \\(B_1\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-7",
    "href": "worksheets/matrix_multiplication.html#problem-7",
    "title": "Problems",
    "section": "Problem 7",
    "text": "Problem 7\nSuppose that \\(B_2A = \\begin{bmatrix} \\leftarrow \\mathbf{r}_1 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_2 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_3 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_4 \\rightarrow \\end{bmatrix}\\). Find \\(B_2\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-8",
    "href": "worksheets/matrix_multiplication.html#problem-8",
    "title": "Problems",
    "section": "Problem 8",
    "text": "Problem 8\nSuppose that \\(B_3A = \\begin{bmatrix} \\leftarrow (\\mathbf{r}_1+\\mathbf{r}_2-\\mathbf{r}_3) \\rightarrow \\\\ \\leftarrow (\\mathbf{r}_2-\\mathbf{r}_3+\\mathbf{r}_4) \\rightarrow \\end{bmatrix}\\). Find \\(B_3\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-9",
    "href": "worksheets/matrix_multiplication.html#problem-9",
    "title": "Problems",
    "section": "Problem 9",
    "text": "Problem 9\nSuppose that \\(B_4E_1 = A\\). Find \\(B_4\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-10-optional",
    "href": "worksheets/matrix_multiplication.html#problem-10-optional",
    "title": "Problems",
    "section": "Problem 10 (Optional)",
    "text": "Problem 10 (Optional)\nSuppose that \\(B_5\\begin{bmatrix} \\leftarrow (\\mathbf{r}_1+\\mathbf{r}_2+\\mathbf{r}_3+\\mathbf{r}_4) \\rightarrow \\\\ \\leftarrow (\\mathbf{r}_2+\\mathbf{r}_3+\\mathbf{r}_4) \\rightarrow \\\\ \\leftarrow (\\mathbf{r}_3+\\mathbf{r}_4) \\rightarrow \\\\ \\leftarrow \\mathbf{r}_4 \\rightarrow \\end{bmatrix} = A\\). Find \\(B_5\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html",
    "href": "applications/image_transforms_matrix.html",
    "title": "Real World Application",
    "section": "",
    "text": "Image Transformation with Matrices\nThis section explores how images are represented as matrices and demonstrates various transformations using Python. We’ll cover both grayscale and color images, showing how matrix operations can be used to create different visual effects.",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#image-transformation-with-matrices",
    "href": "applications/image_transforms_matrix.html#image-transformation-with-matrices",
    "title": "Real World Application",
    "section": "",
    "text": "Required Libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#black-and-white-images",
    "href": "applications/image_transforms_matrix.html#black-and-white-images",
    "title": "Real World Application",
    "section": "1. Black and White Images",
    "text": "1. Black and White Images\n\nUnderstanding Grayscale Representation\nGrayscale images are typically represented as 2D arrays (matrices) where each element represents the intensity of a pixel. When working with uint8 (integers) data type, the values range from 0 (black) to 255 (white). When working with float data type, the values should be normalized to the range 0 to 1.\n\n# Create a simple 3x4 grayscale image\ngrayscale_example = np.array([\n    [0, 85, 170, 255],    # Different shades of gray\n    [255, 170, 85, 0],    # Reversed pattern\n    [128, 128, 128, 128]  # Medium gray\n])\n\nplt.figure(figsize=(6, 4))\nplt.imshow(grayscale_example, cmap='gray')\nplt.colorbar()\nplt.title('3x4 Grayscale Example')\nplt.show()",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#color-images-rgb",
    "href": "applications/image_transforms_matrix.html#color-images-rgb",
    "title": "Real World Application",
    "section": "2. Color Images (RGB)",
    "text": "2. Color Images (RGB)\nColors can be represented in various digital formats - from HSV (Hue, Saturation, Value) to CMYK (Cyan, Magenta, Yellow, Key/Black). Here,we’ll work with the RGB (Red, Green, Blue) color model, where each pixel’s color is created by combining different intensities of these three primary colors.\n\nRGB Color Model\nColor images use three channels: Red, Green, and Blue. Each pixel is represented by three values, creating a 3D array with shape (height, width, 3). As with grayscale images, the values can be either in the range 0-255 (uint8) or 0-1 (float).\n\nCommon Colors:\n\nBlack: (0, 0, 0)\nWhite: (255, 255, 255)\nPure Red: (255, 0, 0)\nPure Green: (0, 255, 0)\nPure Blue: (0, 0, 255)\nYellow: (255, 255, 0) [Red + Green]\nMagenta: (255, 0, 255) [Red + Blue]\nCyan: (0, 255, 255) [Green + Blue]\n\n\n\n\nSimple RGB image with common colors\n\nsimple_rgb = np.array([\n    [[255,0,0], [0,255,0], [0,0,255], [255,255,255]],   # top row\n    [[255,255,0], [0,255,255],[0,0,0], [100,100,100]]   # bottom row\n])\n\nplt.figure(figsize=(4, 4))\nplt.imshow(simple_rgb)\nplt.title('2x4 RGB Image')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNavigating Color Images in NumPy\nWhen working with color images in NumPy, image[a,b,c] lets us access specific pixel values. The first two numbers (a,b) are the pixel coordinates - a selects the row (moving down), b selects the column (moving right). The last number c picks the color channel: 0 for red, 1 for green, or 2 for blue. So image[1,2,1] gets the green value at row 2, column 3.\nTo access entire color channels, you can use : to select all rows and columns. For example, image[:,:,0] gives you the complete red channel, image[:,:,1] the green channel, and image[:,:,2] the blue channel. Each channel is a 2D array of intensities from 0 to 255, which we visualize in grayscale in the folling function - bright pixels show where that color is strong, dark pixels where it’s absent.\n\ndef display_rgb_channels(image):\n    \"\"\"Display an image and its RGB channels separately\"\"\"\n    \n    # Create a figure with 2x2 subplots\n    fig, axes = plt.subplots(2, 2, figsize=(8,8))\n    \n    # Original image\n    axes[0,0].imshow(image)\n    axes[0,0].set_title('Original')\n    \n    # Red channel showed as gray\n    axes[0,1].imshow(image[:,:,0], cmap=\"gray\")\n    axes[0,1].set_title('Red Channel')\n    \n    # Green channel showed as gray\n    axes[1,0].imshow(image[:,:,1], cmap=\"gray\")\n    axes[1,0].set_title('Green Channel')\n    \n    # Blue channel showed as gray\n    axes[1,1].imshow(image[:,:,2], cmap=\"gray\")\n    axes[1,1].set_title('Blue Channel')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Load an image\ncircles = plt.imread('rgb_colors.png')\ndisplay_rgb_channels(circles)\nbutterfly = plt.imread('butterfly.jpg')\ndisplay_rgb_channels(butterfly)",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#converting-between-images-and-matrices",
    "href": "applications/image_transforms_matrix.html#converting-between-images-and-matrices",
    "title": "Real World Application",
    "section": "3. Converting Between Images and Matrices",
    "text": "3. Converting Between Images and Matrices\n\nUnderstanding Image Arrays and Reshaping\nA grayscale image is stored as a 2D array with shape (height, width), while a color image uses a 3D array with shape (height, width, 3). For example, a 100x100 color image has shape (100, 100, 3), where the third dimension holds RGB values.\nMatrix operations require 2D arrays, so we need to reorganize our 3D color images. We transform from height × width × 3 to a matrix of (height × width) rows by 3 columns, flattening the spatial dimensions while keeping color information.\nThe reshape(-1,3) method transforms our 3D color image into a 2D matrix. The -1 tells NumPy to automatically calculate the number of rows needed, while 3 specifies we want 3 columns. For example, an image of shape (100,100,3) becomes a matrix of shape (10000,3), where each row represents one pixel’s values. The columns have a specific meaning: the first column contains all red values, the second green, and the third blue.\n\n\nExample\nLet’s illustrate this is a simple example:\n\nimg_2by2 = np.array([\n    [[1,2,3],[4,5,6]],      # top row\n    [[7,8,9],[10,11,12]]    # bottom row\n])\nprint(\"The original image:\")\nprint(img_2by2)\nprint(\"Shape:\", img_2by2.shape)\n\nprint(\"\\n\" + \"-\"*40 + \"\\n\")  # adds a dividing line\n\nM_2by2 = img_2by2.reshape(-1,3)\nprint(\"The reshaped matrix:\")\nprint(M_2by2)\nprint(\"Shape:\", M_2by2.shape)\n\nThe original image:\n[[[ 1  2  3]\n  [ 4  5  6]]\n\n [[ 7  8  9]\n  [10 11 12]]]\nShape: (2, 2, 3)\n\n----------------------------------------\n\nThe reshaped matrix:\n[[ 1  2  3]\n [ 4  5  6]\n [ 7  8  9]\n [10 11 12]]\nShape: (4, 3)\n\n\nNotice that the columns represent red, green, and blue values respectively. This process is reversible. If we write M_2by2.reshape(img_2by2.shape) we get img_2by2 back - no need to remember the original dimensions since they’re stored in the shape attribute. However, if you want, you can also write M_2by2.reshape(2,2).\n\nM_2by2.reshape(img_2by2.shape)\n\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]])\n\n\n\n\nHelper Functions\nWe’ll use these helper functions to convert between image and matrix formats throughout our examples:\n\ndef image_to_matrix(image):\n    \"\"\"Convert image to matrix format (n_pixels × 3)\"\"\"\n    return image.reshape(-1, 3)\n\ndef matrix_to_image(matrix, original_shape):\n    \"\"\"Convert matrix back to image format\"\"\"\n    return matrix.reshape(original_shape)",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#color-transformations-using-permutation-matrices",
    "href": "applications/image_transforms_matrix.html#color-transformations-using-permutation-matrices",
    "title": "Real World Application",
    "section": "4. Color Transformations Using Permutation Matrices",
    "text": "4. Color Transformations Using Permutation Matrices\n\nSwapping Color Channels\nWe can use permutation matrices to swap color channels:\n\ndef swap_colors(image, permutation_matrix):\n    \"\"\"Apply color permutation to image\"\"\"\n    matrix = image_to_matrix(image)\n    transformed = matrix @ permutation_matrix\n    return matrix_to_image(transformed, image.shape)\n\n# Example permutation matrices\nRGB_to_BGR = np.array([\n    [0, 0, 1],\n    [0, 1, 0],\n    [1, 0, 0]\n])\n\n\n\n\n\n\n\nExercise: Find all six 3×3 permutation matrices.\n\n\n\n\n\n\nIdentity:\n\\(\\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&1\\end{bmatrix}\\)\nSingle swap (R-G):\n\\(\\begin{bmatrix}0&1&0\\\\1&0&0\\\\0&0&1\\end{bmatrix}\\)\nSingle swap (R-B):\n\\(\\begin{bmatrix}0&0&1\\\\0&1&0\\\\1&0&0\\end{bmatrix}\\)\nSingle swap (G-B):\n\\(\\begin{bmatrix}1&0&0\\\\0&0&1\\\\0&1&0\\end{bmatrix}\\)\nCyclic (R→G→B→R):\n\\(\\begin{bmatrix}0&0&1\\\\1&0&0\\\\0&1&0\\end{bmatrix}\\)\nCyclic Cyclic (R→B→G→R):\n\\(\\begin{bmatrix}0&1&0\\\\0&0&1\\\\1&0&0\\end{bmatrix}\\)",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#grayscale-conversion-and-negative",
    "href": "applications/image_transforms_matrix.html#grayscale-conversion-and-negative",
    "title": "Real World Application",
    "section": "5. Grayscale Conversion and Negative",
    "text": "5. Grayscale Conversion and Negative\nWhen converting a color image to grayscale, we don’t simply average the RGB values. Our eyes have different sensitivities to different colors, with green light being perceived as brightest and blue as darkest. To create natural-looking grayscale images, we use weighted averages that match human perception: 29.9% for red, 58.7% for green, and 11.4% for blue.\nThe negative of an image can be obtained by subtracting each pixel value from the maximum possible value (255 for 8-bit images). Thanks to NumPy’s broadcasting capabilities, we can simply write 255 - image and this operation will be applied to every pixel value automatically, whether it’s a grayscale or color image. Here’s a simple function to create image negatives:\n\ndef to_grayscale(image):\n    \"\"\"Convert RGB image to grayscale using weighted sum\"\"\"\n    weights = np.array([0.299, 0.587, 0.114])\n    matrix = image_to_matrix(image)\n    grayscale_values = matrix @ weights\n    return grayscale_values.reshape(image.shape[:2])\n\ndef create_negative(image):\n    \"\"\"Create negative of an image\"\"\"\n    return 255 - image\n\n# Upload black and white image of a dog\ndog = np.array(Image.open('grayscale.png').convert('L'))\n# Show image and negative\nfig, axes = plt.subplots(2, 1, figsize=(8, 10))\naxes[0].set_title('Original Grayscale Image')\naxes[0].imshow(dog, cmap='gray')\naxes[1].set_title('Negative Image')\naxes[1].imshow(255-dog,cmap = 'gray')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#color-filters",
    "href": "applications/image_transforms_matrix.html#color-filters",
    "title": "Real World Application",
    "section": "6. Color Filters",
    "text": "6. Color Filters\n\nImplementing a Sepia Filter\nA sepia filter transforms a regular color image into one with a warm, brownish tone reminiscent of vintage photographs. To create this effect, we need to adjust each color channel using specific weights. For each pixel, the new RGB values are calculated as a combination of the original values:\nThe red channel is amplified with warm tones The green channel is moderately reduced The blue channel is significantly reduced\nThis creates the characteristic reddish-brown tint that gives sepia images their antique appearance.\n\ndef apply_sepia(image):\n    \"\"\"Apply sepia filter to image\"\"\"\n    sepia_matrix = np.array([\n        [0.393, 0.349, 0.272],\n        [0.769, 0.686, 0.534],\n        [0.189, 0.168, 0.131]\n    ])\n    \n    matrix = image_to_matrix(image)\n    sepia = matrix @ sepia_matrix\n    \n    # Clip values to valid range\n    sepia = np.clip(sepia, 0, 1)\n    return matrix_to_image(sepia, image.shape)\n\n\n\nImplementing a Color Intensification Filter\nA color intensification filter makes images more vibrant by amplifying the primary colors while reducing color bleeding between channels. To create this effect, each color channel is multiplied by 1.5 (intensifying its own color) while subtracting a quarter of the other colors’ intensities. This process:\n\nBoosts each channel’s own color\nReduces the influence of other colors\nIncreases contrast between different colored areas\n\nThis creates a more vivid appearance with enhanced color separation and impact.\n\ndef intensify_colors(image):\n    \"\"\"Apply color intensification filter to image\"\"\"\n    intensity_matrix = np.array([\n        [1.5, -0.25, -0.25],\n        [-0.25, 1.5, -0.25],\n        [-0.25, -0.25, 1.5]\n    ])\n    \n    matrix = image_to_matrix(image)\n    intensified = matrix @ intensity_matrix\n    \n    # Clip values to valid range\n    intensified = np.clip(intensified, 0, 1)\n    return matrix_to_image(intensified, image.shape)",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#example-usage",
    "href": "applications/image_transforms_matrix.html#example-usage",
    "title": "Real World Application",
    "section": "Example Usage",
    "text": "Example Usage\nHere’s how to use these transformations on an actual image:\n\n# Load an image: Open it in Image, and convert it to a numpy array as a float\nimage = np.array(Image.open('city_river.jpg')).astype(float)/255\n\n# Display original and transformed versions\nfig, axes = plt.subplots(5, 1, figsize=(10,25))\n\naxes[0].imshow(image)\naxes[0].set_title('Original')\n\naxes[1].imshow(swap_colors(image, RGB_to_BGR))\naxes[1].set_title('RGB to BGR')\n\naxes[2].imshow(to_grayscale(image), cmap='gray')\naxes[2].set_title('Grayscale')\n\naxes[3].imshow(apply_sepia(image))\naxes[3].set_title('Sepia')\n\naxes[4].imshow(intensify_colors(image))\naxes[4].set_title('Intensification')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#conclusion",
    "href": "applications/image_transforms_matrix.html#conclusion",
    "title": "Real World Application",
    "section": "Conclusion",
    "text": "Conclusion\nThis section demonstrates how matrices naturally represent and transform digital images. When an RGB image is reshaped, it becomes a matrix with three columns representing the color channels: \\[\n\\begin{bmatrix}\n\\color{red}\\uparrow & \\color{green}\\uparrow & \\color{blue}\\uparrow\\\\\n\\color{red}\\mathbf{c}_1 & \\color{green}\\mathbf{c}_2 & \\color{blue}\\mathbf{c}_3\\\\\n\\color{red}\\downarrow & \\color{green}\\downarrow & \\color{blue}\\downarrow\n\\end{bmatrix}.\n\\]\nThe power of matrix multiplication (Equation 3.1) becomes evident in image processing. When we multiply this matrix by another matrix on the right, we take linear combinations of these color channels, enabling various transformations:\n\nColor channel permutations (by rearranging columns)\nGrayscale conversion (by weighted averaging of channels)\nSepia filter effects (through specific linear combinations)\n\nThis direct connection between abstract matrix operations and visual transformations provides a concrete example of linear combinations in practice. Understanding how matrices act on these color channels helps explain why matrix multiplication works the way it does and illustrates its practical applications.",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "parts/linear_maps.html",
    "href": "parts/linear_maps.html",
    "title": "Linear Transformation",
    "section": "",
    "text": "This section covers Linear Transformations.\n\nLinear Transformations\nProblems",
    "crumbs": [
      "Linear Transformation"
    ]
  },
  {
    "objectID": "chapters/linear_maps.html",
    "href": "chapters/linear_maps.html",
    "title": "4  Linear Transformations in \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "4.1 Motivating Example\nIf \\(A\\) is an \\(m\\times n\\) matrix, Equation 3.1 tells us that \\(A\\) induces a map \\(A:\\mathbb{R}^n\\to\\mathbb{R}^m\\) defined by the Matrix-Vector formula \\(A\\mathbf{x}\\). This map has two important properties:\nThese properties combined yield a more general result. For any vectors \\(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_k \\in \\mathbb{R}^n\\) and scalars \\(c_1, c_2, \\ldots, c_k \\in \\mathbb{R}\\): \\[A(c_1\\mathbf{x}_1+c_2\\mathbf{x}_2+\\cdots+c_k\\mathbf{x}_k)=c_1(A\\mathbf{x}_1)+c_2(A\\mathbf{x}_2)+\\cdots+c_k(A\\mathbf{x}_k) \\tag{4.1}\\]\nLet’s check 1: Let \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) and \\(\\mathbf{y}=(y_1,\\dots,y_n)\\) be two arbtrary elements of \\(\\mathbb{R}^n\\). Then writing the product in terms of the columns of \\(A\\) and using standard operations of vectors we get: \\[\\begin{aligned}\nA(\\mathbf{x}+\\mathbf{y}) &=\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\\left(\n\\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_n \\end{bmatrix}  +    \n\\begin{bmatrix}y_1\\\\y_2\\\\\\vdots\\\\y_n \\end{bmatrix}     \n\\right) \\\\\n&=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\n\\begin{bmatrix}x_1+y_1\\\\x_2+y_2\\\\\\vdots\\\\x_n+y_n \\end{bmatrix} \\\\\n&= (x_1+y_1)\\mathbf{c}_1+\\cdots +(x_n+y_n)\\mathbf{c}_n\\\\\n&=(x_1\\mathbf{c}_1+\\cdots +x_n\\mathbf{c}_n)+(y_1\\mathbf{c}_1+\\cdots + y_n\\mathbf{c}_n)\\\\\n&=A\\mathbf{x}+A\\mathbf{y}.\n\\end{aligned}\n\\]\nExercise: Check property 2. That is, for every \\(\\mathbf{x}\\in\\mathbb{R}^n\\) and every \\(c\\in\\mathbb{R}\\), \\(A(c\\mathbf{x})=cA\\mathbf{x}\\).\nAs a consequence of these properties we obtain",
    "crumbs": [
      "Linear Transformation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Transformations in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/linear_maps.html#motivating-example",
    "href": "chapters/linear_maps.html#motivating-example",
    "title": "4  Linear Transformations in \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "For every \\(\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^n, A(\\mathbf{x}+\\mathbf{y})=A\\mathbf{x}+A\\mathbf{y}\\), and\nFor every \\(\\mathbf{x}\\in\\mathbb{R}^n\\) and every \\(c\\in\\mathbb{R}\\), \\(A(c\\mathbf{x})=cA\\mathbf{x}\\)\n\n\n\n\n\n\n\n\n\n\nClick to see the proof\n\n\n\n\n\nLet \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) be an arbitrary element in \\(\\mathbb{R}^n\\) and let \\(c\\in\\mathbb{R}\\) be a scalar. Writing the product in terms of the columns of \\(A\\) and using standard operations on vectors we get: \\[\n\\begin{aligned}\nA(c\\mathbf{x}) &=\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\n\\left(\nc\\begin{bmatrix}x_1\\\\ x_2\\\\\\vdots\\\\ x_n \\end{bmatrix}  \n\\right) \\\\\n&= \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\\left(\n\\begin{bmatrix}cx_1\\\\ cx_2\\\\ \\vdots\\\\ cx_n \\end{bmatrix}  \n\\right) \\\\\n&= (cx_1)\\mathbf{c}_1+\\cdots +(cx_n)\\mathbf{c}_n\\\\\n&= c(x_1\\mathbf{c}_1+\\cdots +x_n\\mathbf{c}_n)\\\\\n&= c(A\\mathbf{x})\n\\end{aligned}\n\\]\n\n\n\n\n\nLemma 4.1 Suppose that \\(A\\) is an \\(m\\times n\\) matrix and that \\(B\\) is an \\(n\\times p\\) matrix. Then for every \\(\\mathbf{x}\\in\\mathbb{R}^p\\), \\[(AB)\\mathbf{x}=A(B\\mathbf{x})\\]\n\n\nProof. Express \\(B\\) in terms of its columns: \\(B=\\begin{bmatrix}\\mathbf{b}_1&\\mathbf{b}_2&\\cdots&\\mathbf{b}_p\\end{bmatrix}\\), where each \\(\\mathbf{b}_i\\) is an \\(n\\)-dimensional column vector. By Equation 3.3, the product \\(AB\\) is defined by: \\[\nAB=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\nA\\mathbf{b}_1 & A\\mathbf{b}_2 & \\cdots & A\\mathbf{b}_p \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\n\\] Now, for any vector \\(\\mathbf{x}=(x_1,\\ldots,x_p)\\in\\mathbb{R}^p\\), we use Equation 3.1 to compute \\[\\begin{align}\n(AB)\\mathbf{x}&=x_1(A\\mathbf{b}_1)+x_2(A\\mathbf{b}_2)+\\cdots+x_p(A\\mathbf{b}_p),\\quad\\text{and}\\\\\nB\\mathbf{x}&=x_1\\mathbf{b}_1+x_2\\mathbf{b}_2+\\cdots+x_p\\mathbf{b}_p.\n\\end{align}\\] Therefore, using Equation 4.1 we obtain \\[\\begin{align*}\nA(B\\mathbf{x}) &= A(x_1\\mathbf{b}_1+x_2\\mathbf{b}_2+\\cdots+x_p\\mathbf{b}_p) \\\\\n&= x_1(A\\mathbf{b}_1)+x_2(A\\mathbf{b}_2)+\\cdots+x_p(A\\mathbf{b}_p) \\\\\n&= (AB)\\mathbf{x}\\quad\\square\n\\end{align*}\\]",
    "crumbs": [
      "Linear Transformation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Transformations in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/linear_maps.html#linear-transformations",
    "href": "chapters/linear_maps.html#linear-transformations",
    "title": "4  Linear Transformations in \\(\\mathbb{R}^n\\)",
    "section": "4.2 Linear Transformations",
    "text": "4.2 Linear Transformations\n\nDefinition: A function \\(T:\\mathbb{R}^n\\to\\mathbb{R}^m\\) is a linear transformation if it satisfies two properties:\n\nAdditivity: For every \\(\\mathbf{x}_1,\\mathbf{x}_2\\in\\mathbb{R}^n, T(\\mathbf{x}_1+\\mathbf{x}_2)=T(\\mathbf{x}_1)+T(\\mathbf{x}_2)\\), and\nScalar Multiplication: For every \\(\\mathbf{x}\\in\\mathbb{R}^n\\) and every \\(c\\in\\mathbb{R}\\), \\(T(c\\mathbf{x})=cT(\\mathbf{x})\\)\n\n\nThese properties naturally extend to any finite collection of vectors. For vectors \\(\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_k\\in\\mathbb{R}^n\\) and scalars \\(c_1,c_2,\\ldots,c_k\\in\\mathbb{R}\\), we have \\[T(c_1\\mathbf{x}_1+\\cdots+c_k\\mathbf{x}_k)=c_1T(\\mathbf{x}_1)+\\cdots+c_kT(\\mathbf{x}_k) \\tag{4.2}\\] This is an important formula that we will use many times.\nNotice that we just establihed that an \\(m\\times n\\) matrix \\(A\\) induces a linear transformation \\(A:\\mathbb{R}^n\\to\\mathbb{R}^m\\) defined by \\(A\\mathbf{x}\\). In this section we will demonstrate the converse: that any linear transformation from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\) can be expressed as matrix multiplication. The following simple example will illustrate this fundamental property.\n\nExample: Let \\(T:\\mathbb{R}^2\\to\\mathbb{R}^3\\) be defined by \\(T\\left( \\begin{bmatrix}a\\\\ b\\end{bmatrix} \\right)=\\begin{bmatrix}a+b\\\\ b-2a\\\\ a\\end{bmatrix}\\)\n\nThe first step is to show that the function is a linear transformation. Try to do it using the definition, but feel free to click to see the detailed proof.\n\nExercise: Prove that \\(T:\\mathbb{R}^2\\to\\mathbb{R}^3\\) is a linear transformation.\n\n\n\n\n\n\nClick to see the proof\n\n\n\n\n\n\nProof. To prove that \\(T\\) is a linear transformation, we must verify both properties from the definition:\n\nAdditivity: \\(T(\\mathbf{x}+\\mathbf{y})=T(\\mathbf{x})+T(\\mathbf{y})\\) for all \\(\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^2\\)\nScalar multiplication: \\(T(c\\mathbf{x})=cT(\\mathbf{x})\\) for all \\(\\mathbf{x}\\in\\mathbb{R}^2\\) and \\(c\\in\\mathbb{R}\\)\n\nProperty 1 (Additivity): Let \\(\\mathbf{x}=\\begin{bmatrix}x_1\\\\ x_2\\end{bmatrix}\\) and \\(\\mathbf{y}=\\begin{bmatrix}y_1\\\\ y_2\\end{bmatrix}\\) be arbitrary vectors in \\(\\mathbb{R}^2\\).\nFirst, let’s compute \\(T(\\mathbf{x}+\\mathbf{y})\\): \\[\\begin{align*}\nT(\\mathbf{x}+\\mathbf{y}) &= T\\left(\\begin{bmatrix}x_1+y_1\\\\ x_2+y_2\\end{bmatrix}\\right)\\\\\n&= \\begin{bmatrix}(x_1+y_1)+(x_2+y_2)\\\\ (x_2+y_2)-2(x_1+y_1)\\\\ x_1+y_1\\end{bmatrix}\\\\\n&= \\begin{bmatrix}x_1+x_2+y_1+y_2\\\\ x_2+y_2-2x_1-2y_1\\\\ x_1+y_1\\end{bmatrix}\n\\end{align*}\\]\nNow, let’s compute \\(T(\\mathbf{x})+T(\\mathbf{y})\\): \\[\\begin{align*}\nT(\\mathbf{x})+T(\\mathbf{y}) &= \\begin{bmatrix}x_1+x_2\\\\ x_2-2x_1\\\\ x_1\\end{bmatrix} + \\begin{bmatrix}y_1+y_2\\\\ y_2-2y_1\\\\ y_1\\end{bmatrix}\\\\\n&= \\begin{bmatrix}x_1+x_2+y_1+y_2\\\\ x_2-2x_1+y_2-2y_1\\\\ x_1+y_1\\end{bmatrix}\n\\end{align*}\\]\nSince these are equal for any choice of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), Property 1 is verified.\nProperty 2 (Scalar Multiplication): Let \\(\\mathbf{x}=\\begin{bmatrix}x_1\\\\ x_2\\end{bmatrix}\\) be an arbitrary vector in \\(\\mathbb{R}^2\\) and \\(c\\in\\mathbb{R}\\) be an arbitrary scalar.\nFirst, let’s compute \\(T(c\\mathbf{x})\\): \\[\\begin{align*}\nT(c\\mathbf{x}) &= T\\left(\\begin{bmatrix}cx_1\\\\ cx_2\\end{bmatrix}\\right)\\\\\n&= \\begin{bmatrix}cx_1+cx_2\\\\ cx_2-2(cx_1)\\\\ cx_1\\end{bmatrix}\\\\\n&= \\begin{bmatrix}c(x_1+x_2)\\\\ c(x_2-2x_1)\\\\ cx_1\\end{bmatrix}\n\\end{align*}\\]\nNow, let’s compute \\(cT(\\mathbf{x})\\): \\[\\begin{align*}\ncT(\\mathbf{x}) &= c\\begin{bmatrix}x_1+x_2\\\\ x_2-2x_1\\\\ x_1\\end{bmatrix}\\\\\n&= \\begin{bmatrix}c(x_1+x_2)\\\\ c(x_2-2x_1)\\\\ cx_1\\end{bmatrix}\n\\end{align*}\\]\nSince these are equal for any choice of \\(\\mathbf{x}\\) and \\(c\\), Property 2 is verified.\nTherefore, since both properties hold, \\(T\\) is indeed a linear transformation.\n\n\n\n\n\nNow that we know that \\(T:\\mathbb{R}^2\\to\\mathbb{R}^3\\) is linear we use elementary vector operations, Equation 4.2, and Equation 3.1 to obtain: \\[\\begin{aligned}\nT\\left( \\begin{bmatrix}a\\\\ b\\end{bmatrix} \\right)\n&=T\\left( a\\begin{bmatrix}1\\\\ 0\\end{bmatrix} + b\\begin{bmatrix}0\\\\ 1\\end{bmatrix}\\right) \\\\\n&=aT\\left( \\begin{bmatrix}1\\\\ 0\\end{bmatrix}\\right) + bT\\left(\\begin{bmatrix}0\\\\ 1\\end{bmatrix}\\right) \\\\\n&=a\\begin{bmatrix}1\\\\ -1\\\\ 1\\end{bmatrix} + b\\begin{bmatrix}1\\\\ 1\\\\0\\end{bmatrix}\n= \\begin{bmatrix}1 & 1\\\\ -1 &1\\\\ 1&0\\end{bmatrix}\\begin{bmatrix}a\\\\ b\\end{bmatrix}.\n\\end{aligned}\\] This means that for every \\(\\mathbf{x}\\in\\mathbb{R}^2\\), \\(T(\\mathbf{x})=A\\mathbf{x}\\) for \\(A= \\begin{bmatrix}1 & 1\\\\ -1 &1\\\\ 1&0\\end{bmatrix}\\)\n\nTheorem 4.1 Theorem: Let \\(T:\\mathbb{R}^n\\to\\mathbb{R}^m\\) be a linear transformation. Then there exists an \\(m\\times n\\) matrix \\(A\\) such that for every \\(\\mathbf{x}\\in\\mathbb{R}^n\\), \\(T(\\mathbf{x})=A\\mathbf{x}\\).\n\n\nProof. The proof consists of three main steps:\n\nFirst, let’s identify the canonical basis vectors of \\(\\mathbb{R}^n\\). Let \\(\\mathbf{e}_i\\) denote the \\(i\\)-th canonical basis vector: \\[\\mathbf{e}_1=\\begin{bmatrix}1\\\\0\\\\ \\vdots\\\\ 0\\end{bmatrix},\\quad \\mathbf{e}_2=\\begin{bmatrix}0\\\\1\\\\ \\vdots\\\\ 0\\end{bmatrix},\\quad \\ldots, \\quad \\mathbf{e}_n=\\begin{bmatrix}0\\\\0\\\\ \\vdots\\\\ 1\\end{bmatrix}\\]\nAny vector \\(\\mathbf{x}\\in\\mathbb{R}^n\\) can be written uniquely as a linear combination of these basis vectors: \\[\\mathbf{x}=\\begin{bmatrix}x_1\\\\ x_2\\\\ \\vdots\\\\ x_n\\end{bmatrix}=x_1\\mathbf{e}_1 + x_2\\mathbf{e}_2 + \\cdots + x_n\\mathbf{e}_n\\]\nNow, using the linearity of \\(T\\) (see Equation 4.2), we have: \\[\\begin{aligned}\nT(\\mathbf{x})&=T(x_1\\mathbf{e}_1 + x_2\\mathbf{e}_2 + \\cdots + x_n\\mathbf{e}_n)\\\\\n&=x_1T(\\mathbf{e}_1) + x_2T(\\mathbf{e}_2) + \\cdots + x_nT(\\mathbf{e}_n)\n\\end{aligned}\n\\]\ndefine the matrix \\(A\\) by using the transformed basis vectors as its columns: \\[A=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\nT(\\mathbf{e}_1) & T(\\mathbf{e}_2) & \\cdots & T(\\mathbf{e}_n) \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow \\\\\n\\end{bmatrix}\\]\nThen by the definition of matrix multiplication (see Equation 3.1): \\[\\begin{aligned}\nA\\mathbf{x}&=\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\nT(\\mathbf{e}_1) & T(\\mathbf{e}_2) & \\cdots & T(\\mathbf{e}_n) \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow \\\\\n\\end{bmatrix}\n\\begin{bmatrix}x_1\\\\ x_2\\\\ \\vdots\\\\ x_n\\end{bmatrix} \\\\\n&=x_1T(\\mathbf{e}_1) + x_2T(\\mathbf{e}_2) + \\cdots + x_nT(\\mathbf{e}_n)=T(\\mathbf{x})\n\\end{aligned}.\n\\]\n\nTherefore, we have constructed a matrix \\(A\\) such that \\(T(\\mathbf{x})=A\\mathbf{x}\\) for all \\(\\mathbf{x}\\in\\mathbb{R}^n\\). Note that \\(A\\) is \\(m\\times n\\) since it has \\(n\\) columns, and each column \\(T(\\mathbf{e}_i)\\in\\mathbb{R}^m\\).\n\n\n\n\n\n\n\n\nKey Algorithm\n\n\n\nThe definition of the matrix \\(A\\) provides an algorithm for finding the matrix representation of any linear transformation \\(T:\\mathbb{R}^n\\to\\mathbb{R}^m\\). Simply compute \\(T(\\mathbf{e}_i)\\) for each canonical basis vector and use these vectors as the columns of \\(A\\): \\[A=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\nT(\\mathbf{e}_1) & T(\\mathbf{e}_2) & \\cdots & T(\\mathbf{e}_n) \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\\] In other words, the \\(j\\)-th column of \\(A\\) is the output of the transformation \\(T\\) when applied to the \\(j\\)-th canonical basis vector \\(\\mathbf{e}_j\\).",
    "crumbs": [
      "Linear Transformation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Transformations in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/linear_maps.html#composition-of-linear-transformations",
    "href": "chapters/linear_maps.html#composition-of-linear-transformations",
    "title": "4  Linear Transformations in \\(\\mathbb{R}^n\\)",
    "section": "4.3 Composition of Linear Transformations",
    "text": "4.3 Composition of Linear Transformations\nRecall that if \\(T_1:\\mathbb{R}^n\\to\\mathbb{R}^m\\) and \\(T_2:\\mathbb{R}^m\\to \\mathbb{R}^p\\) are functions, the composition \\(T_2\\circ T_1:\\mathbb{R}^n\\to\\mathbb{R}^p\\) is defined by: \\[T_2\\circ T_1(\\mathbf{x})=T_2(T_1(\\mathbf{x}))\\quad\\text{for any}\\quad \\mathbf{x}\\in\\mathbb{R}^n.\\]\nIn this section we will prove that the composition of linear maps is linear and that matrix multiplication corresponds to composition of linear maps.\n\nThoerem: Suppose that \\(T_1:\\mathbb{R}^n\\to\\mathbb{R}^m\\) and \\(T_2:\\mathbb{R}^m\\to \\mathbb{R}^p\\) are linear maps. Then \\(T_2\\circ T_1:\\mathbb{R}^n\\to\\mathbb{R}^p\\) is linear.\n\n\nProof. To prove \\(T_2\\circ T_1\\) is linear, we need to show it satisfies two properties:\n\nAdditivity: \\((T_2\\circ T_1)(\\mathbf{x} + \\mathbf{y}) = (T_2\\circ T_1)(\\mathbf{x}) + (T_2\\circ T_1)(\\mathbf{y})\\) for all \\(\\mathbf{x},\\mathbf{y} \\in \\mathbb{R}^n\\)\nScalar multiplication: \\((T_2\\circ T_1)(c\\mathbf{x}) = c(T_2\\circ T_1)(\\mathbf{x})\\) for all \\(\\mathbf{x} \\in \\mathbb{R}^n\\) and \\(c \\in \\mathbb{R}\\)\n\nLet’s prove each property:\n\nFirst, let’s prove additivity. Let \\(\\mathbf{x},\\mathbf{y} \\in \\mathbb{R}^n\\). Then:\n\\((T_2\\circ T_1)(\\mathbf{x} + \\mathbf{y})\\) = \\(T_2(T_1(\\mathbf{x} + \\mathbf{y}))\\)\nSince \\(T_1\\) is linear: = \\(T_2(T_1(\\mathbf{x}) + T_1(\\mathbf{y}))\\)\nSince \\(T_2\\) is linear: = \\(T_2(T_1(\\mathbf{x})) + T_2(T_1(\\mathbf{y}))\\)\nBy definition of composition: = \\((T_2\\circ T_1)(\\mathbf{x}) + (T_2\\circ T_1)(\\mathbf{y})\\)\nNext, let’s prove the scalar multiplication property. Let \\(\\mathbf{x} \\in \\mathbb{R}^n\\) and \\(c \\in \\mathbb{R}\\). Then:\n\\((T_2\\circ T_1)(c\\mathbf{x})\\) = \\(T_2(T_1(c\\mathbf{x}))\\)\nSince \\(T_1\\) is linear: = \\(T_2(cT_1(\\mathbf{x}))\\)\nSince \\(T_2\\) is linear: = \\(cT_2(T_1(\\mathbf{x}))\\)\nBy definition of composition: = \\(c(T_2\\circ T_1)(\\mathbf{x})\\)\n\nSince both properties hold, we conclude that \\(T_2\\circ T_1\\) is linear. \\(\\square\\)\n\n\nWe now prove that matrix multiplication corresponds to composition of linear maps\n\n\nTheorem: Let \\(T_1:\\mathbb{R}^n\\to\\mathbb{R}^m\\) and \\(T_2:\\mathbb{R}^m\\to \\mathbb{R}^p\\) be linear maps with matrix representations \\(B\\) and \\(A\\) respectively. Then \\(AB\\) is the matrix representation of \\(T_2\\circ T_1\\).\n\n\nProof. Let \\(\\mathbf{x}\\in\\mathbb{R}^n\\). Since \\(B\\) is the matrix representation of \\(T_1:\\mathbb{R}^n\\to\\mathbb{R}^m\\), it follows from Theorem 4.1 that \\(T_1(\\mathbf{x})=B\\mathbf{x}\\). Similarly, since \\(A\\) is the matrix representation of \\(T_2:\\mathbb{R}^m\\to \\mathbb{R}^p\\) and \\(B\\mathbf{x}\\in\\mathbb{R}^m\\) we have that \\(T_2(B\\mathbf{x})=A(B{\\mathbf{x}})\\). Combining these two facts with Lemma 4.1, we get \\[(T_2\\circ T_1)(\\mathbf{x})=T_2(T_1(\\mathbf{x}))=T_2(B\\mathbf{x})=A(B{\\mathbf{x}})=(AB)\\mathbf{x}\\] which implies that \\(AB\\) is the matrix representation of \\(T_2\\circ T_1\\). \\(\\square\\)\n\n\nSince composition of functions is associative, the previous theorem implies that multiplication of matrices is associative\n\n\nCorollary: Suppose that \\(A, B\\) and \\(C\\) are matrices of sizes \\(m\\times n\\), \\(n\\times p\\) and \\(p\\times q\\), respectively. Then \\[(AB)C=A(BC)\\]\n\nWe can also prove associativity using Lemma 4.1 and the definitions of product Equation 3.3, and we can also prove associativity using the formula Equation 3.4 to show that \\(((AB)C)_{ij}=(A(BC))_{ij}\\).",
    "crumbs": [
      "Linear Transformation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Transformations in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "worksheets/linearMapsEquations1.html",
    "href": "worksheets/linearMapsEquations1.html",
    "title": "Problems",
    "section": "",
    "text": "Linear Maps and Linear Equations 1",
    "crumbs": [
      "Linear Transformation",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/linearMapsEquations1.html#linear-maps-and-linear-equations-1",
    "href": "worksheets/linearMapsEquations1.html#linear-maps-and-linear-equations-1",
    "title": "Problems",
    "section": "",
    "text": "Determine if the following functions are linear. If they are, write them as a matrix multiplication. If they aren’t, explain why not:\n\n\\(T : \\mathbb{R}^2 \\to \\mathbb{R}^3\\) defined by \\[T\\left(\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\right) =\n\\begin{bmatrix}\n2a + b - 1 \\\\\nb + 8a \\\\\n2a - 3b\n\\end{bmatrix}\\]\n\\(T : \\mathbb{R}^3 \\to \\mathbb{R}^2\\) defined by \\[T\\left(\\begin{bmatrix}\na \\\\\nb \\\\\nc\n\\end{bmatrix}\\right) =\n\\begin{bmatrix}\n12a - 2b + 3c \\\\\n3c - 4b + a\n\\end{bmatrix}\\]\n\\(T : \\mathbb{R}^2 \\to \\mathbb{R}^2\\) defined by \\[T\\left(\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\right) =\n\\begin{bmatrix}\na^2 - b^2 \\\\\na + b\n\\end{bmatrix}\\]\n\nUse Gaussian elimination by hand to solve \\[\\begin{bmatrix}\n1 & -2 & -1 &1 \\\\\n2 & 1 & 3 &-1 \\\\\n3 & 3 & 6 & -2\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4\n\\end{bmatrix} =\n\\begin{bmatrix}\n5 \\\\\n-1 \\\\\n-4\n\\end{bmatrix}\\] and then determine if \\(\\begin{bmatrix}\n5 \\\\\n-1 \\\\\n-4\n\\end{bmatrix}\\) can be written in terms of the columns of the matrix.\nUse Gaussian elimination by hand to solve \\[\\begin{bmatrix}\n2 & 3 & 1 \\\\\n4 & 6 & -1 \\\\\n2 & 3 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix} =\n\\begin{bmatrix}\n2 \\\\\n1 \\\\\n1\n\\end{bmatrix}\\] and then detrmine if \\(\\begin{bmatrix}\n2 \\\\\n1 \\\\\n1\n\\end{bmatrix}\\) can be written in terms of the columns of the matrix.\nLet \\(T : \\mathbb{R}^2 \\to \\mathbb{R}^2\\) be a linear map that satisfies \\[T\\left(\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\\right) =\n\\begin{bmatrix}\n2 \\\\\n5\n\\end{bmatrix}\\] and \\[T\\left(\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\\right) =\n\\begin{bmatrix}\n-1 \\\\\n2\n\\end{bmatrix}\\]\n\nFind \\(T\\left(\\begin{bmatrix}\n3 \\\\\n-4\n\\end{bmatrix}\\right)\\)\nFind \\(T\\left(\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\right)\\) (i.e., find a formula of \\(T\\))\nFind a matrix \\(A\\) such that \\[T\\left(\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\right) = A\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\]\n\nLet \\(T : \\mathbb{R}^2 \\to \\mathbb{R}^2\\) be a linear map that satisfies \\[T\\left(\\begin{bmatrix}\n2 \\\\\n5\n\\end{bmatrix}\\right) =\n\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\\] and \\[T\\left(\\begin{bmatrix}\n-1 \\\\\n2\n\\end{bmatrix}\\right) =\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\\]\n\nFind \\(T\\left(\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}\\right)\\)\nFind \\(T\\left(\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\\right)\\)\nFind \\(T\\left(\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\right)\\) (i.e., find a formula of \\(T\\))\nFind a matrix \\(A\\) such that \\[T\\left(\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\right) = A\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix}\\]\n\nFind all solutions \\[\\begin{align*}\nx + 2y - z &= 4 \\\\\n2y + 3z &= -2 \\\\\nz &= 1\n\\end{align*}\\]\nFind all solutions \\[\\begin{align*}\n2x - 3y + z + 2w &= 5 \\\\\ny - \\frac{1}{2}z - 4w &= -3 \\\\\nz + 2w &= -1\n\\end{align*}\\]\nFind all solutions \\[\\begin{align*}\nx + 3y - 2z + w &= 2 \\\\\ny - 2w &= -1 \\\\\n\\end{align*}\\]",
    "crumbs": [
      "Linear Transformation",
      "Problems"
    ]
  },
  {
    "objectID": "parts/system_equations.html",
    "href": "parts/system_equations.html",
    "title": "Systems of Linear Equations",
    "section": "",
    "text": "This section covers system of linear equations.\n\nSystems of Linear Equations\nComputational Problems\nConceptual Problems",
    "crumbs": [
      "Systems of Linear Equations"
    ]
  },
  {
    "objectID": "chapters/system_equations.html",
    "href": "chapters/system_equations.html",
    "title": "5  Systems of Linear Equations",
    "section": "",
    "text": "5.1 Equivalent Representations\nSystems of linear equations form the cornerstone of linear algebra. They emerge naturally in mathematics, physics, engineering, and data science whenever we need to describe multiple linear relationships simultaneously.\nA key observation in linear algebra is that we can represent the same system in three different ways. Each representation offers unique advantages. Consider:\n\\[\n\\begin{aligned}\n3x + 4y + z &= 9 \\\\\nx - y + 2z &= 4 \\\\\n5x - 4y + z &= 3\n\\end{aligned}\n\\]",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "chapters/system_equations.html#equivalent-representations",
    "href": "chapters/system_equations.html#equivalent-representations",
    "title": "5  Systems of Linear Equations",
    "section": "",
    "text": "As a Vector Equation\nHere, we view the system as a sum of scaled vectors: \\[\nx\\begin{bmatrix} 3 \\\\ 1 \\\\ 5 \\end{bmatrix} +\ny\\begin{bmatrix} 4 \\\\ -1 \\\\ -4 \\end{bmatrix} +\nz\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix} =\n\\begin{bmatrix} 9 \\\\ 4 \\\\ 3 \\end{bmatrix}\n\\]\n\n\nAs a Matrix Equation\nHere, we package all coefficients into a single matrix: \\[\n\\begin{bmatrix}\n3 & 4 & 1 \\\\\n1 & -1 & 2 \\\\\n5 & -4 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\ y \\\\ z\n\\end{bmatrix} =\n\\begin{bmatrix}\n9 \\\\ 4 \\\\ 3\n\\end{bmatrix}\n\\]\nThe vector equation representation offers a geometric interpretation of linear systems. When we write the system as a vector equation, we’re asking whether a target vector can be obtained as a linear combination of given vectors. This connects systems of equations directly to fundamental concepts of linear algebra: linear combinations and spanning sets. We can visualize the solution process as asking whether the target vector lies in the span of our coefficient vectors, and if so, how we can reach it through scaling and adding these vectors.\nMatrix equations package the coefficients efficiently. They are particularly useful in the square case, where the coefficient matrix is \\(n\\times n\\). When this matrix is invertible, we can solve the system directly by multiplying both sides by the inverse matrix: if \\(Ax = b\\) and \\(A\\) is invertible, then \\(x = A^{-1}b\\). This not only gives us a theoretical way to express solutions but also connects to computational methods for solving systems. Moreover, whether a matrix is invertible tells us important information about the existence and uniqueness of solutions.\nThese three representations—systems of equations, vector equations, and matrix equations—each illuminate different aspects of linear systems. Fluency in moving between these representations and understanding their connections is important for both theoretical understanding and practical problem-solving.",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "chapters/system_equations.html#characterization-of-the-solution-set",
    "href": "chapters/system_equations.html#characterization-of-the-solution-set",
    "title": "5  Systems of Linear Equations",
    "section": "5.2 Characterization of the Solution Set",
    "text": "5.2 Characterization of the Solution Set\nA fundamental result of linear algebra is that every system of linear equations must have exactly one of these three outcomes:\n\nNo solution exists (inconsistent system)\nExactly one solution exists (unique solution)\nInfinitely many solutions exist\n\nLet’s visualize each case in \\(\\mathbb{R}^2\\) using vector equations:\n\nCase 1: No Solution\n\\[\nx\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} +\ny\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} =\n\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n\\]\nThis system has no solution because:\n\nThe vectors \\(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) and \\(\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\\) span only a line\nThe target vector \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) lies off this line\nNo combination of the vectors can reach the target\n\n\n\nCase 2: Unique Solution\n\\[\nx\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} +\ny\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} =\n\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n\\]\nThis system has exactly one solution: \\(\\quad x=1\\) and \\(y=-1\\).\n\n\nCase 3: Infinite Solutions\n\\[\nx\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} +\ny\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} =\n\\begin{bmatrix} 4 \\\\ 8 \\end{bmatrix}\n\\]\nThis system has infinitely many solutions because:\n\nThe vectors are parallel (one is a multiple of the other)\nThe target vector lies on their shared line\nSolutions: \\(x = 4+4t, y = -2t\\) for any real \\(t\\)",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "chapters/system_equations.html#solution-methods",
    "href": "chapters/system_equations.html#solution-methods",
    "title": "5  Systems of Linear Equations",
    "section": "5.3 Solution Methods",
    "text": "5.3 Solution Methods\nIn Algebra 1 (typically 8th or 9th grade), students learn three approaches:\n\nGraphing (limited to \\(\\mathbb{R}^2\\))\nSubstitution\nElimination\n\nLet’s examine the algebraic methods using this system: \\[\n\\begin{aligned}\n2x + 3y &= 2 \\\\\nx + y &= 7\n\\end{aligned}\n\\]\n\nSubstitution Method\n\nChoose a variable and solve for it: \\[\nx = 7 - y \\quad \\text{(from second equation)}\n\\]\nSubstitute into the other equation: \\[\n\\begin{aligned}\n2(7-y) + 3y &= 2 \\\\\n14 - 2y + 3y &= 2 \\\\\n14 + y &= 2 \\\\\ny &= -12\n\\end{aligned}\n\\]\nBack-substitute: \\[\nx = 7 - (-12) = 19\n\\]\n\n\n\nElimination Method\n\nSet up the aligned system: \\[\n\\begin{aligned}\n2x + 3y &= 2 \\\\\nx + y &= 7\n\\end{aligned}\n\\]\nMultiply second equation by -2: \\[\n\\begin{aligned}\n2x + 3y &= 2 \\\\\n-2x - 2y &= -14\n\\end{aligned}\n\\]\nAdd equations: \\[\n\\begin{aligned}\n2x + 3y &= 2 \\\\\n-2x - 2y &= -14 \\\\\n\\hline\ny &= -12\n\\end{aligned}\n\\]\nBack-substitute: \\[\n\\begin{aligned}\n2x + 3(-12) &= 2 \\\\\n2x - 36 &= 2 \\\\\n2x &= 38 \\\\\nx &= 19\n\\end{aligned}\n\\]\n\n\nWhile both methods work, elimination systematically transforms the system into an “upper triangular” form, making solutions easier to find. Consider this upper triangular system:\n\n\\[\n\\begin{aligned}\nx_1 + 2x_2 + x_3 - x_4 &= 4 \\\\\nx_3 + x_4 &= 1 \\\\\nx_4 &= -2\n\\end{aligned}\n\\]\nFrom this form, we can easily find all solutions:\n\n\\(x_4 = -2\\)\nSubstituting \\(x_4\\) in the second equation, we get \\(x_3 = 3\\)\n\\(x_2\\) is free (can be any real number)\nSusbtituting \\(x_4\\) and \\(x_3\\) in the first equation, we get \\(x_1 = -2x_2 + 3\\)\n\nThe general solution is: \\[\n\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{bmatrix}\n=\\begin{bmatrix}-2x_2+3\\\\x_2\\\\3\\\\-2\\end{bmatrix}\n=\\begin{bmatrix}3\\\\0\\\\3\\\\-2\\end{bmatrix}+\nx_2\\begin{bmatrix}-2\\\\1\\\\0\\\\0\\end{bmatrix}\n\\]\nThis represents all solutions, with \\(x_2\\) as the free parameter.",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "chapters/system_equations.html#gauss-elimination",
    "href": "chapters/system_equations.html#gauss-elimination",
    "title": "5  Systems of Linear Equations",
    "section": "5.4 Gauss Elimination",
    "text": "5.4 Gauss Elimination\nThe elimination method discussed previously naturally extends to a systematic algorithm known as Gaussian Elimination. This algorithm formalizes and generalizes the process of using row operations to transform a system of equations into “upper triangular” form, also called echelon form. The key insight is that through a careful sequence of row operations (adding multiples of one equation to another, swapping equations, or multiplying an equation by a nonzero scalar), we can systematically create zeros below the diagonal, one column at a time.\nWhen a system is in echelon form, the pattern of coefficients forms a “staircase” shape, making it significantly easier to find solutions through back-substitution. This systematic approach works for systems of any size, making it a fundamental tool in linear algebra. The detailed algorithm and its implementation are presented in the accompanying slide presentation.\n\nWhy does Gaussian Elimination work?\nThe mathematical foundation of Gaussian elimination lies in matrix multiplication: each row operation is equivalent to left multiplication by an invertible matrix. The following theorem shows why these operations preserve the solution set of the original system.\n\nTheorem 5.1 Let \\(A\\) be an \\(m\\times n\\) matrix, \\(\\mathbf{b}\\in\\mathbb{R}^m\\) and \\(E\\) be an invertible \\(m\\times m\\) matrix. Then the following conditions are equivalent for a vector \\(\\mathbf{x}_0\\in\\mathbb{R}^n\\):\n\n\\(\\mathbf{x}_0\\) is a solution of the system \\(A\\mathbf{x}=\\mathbf{b}\\)\n\\(\\mathbf{x}_0\\) is a solution of the system \\((EA)\\mathbf{x}=E\\mathbf{b}\\)\n\n\n\nProof. The equivalence follows because \\(E\\) is invertible:\n\nIf \\(A\\mathbf{x}_0=\\mathbf{b}\\), then multiplying both sides by \\(E\\) gives \\((EA)\\mathbf{x}_0=E\\mathbf{b}\\)\nConversely, if \\((EA)\\mathbf{x}_0=E\\mathbf{b}\\), then multiplying both sides by \\(E^{-1}\\) gives \\(A\\mathbf{x}_0=\\mathbf{b}\\). \\(\\square\\)\n\n\nEach elementary row operation corresponds to multiplication by a specific type of invertible matrix:\n\nRow swap \\((R_i \\leftrightarrow R_j)\\): Multiply by a permutation matrix that differs from the identity matrix only in rows \\(i\\) and \\(j\\), where the 1’s are swapped. The inverse is itself since swapping twice returns to the original\nRow scaling \\((cR_i)\\): Multiply by diagonal matrix with \\(c\\) in position \\(i\\), 1’s elsewhere. The inverse multiplies row \\(i\\) by \\(\\frac{1}{c}\\)\nRow addition \\((R_i + cR_j)\\): Multiply by the matrix that differs from the identity only in position \\((i,j)\\) where there is a \\(c\\). This adds \\(c\\) times row \\(j\\) to row \\(i\\). The inverse has \\(-c\\) in position \\((i,j)\\), which subtracts \\(c\\) times row \\(j\\) from row \\(i\\)\n\nTherefore, if we perform a sequence of row operations to transform \\(A\\mathbf{x}=\\mathbf{b}\\) into row echelon form, we’re effectively multiplying both sides by a product of invertible matrices. This preserves the solution set while making the system easier to solve.\nThis also explains why we use the augmented matrices. Notice that \\([A|\\mathbf{b}]\\) has \\(m\\) rows and \\(n+1\\) columns, then it follows from Equation 3.3 that \\[E[A|\\mathbf{b}]=[EA|E\\mathbf{b}].\\] So row operations on the augmented matrix simultaneously transform both \\(A\\) and \\(\\mathbf{b}\\) while preserving their relationship.",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "chapters/system_equations.html#homogeneous-systems",
    "href": "chapters/system_equations.html#homogeneous-systems",
    "title": "5  Systems of Linear Equations",
    "section": "5.5 Homogeneous Systems",
    "text": "5.5 Homogeneous Systems\nA linear system \\(A\\mathbf{x}=\\mathbf{b}\\) is homogeneous if \\(\\mathbf{b}=\\mathbf{0}\\). That is, a homogeneous system has the form: \\[A\\mathbf{x}=\\mathbf{0}\\] The zero vector \\(\\mathbf{x}=\\mathbf{0}\\) is always a solution to a homogeneous system. This leads to a fundamental property: a homogeneous system has either:\n\nExactly one solution: \\(\\mathbf{x}=\\mathbf{0}\\)\nInfinitely many solutions\n\nThis dichotomy follows because if a homogeneous system has any nonzero solution \\(\\mathbf{v}\\), then \\(c\\mathbf{v}\\) is also a solution for any scalar \\(c\\), yielding infinitely many solutions.\n\nSolving Homogeneous Systems\nTo solve a homogeneous system, we reduce \\(A\\) to row echelon form and analyze the pivots.\nIn row echelon form, a free column is any column that does not contain a pivot. When solving the system, variables corresponding to free columns can take any value, while the other variables are determined by these choices. Each free column thus generates a dimension of solutions.\nConsider the system: \\[\\begin{bmatrix}\n1 & 2 & 1 & 3\\\\\n2 & 4 & 0 & 2\\\\\n3 & 6 & -1 & 1\n\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}\\]\nRow reduction gives: \\[\\begin{bmatrix}\n1 & 2 & 0 & 1\\\\\n0 & 0 & 1 & 2\\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}\\]\nNotice that columns 2 and 4 are free because they have no pivots. Using back substitution, we that the solution of the reduced system is:\n\n\\(x_4\\) is free\n\\(x_3=-2x_4\\),\n\\(x_2\\) is free, and\n\\(x_1=-2x_2-x_4\\).\n\nWriting all values in terms of \\(x_2\\) and \\(x_4\\) we see that solutions are of the form: \\[\n\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{bmatrix}\n=\\begin{bmatrix}-2x_2-x_4\\\\x_2\\\\-2x_4\\\\x_4\\end{bmatrix} =x_2\\begin{bmatrix}-2\\\\ 1\\\\ 0\\\\ 0\\end{bmatrix} + x_4\\begin{bmatrix}-1\\\\ 0\\\\ -2\\\\ 1\\end{bmatrix}\\]\nSince \\(x_2,x_4\\) can be any real numbers, the system has infinitely many solutions.\n\n\nConnection Between Solutions and Column Dependencies\nSolutions to homogeneous systems reveal linear dependencies among the columns of the coefficient matrix. Consider:\n\\[A=\\begin{bmatrix}\n1 & 2 & 1 & 3\\\\\n2 & 4 & 0 & 2\\\\\n3 & 6 & -1 & 1\n\\end{bmatrix} =\\begin{bmatrix}\n\\uparrow & \\uparrow & \\uparrow & \\uparrow \\\\\n\\mathbf{a}_1 & \\mathbf{a}_2 & \\mathbf{a}_3 & \\mathbf{a}_4 \\\\\n\\downarrow & \\downarrow & \\downarrow & \\downarrow\n\\end{bmatrix}\\]\nwhere \\(\\mathbf{a}_1,\\mathbf{a}_2,\\mathbf{a}_3,\\mathbf{a}_4\\) are the columns of \\(A\\).\nWe just saw that the general solution to \\(A\\mathbf{x}=\\mathbf{0}\\) is: \\[ \\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{bmatrix} =\nx_2\\begin{bmatrix}-2\\\\ 1\\\\ 0\\\\ 0\\end{bmatrix} +\nx_4\\begin{bmatrix}-1\\\\ 0\\\\ -2\\\\ 1\\end{bmatrix}\\]\nEach basic solution reveals a column dependency (Equation 3.1):\n\nSetting \\(x_2=1, x_4=0\\) gives solution \\((-2,1,0,0)\\in\\mathbb{R}^4\\). This means: \\[-2\\mathbf{a}_1 + \\mathbf{a}_2 = \\mathbf{0}\\quad\\text{which implies}\\quad \\mathbf{a}_2 = 2\\mathbf{a}_1.\\]\nSetting \\(x_2=0, x_4=1\\) gives solution \\((-1,0,-2,1)\\in\\mathbb{R}^4\\). This means: \\[-\\mathbf{a}_1 - 2\\mathbf{a}_3 + \\mathbf{a}_4 = \\mathbf{0}\\quad\\text{which implies}\\quad \\mathbf{a}_4 = \\mathbf{a}_1 + 2\\mathbf{a}_3.\\]\n\nThese relationships, which can be easily verified by replacing the values of the columns, show that free columns (\\(\\mathbf{a}_2\\) and \\(\\mathbf{a}_4\\)) can be expressed as linear combinations of pivot columns (\\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_3\\)).\n\n\nHomogeneous Systems: Dependence and Independence\nThe previous example illustrates important principles that we expand in this section. Let \\(A\\) be an \\(m\\times n\\) matrix with columns \\(\\mathbf{a}_1,\\mathbf{a}_2,\\dots,\\mathbf{a}_n\\in\\mathbb{R}^m\\). Three key ideas connect solutions and column dependencies:\n\nIf \\(\\mathbf{x}\\neq\\mathbf{0}\\) solves \\(A\\mathbf{x}=\\mathbf{0}\\), then \\(t\\mathbf{x}\\) is also a solution for any \\(t\\in\\mathbb{R}\\), giving infinitely many solutions.\nFrom Equation 3.1, \\(\\mathbf{x}=(x_1,\\dots,x_n)\\in\\mathbb{R}^n\\) solves \\(A\\mathbf{x}=\\mathbf{0}\\) if and only if\n\\[x_1\\mathbf{a}_1+x_2\\mathbf{a}_2+\\cdots+x_n\\mathbf{a}_n=\\mathbf{0}\\]\nWhen any \\(x_i\\neq 0\\), we can solve for \\(\\mathbf{a}_i\\), expressing \\(\\mathbf{a}_i\\) as a linear combination of the other columns: \\[\\mathbf{a}_i = -\\frac{x_1}{x_i}\\mathbf{a}_1-\\cdots-\\frac{x_{i-1}}{x_i}\\mathbf{a}_{i-1}-\\frac{x_{i+1}}{x_i}\\mathbf{a}_{i+1}-\\cdots-\\frac{x_n}{x_i}\\mathbf{a}_n.\\] Conversely, if we can write \\(\\mathbf{a}_i\\) in terms of the other columns, there exist constants \\(c_1,\\dots,c_{i-1},c_{i+1},\\dots,c_n\\) such that \\[\\mathbf{a}_i=c_1\\mathbf{a}_1+\\cdots+c_{i-1}\\mathbf{a}_{i-1}+c_{i+1}\\mathbf{a}_{i+1}+\\cdots+c_n\\mathbf{a}_n.\\] Then subtracting \\(\\mathbf{a}_i\\) from both sides, we obtain \\[\\mathbf{0}=c_1\\mathbf{a}_1+\\cdots+c_{i-1}\\mathbf{a}_{i-1}+1\\mathbf{a}_i+c_{i+1}\\mathbf{a}_{i+1}+\\cdots+c_n\\mathbf{a}_n,\\] and we obtain a non-zero solution of the homogeneous system because the coefficient of \\(\\mathbf{a}_i\\) is 1.\n\nThe following two theorems capture these important relations:\n\nTheorem 5.2 Suppose that \\(A\\) is an \\(m\\times n\\) matrix. The following are equivalent:\n\nThe homogeneous system \\(A\\mathbf{x}=\\mathbf{0}\\) has infintely many solutions\nThe homogeneous system \\(A\\mathbf{x}=\\mathbf{0}\\) has a non-zero solution\nWe can write a column of \\(A\\) as a linear combination of the other columns\nThe row reduced Echelon form of \\(A\\) has a free column\n\n\n\nTheorem 5.3 Suppose that \\(A\\) is an \\(m\\times n\\) matrix with columns \\(\\mathbf{a}_1,\\mathbf{a}_2\\dots,\\mathbf{a}_n\\in\\mathbb{R}^m\\). The following are equivalent:\n\nThe homogeneous system \\(A\\mathbf{x}=\\mathbf{0}\\) has a unique solution\nLet \\(\\mathbf{x}\\in\\mathbb{R}^n\\). If \\(A\\mathbf{x}=\\mathbf{0}\\), then \\(\\mathbf{x}=\\mathbf{0}\\)\nLet \\(c_1,\\dots,c_n\\in\\mathbb{R}\\). If \\(c_1\\mathbf{a}_1+c_2\\mathbf{a}_2+\\cdots+c_n\\mathbf{a}_n=\\mathbf{0}\\), then \\(c_1=c_2=\\cdots=c_n=0.\\)\nWe cannot write any column of \\(A\\) as a linear combination of the other columns\nThe row reduced Echelon form of \\(A\\) doesn’t have any free columns\n\n\n\nCondition 3 of Theorem 5.3 motivates the following definition\n\nDefinition 5.1 Suppose that \\(V\\) is a vector space and that \\(\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\in V\\). The set \\(S=\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\}\\) is linearly independent if for every \\(c_1,\\dots,c_k\\in\\mathbb{R}\\), if \\(c_1\\mathbf{v}_1+c_2\\mathbf{v}_2+\\cdots+c_k\\mathbf{v}_k=\\mathbf{0}\\), then \\(c_1=c_2=\\cdots=c_k=0.\\)\n\n\nNotice that \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\}\\) is linearly independent if and only if we cannot write any of these vectors as a linear combination of the other ones. However, checking this requires solving \\(k\\) vector equations.\nFrom a computational point of view, the statement in the definition is much more practical. We only solve one vector equation. We look at: \\[ c_1\\mathbf{v}_1+c_2\\mathbf{v}_2+\\cdots+c_k\\mathbf{v}_k=\\mathbf{0}\\] and we solve it. If the only solution is \\(c_1=c_2=\\cdots=c_k=0\\), the set is linearly independent.\n\nIf \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\}\\) is not linearly dependent, then it is linearly dependent. In this case we can write a vector as a linear combination of the others.",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "chapters/system_equations.html#linear-systems-and-the-span-of-columns",
    "href": "chapters/system_equations.html#linear-systems-and-the-span-of-columns",
    "title": "5  Systems of Linear Equations",
    "section": "5.6 Linear Systems and the Span of Columns",
    "text": "5.6 Linear Systems and the Span of Columns\nLet \\(A\\) be an \\(m\\times n\\) matrix with columns \\(\\mathbf{a}_1,\\mathbf{a}_2,\\dots,\\mathbf{a}_n\\in\\mathbb{R}^m\\). In this section we address three questions:\n\nGiven \\(\\mathbf{b}\\in\\mathbb{R}^m\\), when does \\(A\\mathbf{x}=\\mathbf{b}\\) have at least one solution?\nIf \\(A\\mathbf{x}=\\mathbf{b}\\) has a solution, when is that solution unique?\nUnder what conditions does \\(A\\mathbf{x}=\\mathbf{b}\\) have solutions for every \\(\\mathbf{b}\\in\\mathbb{R}^m\\)\n\nLet’s start with existence. Suppose that \\(A\\mathbf{x}=\\mathbf{b}\\) . By Equation 3.1: \\[A\\mathbf{x} = x_1\\mathbf{a}_1 + x_2\\mathbf{a}_2 + \\cdots + x_n\\mathbf{a}_n = \\mathbf{b}\\]\nTherefore, \\(\\mathbf{b}\\) must be a linear combination of the columns of \\(A\\). The set of all such linear combinations is called the span of the columns (see Section 1.3). Then we have\n\nTheorem 5.4 Let \\(A\\) be an \\(m\\times n\\) matrix and \\(\\mathbf{b}\\in\\mathbb{R}^m\\). Then the following conditions are equivalent:\n\n\\(A\\mathbf{x}=\\mathbf{b}\\) has a solution\n\\(\\mathbf{b}\\) can be written as a linear combination of the columns of \\(A\\)\n\n\nLet’s examine some examples. Consider: \\[A=\\begin{bmatrix}1&2&1&3\\\\2&4&0&2\\\\3&6&-1&1\\end{bmatrix},\\quad\\mathbf{b}_1=\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix},\\quad\\mathbf{b}_2=\\begin{bmatrix}1\\\\1\\\\0\\end{bmatrix}\\]\nFirst, let’s solve \\(A\\mathbf{x}=\\mathbf{b}_1\\). The row reduced echelon form of \\([A|\\mathbf{b}_1]\\) is: \\[\n\\left[\\begin{array}{cccc|c}1&2&1&3&1\\\\2&4&0&2&1\\\\3&6&-1&1&1\\end{array}\\right] \\xrightarrow{\\text{RREF}}\n\\left[\\begin{array}{cccc|c}1&2&0&1&\\frac{1}{2}\\\\0&0&1&2&\\frac{1}{2}\\\\0&0&0&0&0\\end{array}\\right]\n\\]\nThis system has solutions. Using back substitution:\n\n\\(x_4\\) is free\n\\(x_3=\\frac{1}{2}-2x_4\\)\n\\(x_2\\) is free\n\\(x_1=\\frac{1}{2}-2x_2-x_4\\)\n\nThe general solution is: \\[\n\\begin{bmatrix}x_1\\\\ x_2\\\\ x_3\\\\ x_4\\end{bmatrix} =\n\\begin{bmatrix}\\frac{1}{2}\\\\ 0\\\\ \\frac{1}{2}\\\\ 0\\end{bmatrix} +x_2 \\begin{bmatrix}-2\\\\ 1\\\\ 0\\\\ 0\\end{bmatrix}\n+x_4 \\begin{bmatrix}-1\\\\ 0\\\\ -2\\\\ 1\\end{bmatrix}\n\\] Since \\(x_2\\) and \\(x_4\\) can take arbitrary elements, there are infinitely many solutions. When \\(x_2=x_4=0\\), we get \\((\\frac{1}{2},0,\\frac{1}{2},0)\\), showing that: \\[\\mathbf{b}_1=\\frac{1}{2}\\mathbf{a}_1+\\frac{1}{2}\\mathbf{a}_3,\\] which we can verify by replacing the values of the columns of \\(A\\) in the equation.\nThis example illustrates the following\n\nTheorem 5.5 Let \\(A\\) be an \\(m\\times n\\) matrix, \\(\\mathbf{b}\\in\\mathbb{R}^m\\), and suppose that \\(A\\mathbf{x}=\\mathbf{b}\\) has at least one solution. Then the following conditions are equivalent:\n\n\\(A\\mathbf{x}=\\mathbf{b}\\) has a unique solution\nThe matrix \\(R\\), the reduced row echelon form of \\(A\\), has a pivot position in every column.\nThe matrix \\(R\\), the reduced row echelon form of \\(A\\), does not have a free column.\n\n\nNow consider \\(A\\mathbf{x}=\\mathbf{b}_2\\). The row reduced echelon form of \\([A|\\mathbf{b}_2]\\) is: \\[\n\\left[\\begin{array}{cccc|c}1&2&1&3&1\\\\2&4&0&2&1\\\\3&6&-1&1&0\\end{array}\\right] \\xrightarrow{\\text{RREF}}\n\\left[\\begin{array}{cccc|c}1&2&0&1&0\\\\0&0&1&2&0\\\\0&0&0&0&1\\end{array}\\right]\n\\]\nThis system has no solution since the last equation becomes: \\[0x_1+0x_2+0x_3+0x_4=1\\]\nFor the general case, let \\(\\mathbf{b}_3=\\begin{bmatrix}a\\\\b\\\\c\\end{bmatrix}\\) be arbitrary in \\(\\mathbb{R}^3\\). We can find solutions by reducing the augmented matrix \\([A|\\mathbf{b}_3]\\) to echelon form. We can do this in SymPy using the echelon_form() method that finds the row echelon form and not the row reduced echelon form.\n\nfrom sympy import Matrix, symbols\n\na,b,c = symbols(\"a b c\")\nA = Matrix([[1,2,1,3],[2,4,0,2],[3,6,-1,1]])\nB = Matrix.hstack(A,Matrix([a,b,c]))    # finds the augmented matrix\nB.echelon_form()    # finds the echelon form\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 2 & 1 & 3 & a\\\\0 & 0 & -2 & -4 & - 2 a + b\\\\0 & 0 & 0 & 0 & - 2 a + 4 b - 2 c\\end{matrix}\\right]\\)\n\n\nThe last equation is: \\[0x_1+0x_2+0x_3+0x_4=-2a+4b-2c.\\] Then we conclude that \\(A\\mathbf{x}=\\mathbf{b}_3\\) has a solution if and only if \\(-2a+4b-2c=0\\), which explains why we have a solution for \\(\\mathbf{b}_1=(1,1,1)\\) but not for \\(\\mathbf{b}_2=(1,1,0)\\).\nThe last two examples illustrate the following\n\nTheorem 5.6 Let \\(A\\) be an \\(m\\times n\\) matrix and let \\(R\\) be the reduced row echelon form of \\(A\\). The following conditions are equivalent:\n\n\\(A\\mathbf{x}=\\mathbf{b}\\) has a solution for every \\(\\mathbf{b}\\in\\mathbb{R}^m\\).\n\\(R\\) has a pivot position in every row.\n\\(R\\) does not have a zero row.\nEvery \\(\\mathbf{b}\\in\\mathbb{R}^m\\) can be written as a linear combination of the columns of \\(A\\)\n\n\nThe last condition motivates the following definition\n\nDefinition 5.2 Suppose that \\(V\\) is a vector space and that \\(\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\in V\\). The set \\(S=\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\}\\) spans \\(V\\) if for every \\(\\mathbf{v}\\in V\\), there exist \\(c_1,\\dots,c_k\\in\\mathbb{R}\\) such that \\[c_1\\mathbf{v}_1+c_2\\mathbf{v}_2+\\cdots+c_k\\mathbf{v}_k=\\mathbf{v}\\]",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "chapters/system_equations.html#finding-inverses",
    "href": "chapters/system_equations.html#finding-inverses",
    "title": "5  Systems of Linear Equations",
    "section": "5.7 Finding Inverses",
    "text": "5.7 Finding Inverses\nLet \\(A\\) be an \\(n\\times n\\) matrix. Recall that \\(A\\) is invertible if there exists an \\(n\\times n\\) matrix \\(A^{-1}\\) such that: \\[AA^{-1}=I_n\\quad\\text{and}\\quad A^{-1}A=I_n\\] where \\(I_n\\) is the \\(n\\times n\\) identity matrix. If \\(A\\) and \\(B\\) are invertible \\(n\\times n\\) matrices. Then \\(AB\\) is also invertible and the inverse is the product of their inverses in reverse order: \\[(AB)^{-1} = B^{-1}A^{-1} \\tag{5.1}\\] We can easily verify this: \\[(AB)(B^{-1}A^{-1}) = A(BB^{-1})A^{-1} = AI_nA^{-1} = AA^{-1} = I_n\\] And similarly for \\((B^{-1}A^{-1})(AB)\\).\nSuppose now that \\(A\\) is an invertible \\(n\\times n\\) matrix. Then for any \\(\\mathbf{b}\\in\\mathbb{R}^n\\), the system \\(A\\mathbf{x}=\\mathbf{b}\\) has unique solution: \\[\\mathbf{x}=A^{-1}\\mathbf{b}.\\] From Theorem 5.6, the row reduced echelon form \\(R\\) of an invertible matrix \\(A\\) must have:\n\nA pivot in every row\na pivot in every column, because \\(R\\) has an equal number of rows and columns\nNo free columns\n\nThis leads to a key result:\n\nTheorem: If \\(A\\) is an invertible \\(n\\times n\\) matrix, its row reduced echelon form \\(R\\) is the identity matrix \\(I_n\\).\n\nThis gives us an algorithm to find \\(A^{-1}\\):\n\nForm the augmented matrix \\([A|I_n]\\)\nRow reduce to get \\([I_n|B]\\) for some matrix \\(B\\)\nThen \\(B = A^{-1}\\)\n\nWe’ll explore this algorithm in detail during the lab.\n\n5.7.1 Writing Invertible Matrices as Products of Elementary Matrices\nLet \\(A\\) be an invertible matrix. Then the row reduced echelon form of \\(A\\) is the identity. Each step of the row reduction process corresponds to left multiplication by an elementary matrix. Recall that elementary matrices are invertible and their inverses are also elementary matrices. Therefore, we can find elementary matrices \\(E_1,\\ldots,E_k\\) such that: \\[E_k\\cdots E_2E_1A = I_n\\] Solving for \\(A\\) we get that \\[A=E_k^{-1}E_{k-1}^{-1}\\cdots E_2^{-1}E_1^{-1}.\\]\nTo illustrate, consider the case where \\(E_4E_3E_2E_1A = I_n\\). We can solve for \\(A\\):\n\nStart with \\(E_4E_3E_2E_1A = I_n\\)\nMultiply from the left by \\(E_4^{-1}\\):\n\n\\(E_3E_2E_1A = E_4^{-1}\\)\n\nMultiply from the left by \\(E_3^{-1}\\):\n\n\\(E_2E_1A = E_3^{-1}E_4^{-1}\\)\n\nMultiply from the left by \\(E_2^{-1}\\):\n\n\\(E_1A = E_2^{-1}E_3^{-1}E_4^{-1}\\)\n\nFinally multiply from the left by \\(E_1^{-1}\\):\n\n\\(A = E_1^{-1}E_2^{-1}E_3^{-1}E_4^{-1}\\)",
    "crumbs": [
      "Systems of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Systems of Linear Equations</span>"
    ]
  },
  {
    "objectID": "worksheets/gaussianElimination.html",
    "href": "worksheets/gaussianElimination.html",
    "title": "Computational Problems",
    "section": "",
    "text": "Gaussian Elimination",
    "crumbs": [
      "Systems of Linear Equations",
      "Computational Problems"
    ]
  },
  {
    "objectID": "worksheets/gaussianElimination.html#gaussian-elimination",
    "href": "worksheets/gaussianElimination.html#gaussian-elimination",
    "title": "Computational Problems",
    "section": "",
    "text": "Problem 1:\nFind the reduced row echelon form of the following matrices by hand:\n\n\\(\\begin{bmatrix}\n-1 & 1  \\\\\n-1 & 0  \\\\\n0 & -1  \\\\\n-1 & 2\n\\end{bmatrix}\\)\n\\(\\begin{bmatrix}\n1 & 2 & 0 \\\\\n1 & 3 & 3 \\\\\n-1 & 0 & -1 \\\\\n-3 & 0 & 0\n\\end{bmatrix}\\)\n\\(\\begin{bmatrix}\n0 & -3 & 2 & -2 \\\\\n0 & 2 & 2 & -2\n\\end{bmatrix}\\)",
    "crumbs": [
      "Systems of Linear Equations",
      "Computational Problems"
    ]
  },
  {
    "objectID": "worksheets/gaussianElimination.html#problem-2",
    "href": "worksheets/gaussianElimination.html#problem-2",
    "title": "Computational Problems",
    "section": "Problem 2:",
    "text": "Problem 2:\nSolve the following system of equations by hand:\n\\[\n\\begin{aligned}\nx_2 + 5x_3 &= -4 \\\\\nx_1 + 4x_2 + 3x_3 &= -2 \\\\\n2x_1 + 7x_2 + x_3 &= -2\n\\end{aligned}\n\\]",
    "crumbs": [
      "Systems of Linear Equations",
      "Computational Problems"
    ]
  },
  {
    "objectID": "worksheets/gaussianElimination.html#problem-3",
    "href": "worksheets/gaussianElimination.html#problem-3",
    "title": "Computational Problems",
    "section": "Problem 3:",
    "text": "Problem 3:\nSolve the following system of equations. Use SymPy to find the rref.\n\\[\n\\begin{aligned}\n6x_3 + 2x_4 - 4x_5 - 8x_6 &= 8 \\\\\n3x_3 + x_4 - 2x_5 - 4x_6 &= 4 \\\\\n2x_1 - 3x_2 + x_3 + 4x_4 - 7x_5 + x_6 &= 2 \\\\\n6x_1 - 9x_2 + 11x_4 - 19x_5 + 3x_6 &= 1\n\\end{aligned}\n\\]",
    "crumbs": [
      "Systems of Linear Equations",
      "Computational Problems"
    ]
  },
  {
    "objectID": "worksheets/alwaysSometimesNever.html",
    "href": "worksheets/alwaysSometimesNever.html",
    "title": "Conceptual Problems",
    "section": "",
    "text": "Understanding the relation between solutions of system of linear equations, linear independence, and spanning\nFor each the following statements, answer: “always”, “sometimes”, or “never”, and provide a clear explanation that justifies your answer.",
    "crumbs": [
      "Systems of Linear Equations",
      "Conceptual Problems"
    ]
  },
  {
    "objectID": "worksheets/alwaysSometimesNever.html#understanding-the-relation-between-solutions-of-system-of-linear-equations-linear-independence-and-spanning",
    "href": "worksheets/alwaysSometimesNever.html#understanding-the-relation-between-solutions-of-system-of-linear-equations-linear-independence-and-spanning",
    "title": "Conceptual Problems",
    "section": "",
    "text": "Suppose that \\(A\\) is a \\(3\\times4\\) matrix.\n\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a unique solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has infinitely many solutions.\n\nSuppose that \\(A\\) is a \\(5\\times4\\) matrix.\n\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a unique solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has infinitely many solutions.\n\nLet \\(A\\) be a \\(3\\times 4\\) matrix, with columns \\(A=\\begin{bmatrix}\\boldsymbol{c}_1&\\boldsymbol{c}_2&\\boldsymbol{c}_3&\\boldsymbol{c}_4\\end{bmatrix}\\) and with rows represented by the columns of the transpose \\(A^T=\\begin{bmatrix}\\boldsymbol{r}_1&\\boldsymbol{r}_2&\\boldsymbol{r}_3\\end{bmatrix}\\)\n\nThe columns of \\(A\\), \\(\\{\\boldsymbol{c}_1,\\boldsymbol{c}_2,\\boldsymbol{c}_3,\\boldsymbol{c}_4\\}\\), are linearly independent.\nThe columns of \\(A\\), \\(\\{\\boldsymbol{c}_1,\\boldsymbol{c}_2,\\boldsymbol{c}_3,\\boldsymbol{c}_4\\}\\), span \\(\\mathbb{R}^3\\)\nThe columns of \\(A^T\\), \\(\\{\\boldsymbol{r}_1,\\boldsymbol{r}_2,\\boldsymbol{r}_3\\}\\), are linearly independent.\nThe columns of \\(A^T\\), \\(\\{\\boldsymbol{r}_1,\\boldsymbol{r}_2,\\boldsymbol{r}_3\\}\\), span \\(\\mathbb{R}^4\\)\n\nLet \\(\\boldsymbol{b}\\in \\mathbb{R}^4\\) and let \\(A\\) be a \\(4\\times 4\\) matrix, with columns \\(A=\\begin{bmatrix}\\boldsymbol{c}_1& \\boldsymbol{c}_2&\\boldsymbol{c}_3&\\boldsymbol{c}_4\\end{bmatrix}\\). Suppose that \\(\\{\\boldsymbol{c}_1,\\boldsymbol{c}_2,\\boldsymbol{c}_3,\\boldsymbol{c}_4\\}\\) is linearly independent.\n\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a unique solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has infinitely many solutions.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a unique solution (assume there is at least one solution).\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has infinitely many solution (assume there is at least one solution).\nThe columns \\(\\{\\boldsymbol{c}_1,\\boldsymbol{c}_2,\\boldsymbol{c}_3,\\boldsymbol{c}_4\\}\\) span \\(\\mathbb{R}^4\\).\n\nLet \\(\\boldsymbol{b}\\in \\mathbb{R}^4\\) and let \\(A\\) be a \\(4\\times 4\\) matrix, with columns \\(A=\\begin{bmatrix}\\boldsymbol{c}_1& \\boldsymbol{c}_2&\\boldsymbol{c}_3&\\boldsymbol{c}_4\\end{bmatrix}\\). Suppose that \\(\\{\\boldsymbol{c}_1,\\boldsymbol{c}_2,\\boldsymbol{c}_3,\\boldsymbol{c}_4\\}\\) spans \\(\\mathbb{R}^4\\).\n\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a unique solution (assume there is at least one solution).\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has infinitely many solutions (assume there is at least one solution).\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a unique solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has infinitely many solutions.\nThe columns \\(\\{\\boldsymbol{c}_1,\\boldsymbol{c}_2,\\boldsymbol{c}_3,\\boldsymbol{c}_4\\}\\) are linearly independence.\n\nLet \\(\\boldsymbol{b}\\in \\mathbb{R}^5\\) and let \\(A\\) be a \\(5\\times 4\\) matrix with columns \\(A=\\begin{bmatrix}\\boldsymbol{c}_1& \\boldsymbol{c}_2&\\boldsymbol{c}_3&\\boldsymbol{c}_4\\end{bmatrix}\\). Suppose that \\(\\{\\boldsymbol{c}_1,\\boldsymbol{c}_2,\\boldsymbol{c}_3,\\boldsymbol{c}_4\\}\\) is linearly independent.\n\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a unique solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has infinitely many solutions.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a unique solution (assume there is at least one solution).\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has infinitely many solution (assume there is at least one solution).\n\nLet \\(\\boldsymbol{b}\\in \\mathbb{R}^4\\) and let \\(A\\) be a \\(4\\times 5\\) matrix with columns \\(A=\\begin{bmatrix}\\boldsymbol{c}_1& \\boldsymbol{c}_2&\\boldsymbol{c}_3&\\boldsymbol{c}_4&\\boldsymbol{c}_5\\end{bmatrix}\\) and suppose that the row reduced matrix of \\(A\\) has exactly one free variable.\n\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has a unique solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{0}\\) has infinitely many solutions.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a solution.\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has a unique solution (assume there is at least one solution).\nThe equation \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) has infinitely many solution (assume there is at least one solution).",
    "crumbs": [
      "Systems of Linear Equations",
      "Conceptual Problems"
    ]
  },
  {
    "objectID": "parts/subspaces_bases.html",
    "href": "parts/subspaces_bases.html",
    "title": "Subspaces and Bases",
    "section": "",
    "text": "This section covers subspaces and bases\n\nBases\nSubspaces of \\(\\mathbb{R}^n\\)\nProblems",
    "crumbs": [
      "Subspaces and Bases"
    ]
  },
  {
    "objectID": "chapters/bases.html",
    "href": "chapters/bases.html",
    "title": "6  Bases of Vector Spaces",
    "section": "",
    "text": "6.1 Definition and Properties\nBases are fundamental structures in linear algebra that provide a systematic way to represent vectors in a vector space. They act as coordinate systems, allowing us to uniquely express every vector as a combination of a minimal set of basis vectors. A deep understanding of bases is essential for:\nHere are three fundamental definitions of the theory of vector spaces.",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bases of Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/bases.html#definition-and-properties",
    "href": "chapters/bases.html#definition-and-properties",
    "title": "6  Bases of Vector Spaces",
    "section": "",
    "text": "Definition 6.1 Let \\(V\\) be a vector space. A set of vectors \\(S = \\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_k\\}\\) in \\(V\\) is linearly independent if the only solution to \\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k = \\mathbf{0}\\] is the trivial solution \\(c_1 = c_2 = \\cdots = c_k = 0\\).\nEquivalently, no vector in \\(S\\) can be written as a linear combination of the other vectors in \\(S\\).\n\n\nDefinition 6.2 Let \\(V\\) be a vector space. A set of vectors \\(S = \\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_k\\}\\) in \\(V\\) spans \\(V\\) if every vector \\(\\mathbf{v}\\in V\\) can be written as a linear combination of vectors in \\(S\\). That is, for every \\(\\mathbf{v}\\in V\\), there exist constants \\(c_1,\\ldots,c_k\\in\\mathbb{R}\\) such that \\[\\mathbf{v} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k\\]\n\n\nDefinition 6.3 Let \\(V\\) be a vector space. A set of vectors \\(S = \\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_k\\}\\) in \\(V\\) is a basis of \\(V\\) if it satisfies both:\n\n\\(S\\) is linearly independent\n\\(S\\) spans \\(V\\)\n\nIn other words, a basis is a linearly independent spanning set.\n\n\n\n\n\n\n\nThink About This\n\n\n\nA basis gives us a minimal spanning set: it contains just enough vectors to span the space, with no redundant vectors. This is why bases are so useful - they provide an efficient way to represent all vectors in the space.\n\n\n\n6.1.1 Verifying Linear Independence and Spanning\nWhen working with a set of vectors \\(S=\\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_k\\}\\), we often need to verify whether it is linearly independent or whether it spans a space. Here’s how to check each property:\n\nChecking Linear Independence\nTo verify if the set of vectors is linearly independent, we look at the homogeneous equation \\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k = \\mathbf{0}\\] and we find all solutions. If the only solution is the trivial solution \\(c_1=c_2=\\cdots=c_k=0\\), the set is linearly independent. Otherwise it is linearly dependent.\n\nExample in \\(\\mathbb{R}^3\\)\nLet’s check if the following vectors are linearly independent: \\[S=\\left\\{\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\1\\end{bmatrix}, \\begin{bmatrix}1\\\\1\\\\3\\end{bmatrix}\\right\\}\\]\nSet up the equation: \\[c_1\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix} + c_2\\begin{bmatrix}0\\\\1\\\\1\\end{bmatrix} + c_3\\begin{bmatrix}1\\\\1\\\\3\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix},\\] and solve it by converting it to an augmented matrix and then use Gaussian Elimination: \\[\n\\left[\\begin{array}{ccc|c}1&0&1&0\\\\0&1&1&0\\\\1&1&3&0\\end{array}\\right] \\xrightarrow{\\text{RREF}}\n\\left[\\begin{array}{ccc|c}1&0&0&0\\\\0&1&0&0\\\\0&0&1&0\\end{array}\\right]\n\\]\nFrom the row reduced echelon matrix we conclude that the only solution is \\(c_1=c_2=c_3=0\\), which proves that the set is linear independence.\n\n\n\nChecking Spanning\nTo verify if a set of vectors spans the vector space \\(V\\), we look at the vector equation: \\[\\mathbf{b} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k,\\] where \\(\\mathbf{b}\\in V\\) is an arbitrary vector. If this equation has a solution for every \\(\\mathbf{b}\\), the set of vectors spans \\(V\\), otherwise it doesn’t span \\(V\\).\n\nExample in \\(\\mathbb{R}^3\\)\nLet’s check if the following vectors span \\(\\mathbb{R}^3\\): \\[S=\\left\\{\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\1\\end{bmatrix}, \\begin{bmatrix}1\\\\1\\\\3\\end{bmatrix}\\right\\}\\]\nAn arbitrary vector in \\(\\mathbb{R}^3\\) is of the form \\(\\mathbf{b}=(x_1,x_2,x_3)\\).\nSet up the vector equation: \\[c_1\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix} + c_2\\begin{bmatrix}0\\\\1\\\\1\\end{bmatrix} + c_3\\begin{bmatrix}1\\\\1\\\\3\\end{bmatrix}=\\begin{bmatrix}x_1\\\\x_2\\\\x_2\\end{bmatrix},\\] and solve it by converting it to an augmented matrix and then use Gaussian Elimination: \\[\n\\left[\\begin{array}{ccc|c}1&0&1&x_1\\\\0&1&1&x_2\\\\1&1&3&x_3\\end{array}\\right] \\xrightarrow{\\text{RREF}}\n\\left[\\begin{array}{ccc|c}1&0&0&2x_1+x_2-x_3\\\\0&1&0&x_1+2x_2-x_3\\\\0&0&1&-x_1-x_2+x_3\\end{array}\\right]\n\\]\nFrom the row reduced echelon matrix we conlude that the vector equation has a solution for every \\(\\mathbf{b}\\in\\mathbf{R}^3\\), which proves that the set spans \\(\\mathbb{R}^3\\).\n\n\n\n\n\n\nImportant Points\n\n\n\n\nLinear Independence: We set the linear combination equal to zero and check for only the trivial solution\nSpanning: We set the linear combination equal to an arbitrary vector and check if solutions always exist\nLinear independence deals with uniqueness, while spanning deals with existence\n\n\n\n\n\n\n\n6.1.2 Examples of Bases\nThe most fundamental example in any \\(\\mathbb{R}^n\\) is the canonical basis (also called the standard basis) \\[\\{\\mathbf{e}_1,\\mathbf{e}_2,\\ldots,\\mathbf{e}_n\\}\\] where \\(\\mathbf{e}_i\\) has a 1 in position \\(i\\) and 0’s elsewhere: \\[\\mathbf{e}_1=\\begin{bmatrix}1\\\\0\\\\\\vdots\\\\0\\end{bmatrix},\n\\mathbf{e}_2=\\begin{bmatrix}0\\\\1\\\\\\vdots\\\\0\\end{bmatrix}, \\ldots,\n\\mathbf{e}_n=\\begin{bmatrix}0\\\\0\\\\\\vdots\\\\1\\end{bmatrix}.\\]\nHere are several examples of bases for \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\):\n\nExamples of Bases in \\(\\mathbb{R}^2\\):\n\\[S_1=\\left\\{\\begin{bmatrix}1\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\end{bmatrix}\\right\\},\\quad\n   S_2=\\left\\{\\begin{bmatrix}1\\\\1\\end{bmatrix}, \\begin{bmatrix}-1\\\\1\\end{bmatrix}\\right\\}, \\quad\n   S_3=\\left\\{\\begin{bmatrix}2\\\\0\\end{bmatrix}, \\begin{bmatrix}0\\\\3\\end{bmatrix}\\right\\}.\\]\n\n\nExamples of Bases in \\(\\mathbb{R}^3\\)\n\\[S_1=\\left\\{\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\1\\end{bmatrix}, \\begin{bmatrix}1\\\\1\\\\0\\end{bmatrix}\\right\\}, \\quad\n   S_2 = \\left\\{\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}, \\begin{bmatrix}1\\\\1\\\\0\\end{bmatrix}, \\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\\right\\}\\]\n\n\n\n6.1.3 Uniqueness of Representations\nFor any vector \\(\\mathbf{v} \\in V\\), if \\(S\\) is a basis, then \\(\\mathbf{v}\\) can be written uniquely as: \\[\\mathbf{v} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + ... + c_n\\mathbf{v}_n\\]\nExistence follows from the spanning property. To prove uniqueness, assume we have two representations:\n\n\\(\\mathbf{v} = c_1\\mathbf{v}_1 + ... + c_n\\mathbf{v}_n\\) and\n\\(\\mathbf{v} = d_1\\mathbf{v}_1 + ... + d_n\\mathbf{v}_n\\)\n\nThen, subtracting these equations, we get \\[\\mathbf{0} = (c_1-d_1)\\mathbf{v}_1 + ... + (c_n-d_n)\\mathbf{v}_n\\] Since \\(S\\) is linearly independent, we must have: \\(c_1-d_1 = c_2-d_2 = \\cdots = c_n-d_n = 0\\) and we conclude that \\(c_i = d_i\\) for all \\(i\\)",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bases of Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/bases.html#coordinates",
    "href": "chapters/bases.html#coordinates",
    "title": "6  Bases of Vector Spaces",
    "section": "6.2 Coordinates",
    "text": "6.2 Coordinates\nGiven a basis \\(S = \\{\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_n\\}\\) of a vector space \\(V\\) and a vector \\(\\mathbf{v}\\in V\\), there exist unique constants \\(c_1, c_2, ..., c_n\\in\\mathbb{R}\\) such that\n\\[\\mathbf{v} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n\\]\nThese coefficients \\((c_1,c_2,\\cdots,c_n)\\) are called the coordinates of \\(\\mathbf{v}\\) with respect to the basis \\(S\\). We write this coordinate vector as \\([\\mathbf{v}]_S\\), which is an element of \\(\\mathbb{R}^n\\).\nThe relationship between a vector and its coordinates can be expressed as:\n\\[[\\mathbf{v}]_S = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix}\\quad\\Longleftrightarrow\\quad\n\\mathbf{v} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n \\tag{6.1}\\]\nThis equivalence leads to two fundamental problems in working with coordinates:\n\nCoordinate Problem: Given a vector \\(\\mathbf{v}\\), find its coordinates \\([\\mathbf{v}]_S\\)\nReconstruction Problem: Given coordinates \\([\\mathbf{v}]_S\\), find the vector \\(\\mathbf{v}\\)\n\nLet’s explore these problems through an example:\n\nExercise: Let \\(S=\\left\\{\\begin{bmatrix}1\\\\1\\end{bmatrix},\\begin{bmatrix}0\\\\1\\end{bmatrix}\\right\\}\\) be a basis of \\(\\mathbb{R}^2\\). Find:\n\nIf \\(\\mathbf{v}=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\), find \\([\\mathbf{v}]_S\\)\nIf \\([\\mathbf{v}]_S=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\), find \\(\\mathbf{v}\\)\n\n\n\n\n\n\n\n\nClick to see detailed answers\n\n\n\n\n\n\n6.3 Problem 1\nWe have that \\(\\mathbf{v}=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\) and we need to find \\([\\mathbf{v}]_S\\)\nTo find \\([\\mathbf{v}]_S\\), we need to find scalars \\(c_1\\) and \\(c_2\\) such that:\n\\[\\mathbf{v} = c_1\\begin{bmatrix}1\\\\1\\end{bmatrix} + c_2\\begin{bmatrix}0\\\\1\\end{bmatrix}\\]\nThis gives us the system: \\(\\begin{bmatrix}2\\\\3\\end{bmatrix} = c_1\\begin{bmatrix}1\\\\1\\end{bmatrix} + c_2\\begin{bmatrix}0\\\\1\\end{bmatrix}\\)\nWhich yields: \\[\\begin{cases}\nc_1 = 2 \\quad \\text{(from first row)}\\\\\nc_1 + c_2 = 3 \\quad \\text{(from second row)}\n\\end{cases}\\]\nSolving:\n\nFrom first equation: \\(c_1 = 2\\)\nSubstitute into second: \\(2 + c_2 = 3\\)\nTherefore: \\(c_2 = 1\\)\n\nThus, \\([\\mathbf{v}]_S=\\begin{bmatrix}2\\\\1\\end{bmatrix}\\)\n\n6.3.1 Verification:\nWe can verify by checking that: \\(2\\begin{bmatrix}1\\\\1\\end{bmatrix} + 1\\begin{bmatrix}0\\\\1\\end{bmatrix} = \\begin{bmatrix}2\\\\3\\end{bmatrix}\\)\n\n\n\n6.4 Problem 2\nWe have that \\([\\mathbf{v}]_S=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\) and we need to find \\(\\mathbf{v}\\)\nFrom Equation 6.1 we have that: \\[\\mathbf{v}\n= 2\\begin{bmatrix}1\\\\1\\end{bmatrix} + 3\\begin{bmatrix}0\\\\1\\end{bmatrix}\n= \\begin{bmatrix}2\\\\2\\end{bmatrix} + \\begin{bmatrix}0\\\\3\\end{bmatrix}\n= \\begin{bmatrix}2\\\\5\\end{bmatrix}\n\\]\nTherefore, \\(\\mathbf{v}=\\begin{bmatrix}2\\\\5\\end{bmatrix}\\)\n\n6.4.1 General Method:\n\nTo find \\([\\mathbf{v}]_S\\): Solve the equation \\(\\mathbf{v} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2\\) for the coefficients\nTo find \\(\\mathbf{v}\\) from \\([\\mathbf{v}]_S\\): Multiply each basis vector by its corresponding coordinate and sum\n\n\n\n\n\n\nEvery basis \\(S\\) gives us a way to represent vectors through their coordinates. This defines a function from the vector space \\(V\\) to \\(\\mathbb{R}^n\\): \\[\\begin{align*}\n\\varphi_S: V &\\to \\mathbb{R}^n\\\\\n\\mathbf{v} &\\mapsto [\\mathbf{v}]_S\n\\end{align*}\\]\nThe following theorem shows that this coordinate map \\(\\varphi_S\\) is linear - it preserves both addition and scalar multiplication.\n\nTheorem 6.1 Suppose that \\(S=\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) is a basis of the vector space \\(V\\). Then\n\nFor every \\(\\mathbf{u},\\mathbf{w}\\in V\\), \\([\\mathbf{u}+\\mathbf{v}]_S=[\\mathbf{u}]_S+[\\mathbf{w}]_S\\).\nFor every \\(\\mathbf{u}\\in V\\) and \\(c\\in\\mathbf{R}\\), \\([c\\mathbf{u}]_S=c[\\mathbf{u}]_S\\).\n\n\n\nExercise: Prove Theorem 6.1.\n\n\n\n\n\n\n\nClick to see the proof\n\n\n\n\n\nLet’s prove that coordinates respect vector addition and scalar multiplication.\n\n6.4.2 Part 1: Addition Property\nClaim: For every \\(\\mathbf{u},\\mathbf{w}\\in V\\), \\([\\mathbf{u}+\\mathbf{w}]_S=[\\mathbf{u}]_S+[\\mathbf{w}]_S\\)\nProof: Let \\([\\mathbf{u}]_S=(a_1,\\ldots,a_n)\\) and \\([\\mathbf{w}]_S=(b_1,\\ldots,b_n)\\)\nBy definition of coordinates, this means: \\[\\mathbf{u}=a_1\\mathbf{v}_1+\\cdots+a_n\\mathbf{v}_n\\] \\[\\mathbf{w}=b_1\\mathbf{v}_1+\\cdots+b_n\\mathbf{v}_n\\]\nConsider \\(\\mathbf{u}+\\mathbf{w}\\): \\[\\begin{align*}\n   \\mathbf{u}+\\mathbf{w}&=(a_1\\mathbf{v}_1+\\cdots+a_n\\mathbf{v}_n)+(b_1\\mathbf{v}_1+\\cdots+b_n\\mathbf{v}_n)\\\\\n   &=(a_1+b_1)\\mathbf{v}_1+\\cdots+(a_n+b_n)\\mathbf{v}_n\n   \\end{align*}\\]\nTherefore, \\([\\mathbf{u}+\\mathbf{w}]_S=(a_1+b_1,\\ldots,a_n+b_n)\\)\nBut this is exactly \\([\\mathbf{u}]_S+[\\mathbf{w}]_S=(a_1,\\ldots,a_n)+(b_1,\\ldots,b_n)\\)\nTherefore, \\([\\mathbf{u}+\\mathbf{w}]_S=[\\mathbf{u}]_S+[\\mathbf{w}]_S\\)\n\n\n6.4.3 Part 2: Scalar Multiplication Property\nClaim: For every \\(\\mathbf{u}\\in V\\) and \\(c\\in\\mathbb{R}\\), \\([c\\mathbf{u}]_S=c[\\mathbf{u}]_S\\)\nProof: Let \\([\\mathbf{u}]_S=(a_1,\\ldots,a_n)\\)\nBy definition of coordinates: \\[\\mathbf{u}=a_1\\mathbf{v}_1+\\cdots+a_n\\mathbf{v}_n\\]\nConsider \\(c\\mathbf{u}\\): \\[\\begin{align*}\n   c\\mathbf{u}&=c(a_1\\mathbf{v}_1+\\cdots+a_n\\mathbf{v}_n)\\\\\n   &=(ca_1)\\mathbf{v}_1+\\cdots+(ca_n)\\mathbf{v}_n\n   \\end{align*}\\]\nTherefore, \\([c\\mathbf{u}]_S=(ca_1,\\ldots,ca_n)\\)\nBut this is exactly \\(c[\\mathbf{u}]_S=c(a_1,\\ldots,a_n)\\)\nTherefore, \\([c\\mathbf{u}]_S=c[\\mathbf{u}]_S\\)",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bases of Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/bases.html#problem-1",
    "href": "chapters/bases.html#problem-1",
    "title": "6  Bases of Vector Spaces",
    "section": "6.3 Problem 1",
    "text": "6.3 Problem 1\nWe have that \\(\\mathbf{v}=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\) and we need to find \\([\\mathbf{v}]_S\\)\nTo find \\([\\mathbf{v}]_S\\), we need to find scalars \\(c_1\\) and \\(c_2\\) such that:\n\\[\\mathbf{v} = c_1\\begin{bmatrix}1\\\\1\\end{bmatrix} + c_2\\begin{bmatrix}0\\\\1\\end{bmatrix}\\]\nThis gives us the system: \\(\\begin{bmatrix}2\\\\3\\end{bmatrix} = c_1\\begin{bmatrix}1\\\\1\\end{bmatrix} + c_2\\begin{bmatrix}0\\\\1\\end{bmatrix}\\)\nWhich yields: \\[\\begin{cases}\nc_1 = 2 \\quad \\text{(from first row)}\\\\\nc_1 + c_2 = 3 \\quad \\text{(from second row)}\n\\end{cases}\\]\nSolving:\n\nFrom first equation: \\(c_1 = 2\\)\nSubstitute into second: \\(2 + c_2 = 3\\)\nTherefore: \\(c_2 = 1\\)\n\nThus, \\([\\mathbf{v}]_S=\\begin{bmatrix}2\\\\1\\end{bmatrix}\\)\n\n6.3.1 Verification:\nWe can verify by checking that: \\(2\\begin{bmatrix}1\\\\1\\end{bmatrix} + 1\\begin{bmatrix}0\\\\1\\end{bmatrix} = \\begin{bmatrix}2\\\\3\\end{bmatrix}\\)",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bases of Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/bases.html#problem-2",
    "href": "chapters/bases.html#problem-2",
    "title": "6  Bases of Vector Spaces",
    "section": "6.4 Problem 2",
    "text": "6.4 Problem 2\nWe have that \\([\\mathbf{v}]_S=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\) and we need to find \\(\\mathbf{v}\\)\nFrom Equation 6.1 we have that: \\[\\mathbf{v}\n= 2\\begin{bmatrix}1\\\\1\\end{bmatrix} + 3\\begin{bmatrix}0\\\\1\\end{bmatrix}\n= \\begin{bmatrix}2\\\\2\\end{bmatrix} + \\begin{bmatrix}0\\\\3\\end{bmatrix}\n= \\begin{bmatrix}2\\\\5\\end{bmatrix}\n\\]\nTherefore, \\(\\mathbf{v}=\\begin{bmatrix}2\\\\5\\end{bmatrix}\\)\n\n6.4.1 General Method:\n\nTo find \\([\\mathbf{v}]_S\\): Solve the equation \\(\\mathbf{v} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2\\) for the coefficients\nTo find \\(\\mathbf{v}\\) from \\([\\mathbf{v}]_S\\): Multiply each basis vector by its corresponding coordinate and sum",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bases of Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/bases.html#dimension",
    "href": "chapters/bases.html#dimension",
    "title": "6  Bases of Vector Spaces",
    "section": "6.5 Dimension",
    "text": "6.5 Dimension\nA very important linear algebra result is that all bases of the same vector space have the same number of elements. This isn’t obvious at first glance – after all, we can find many different bases for a vector space. Yet this common number, which we call the dimension of the vector space, is a fundamental invariant.\nSince the cannonical basis of \\(\\mathbb{R}^n\\) has \\(n\\) elements, all bases have \\(n\\)-elements, and \\(\\mathbb{R}^n\\) has dimension equal to \\(n\\). This result is stated in the next theorem:\n\nTheorem 6.2 Suppose that \\(S = \\{\\mathbf{v}_1,...,\\mathbf{v}_k\\}\\) is a subset of \\(\\mathbb{R}^n\\). Then\n\nIf \\(S\\) is linearly independent, \\(k\\leq n\\).\nIf \\(S\\) spans \\(\\mathbb{R}^n\\), then \\(k\\geq n\\).\n\nConsequently, if \\(S\\) is a basis (both linearly independent and spanning), then \\(k=n\\).\n\n\nProof. To prove this theorem, we analyze the \\(n\\times k\\) matrix whose columns are the vectors in \\(S\\): \\[\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{v}_1 & \\mathbf{v}_2 & \\cdots & \\mathbf{v}_k \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}.\\]\nLet \\(R\\) be the reduced row echelon form (RREF) of this matrix. We then consider two cases:\n\nCase of Linear Independence: If \\(S\\) is linearly independent, then by Theorem 5.3, \\(R\\) has a pivot position in every column. Since \\(S\\) contains \\(k\\) vectors, \\(R\\) has \\(k\\) columns and therefore \\(k\\) pivots. However, in a matrix with \\(n\\) rows, we cannot have more than \\(n\\) pivots (since we can have at most one pivot per row). Therefore, \\(k \\leq n\\).\nCase of Spanning: If \\(S\\) spans \\(\\mathbb{R}^n\\), then by Theorem 5.6, \\(R\\) has a pivot position in every row. Since \\(R\\) has \\(n\\) rows, it must have \\(n\\) pivots. However, we cannot have more pivots than columns (since we can have at most one pivot per column), and \\(R\\) has \\(k\\) columns (one for each vector in \\(S\\)). Therefore, \\(k \\geq n\\). \\(\\square\\)\n\n\n\n6.5.1 Special Case: \\(n\\) Vectors in \\(\\mathbb{R}^n\\)\nThe previous theorem has a remarkable consequence when the number of vectors equals the dimension. In this case, either property (linear independence or spanning) automatically implies the other.\n\nCorollary 6.1 Let \\(S = \\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\}\\) be a set of exactly \\(n\\) vectors in \\(\\mathbb{R}^n\\). Then:\n\nIf \\(S\\) is linearly independent, then \\(S\\) automatically spans \\(\\mathbb{R}^n\\)\nIf \\(S\\) spans \\(\\mathbb{R}^n\\), then \\(S\\) is automatically linearly independent\n\nIn other words, for a set of \\(n\\) vectors in \\(\\mathbb{R}^n\\), either property alone is sufficient to prove that \\(S\\) is a basis.\n\n\n\n\n\n\n\nIdea of the Proof\n\n\n\nThe proof follows from the same matrix analysis as before:\n\nLinear independence gives us \\(n\\) pivots in \\(n\\) columns, forcing every row to have a pivot\nSpanning gives us \\(n\\) pivots in \\(n\\) rows, forcing every column to have a pivot",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bases of Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/bases.html#orthonormal-bases-of-mathbbrn",
    "href": "chapters/bases.html#orthonormal-bases-of-mathbbrn",
    "title": "6  Bases of Vector Spaces",
    "section": "6.6 Orthonormal Bases of \\(\\mathbb{R}^n\\)",
    "text": "6.6 Orthonormal Bases of \\(\\mathbb{R}^n\\)\nA particularly useful type of basis combines orthogonality and normalization:\n\nA basis \\(S = \\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\}\\) of \\(\\mathbb{R}^n\\) is orthonormal if:\n\n\\(\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0\\) for all \\(i \\neq j\\) (orthogonal vectors)\n\\(\\|\\mathbf{v}_i\\| = 1\\) for all \\(i\\) (unit vectors)\n\n\nThe standard basis \\(\\{\\mathbf{e}_1,\\ldots,\\mathbf{e}_n\\}\\) is the most obvious example of an orthonormal basis, but there are many, like this example in \\(\\mathbb{R}^3\\):\n\\[\\left\\{\\frac{1}{\\sqrt{3}}\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}, \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\-1\\\\0\\end{bmatrix}, \\frac{1}{\\sqrt{6}}\\begin{bmatrix}1\\\\1\\\\-2\\end{bmatrix}\\right\\}.\\]\n\n6.6.1 Finding Coordinates\nWorking with orthonormal bases simplifies finding coordinates dramatically. For any basis, finding \\([\\mathbf{v}]_S\\) means solving a system of equations. However, with an orthonormal basis \\(S = \\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\}\\), we can find each coordinate independently: \\[[\\mathbf{v}]_S = \\begin{bmatrix} \\mathbf{v}\\cdot\\mathbf{v}_1 \\\\ \\vdots \\\\ \\mathbf{v}\\cdot\\mathbf{v}_n \\end{bmatrix}\\]\nTo see this, suppose that \\([\\mathbf{v}]_S=(c_1,c_2,\\dots,c_n)\\). Then \\[\\mathbf{v}=c_1\\mathbf{v}_1+c_2\\mathbf{v}_2+\\cdots+c_n\\mathbf{v}_n.\\] Fix \\(i\\leq n\\) and take the dot product of \\(\\mathbf{v}\\) with \\(\\mathbf{v}_i\\) to get: \\[\\mathbf{v}\\cdot\\mathbf{v}_i =c_1(\\mathbf{v}_1\\cdot\\mathbf{v}_i)+c_2(\\mathbf{v}_2\\cdot\\mathbf{v}_i)+\\cdots+c_n(\\mathbf{v}_n\\cdot\\mathbf{v}_i).\\] Since \\(S = \\{\\mathbf{v}_1,\\ldots,\\mathbf{v}_n\\}\\) is orthonormal, \\(\\mathbf{v}_j\\cdot\\mathbf{v}_i=1\\) if \\(j=i\\) and \\(\\mathbf{v}_j\\cdot\\mathbf{v}_i=0\\) if \\(j\\not=i\\). Therefore \\[\\mathbf{v}\\cdot\\mathbf{v}_i=c_i.\\] This shows we can find each coefficient by computing a dot product.\n\n\n6.6.2 Constructing Orthonormal Bases\nFinding orthonormal bases usually requires computation (like the Gram-Schmidt process, which we’ll see in the lab). However, some special orthonormal bases are known. One remarkable example is the Hadamard basis, which exists in dimensions that are powers of 2. For \\(n=4\\), the Hadamard basis is:\n\\[\\left\\{\n\\frac{1}{2}\\begin{bmatrix}1\\\\1\\\\1\\\\1\\end{bmatrix},\n\\frac{1}{2}\\begin{bmatrix}1\\\\1\\\\-1\\\\-1\\end{bmatrix},\n\\frac{1}{2}\\begin{bmatrix}1\\\\-1\\\\1\\\\-1\\end{bmatrix},\n\\frac{1}{2}\\begin{bmatrix}1\\\\-1\\\\-1\\\\1\\end{bmatrix}\n\\right\\}\\]\n\n\n6.6.3 Orthogonal Matrices\n\nDefinition 6.4 An \\(n\\times n\\) matrix \\(A\\) is said to be an orthogonal matrix if \\[A^TA=AA^T=I.\\]\n\nIf follows from Equation 3.5 that the \\(n\\times n\\) matrix \\(A\\) is orthogonal if and only if the columns of \\(A\\) are an orthonormal basis of \\(\\mathbb{R}^n\\).\nThis property makes orthogonal matrices particularly useful because:\n\nThe inverse is simply the transpose: \\(A^{-1} = A^T\\)\nThey preserve distances: \\(\\|A\\mathbf{v}\\| = \\|\\mathbf{v}\\|\\)\nThey preserve angles: if \\(\\mathbf{u}\\cdot\\mathbf{v} = 0\\), then \\((A\\mathbf{u})\\cdot(A\\mathbf{v}) = 0\\)\n\nCommon examples include:\n\nRotation matrices in \\(\\mathbb{R}^2\\): \\(\\begin{bmatrix}\\cos\\theta & -\\sin\\theta\\\\\\sin\\theta & \\cos\\theta\\end{bmatrix}\\)\nReflection matrices across a unit vector \\(\\mathbf{u}\\): \\(I - 2\\mathbf{u}\\mathbf{u}^T\\)\n\n\nEvery orthogonal matrix represents either a rotation, a reflection, or a combination of both. These transformations preserve distances and angles, making them crucial in applications from computer graphics to quantum mechanics.",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bases of Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/subspaces_Rn.html",
    "href": "chapters/subspaces_Rn.html",
    "title": "7  Subspaces of \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "7.1 The Null Space\nRecall that a subset \\(W\\) of \\(\\mathbb{R}^n\\) is a subspace of \\(\\mathbb{R}^n\\) if it satisfies the following three conditions:\nWe showed in Theorem 2.1 that subspaces are also vector spaces with the operations they inherit.\nSince \\(\\text{nul}(A)\\) consists of the solutions of the homogeneous system \\(A\\mathbf{x}=\\mathbf{0}\\), we solve it using Gaussain elimination and back substitution. We will see that this process gives a basis of \\(\\text{nul}(A)\\).\nExample 1: Find a basis for the null space of \\[A = \\begin{bmatrix}\n-1 & 2 & 0 & 1 & 1  \\\\\n2 & 2 & 6 & 4 & -1  \\\\\n1 & -1 & 1 & 0 & 1  \\\\\n1 & 2 & 4 & 3 & 2\n\\end{bmatrix}.\\]\nTo find the solutions of \\(A\\mathbf{x}=\\mathbf{0}\\), we look at the augmented matrix \\([A|\\mathbf{0}]\\), row reduce it:\n\\[\\left[\n\\begin{array}{ccccc|c}\n-1 & 2 & 0 & 1 & 1 & 0 \\\\\n2 & 2 & 6 & 4 & -1 & 0 \\\\\n1 & -1 & 1 & 0 & 1 & 0 \\\\\n1 & 2 & 4 & 3 & 2 & 0\n\\end{array}\\right]\n\\xrightarrow{\\text{RREF}}\n\\left[\n\\begin{array}{ccccc|c}\n1 & 0 & 2 & 1 & 0 & 0\\\\\n0 & 1 & 1 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\n\\right],\\] and then we solve the system by back substitution:\nWrting the solution in vector form we get:\n\\[\\mathbf{x}=\\begin{bmatrix}x_1\\\\ x_2\\\\ x_3\\\\ x_4\\\\ x_5\\end{bmatrix}\n=\\begin{bmatrix}-2x_3-x_4\\\\-x_3-x_4\\\\ x_3\\\\ x_4\\\\0\\end{bmatrix}\n=x_3\\begin{bmatrix}-2\\\\-1\\\\ 1\\\\ 0\\\\0\\end{bmatrix}\n+x_4\\begin{bmatrix}-1\\\\-1\\\\ 0\\\\ 1\\\\0\\end{bmatrix}.\n\\]\nWe claim that \\[S=\\left\\{\\begin{bmatrix}-2\\\\-1\\\\ 1\\\\ 0\\\\0\\end{bmatrix},\n\\begin{bmatrix}-1\\\\-1\\\\ 0\\\\ 1\\\\0\\end{bmatrix}\\right\\}\\] is a basis for \\(\\text{nul}(A)\\). Indeed, we just showed that any solution of \\(A\\mathbf{x}=\\mathbf{0}\\) can be written as a linear combination of these vectors, showing that \\(S\\) spans \\(\\text{nul}(A)\\). Then notice that \\(S\\) is linearly independent, proving that \\(S\\) is a basis of \\(\\text{nul}(A)\\).",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Subspaces of $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/subspaces_Rn.html#the-null-space",
    "href": "chapters/subspaces_Rn.html#the-null-space",
    "title": "7  Subspaces of \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "Definition 7.1 Let \\(A\\) be an \\(m\\times n\\) matrix, the null space of the matrix \\(A\\) is defined by \\[\\text{nul}(A) = \\{\\mathbf{x}\\in\\mathbb{R}^n:A\\mathbf{x}=\\mathbf{0}\\}\\]\n\n\nProposition: Let \\(A\\) be an \\(m\\times n\\) matrix. Then nul\\((A)\\) is a subspace of \\(\\mathbb{R}^n\\)\n\n\nProof. We need to check the three properties:\n\nZero Vector Property: \\(A\\mathbf{0} = \\mathbf{0}\\) by properties of matrix multiplication. Therefore, \\(\\mathbf{0} \\in \\text{nul}(A)\\)\nClosure under Addition: Let \\(\\mathbf{x}, \\mathbf{y} \\in \\text{nul}(A)\\). Then \\(A\\mathbf{x} = \\mathbf{0}\\) and \\(A\\mathbf{y} = \\mathbf{0}\\). Then \\(A(\\mathbf{x} + \\mathbf{y}) = A\\mathbf{x} + A\\mathbf{y} = \\mathbf{0} + \\mathbf{0} = \\mathbf{0}\\). Therefore, \\(\\mathbf{x} + \\mathbf{y} \\in \\text{nul}(A)\\)\nClosure under Scalar Multiplication: Let \\(\\mathbf{x} \\in \\text{nul}(A)\\) and \\(c \\in \\mathbb{R}\\). Then \\(A(c\\mathbf{x}) = cA\\mathbf{x} = c\\mathbf{0} = \\mathbf{0}\\). Therefore, \\(c\\mathbf{x} \\in \\text{nul}(A)\\) \\(\\square\\)\n\n\n\nIf \\(A\\) is an \\(m\\times n\\) matrix:\n\n\\(\\text{nul}(A)\\) is a subspace of \\(\\mathbb{R}^n\\), and\n\\(\\text{nul}(A^T)\\) is a subspace of \\(\\mathbb{R}^m\\).\n\n\n\n\n\n\n\nFrom the third row: \\(x_5=0\\)\n\\(x_4\\) is free\n\\(x_3\\) is free\nFrom the second row: \\(x_2= -x_3-x_4\\), and\nFrom the first row: \\(x_1=-2x_3-x_4\\).\n\n\n\n\n\n\n\n\n\n\nGaussian Elimination Naturally Produces Basis Vectors\n\n\n\nThe point is that back substitution gives us vectors that:\n\nAre automatically linearly independent (due to the staggered positioning of pivots and free variables)\nGenerate all solutions (since we systematically express each variable in terms of free variables)\n\nThese two properties are exactly what’s needed for a basis. The systematic nature of Gaussian elimination ensures this happens every time.",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Subspaces of $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/subspaces_Rn.html#the-span-of-a-set-of-vectors",
    "href": "chapters/subspaces_Rn.html#the-span-of-a-set-of-vectors",
    "title": "7  Subspaces of \\(\\mathbb{R}^n\\)",
    "section": "7.2 The Span of a Set of Vectors",
    "text": "7.2 The Span of a Set of Vectors\nLet \\(\\mathbf{v}_1, ..., \\mathbf{v}_k \\in \\mathbb{R}^n\\). Recall (Section 1.3) that the span of a set of vectors is defined by \\[\\text{span}\\{\\mathbf{v}_1, ..., \\mathbf{v}_k\\} = \\{c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + ... + c_k\\mathbf{v}_k : c_1, c_2, ..., c_k \\in \\mathbb{R}\\}.\\]\nParts of the following theorem were discussed in Theorem 1.1\n\nProposition: \\(\\text{span}\\{\\mathbf{v}_1, ..., \\mathbf{v}_k\\}\\) is a subspace of \\(\\mathbb{R}^n\\)\n\n\nProof. Let \\(W=\\text{span}\\{\\mathbf{v}_1, ..., \\mathbf{v}_k\\}\\). We need to check the three conditions:\n\nZero Vector Property: Let \\(c_1 = c_2 = ... = c_k = 0\\) Then \\(0\\mathbf{v}_1 + 0\\mathbf{v}_2 + ... + 0\\mathbf{v}_k = \\mathbf{0}\\) Therefore, \\(\\mathbf{0} \\in W\\)\nClosure under Addition: Let \\(\\mathbf{x}, \\mathbf{y} \\in W\\) where: \\(\\mathbf{x} = a_1\\mathbf{v}_1 + ... + a_k\\mathbf{v}_k\\) \\(\\mathbf{y} = b_1\\mathbf{v}_1 + ... + b_k\\mathbf{v}_k\\)\nThen: \\(\\mathbf{x} + \\mathbf{y} = (a_1+b_1)\\mathbf{v}_1 + ... + (a_k+b_k)\\mathbf{v}_k \\in W\\)\nClosure under Scalar Multiplication: Let \\(\\mathbf{x} \\in W\\) where \\(\\mathbf{x} = a_1\\mathbf{v}_1 + ... + a_k\\mathbf{v}_k\\) For any \\(c \\in \\mathbb{R}\\): \\(c\\mathbf{x} = (ca_1)\\mathbf{v}_1 + ... + (ca_k)\\mathbf{v}_k \\in W\\) \\(\\square\\)\n\n\nAn important example is the column space of a matrix:\n\nDefinition 7.2 Let \\(A\\) be an \\(m\\times n\\) matrix \\[A = \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix},\\] \\(\\text{col}(A)=\\text{span}\\{\\mathbf{c}_1,\\mathbf{c}_2,\\dots,\\mathbf{c}_n\\}\\)\n\n\nIf \\(A\\) is an \\(m\\times n\\) matrix:\n\n\\(\\text{col}(A)\\) is a subspace of \\(\\mathbb{R}^m\\), and\n\\(\\text{col}(A^T)\\) is a subspace of \\(\\mathbb{R}^n\\).\n\n\n\n7.2.1 Finding bases\nLet \\(W=\\text{span}\\{\\mathbf{v}_1, ..., \\mathbf{v}_k\\}\\subset\\mathbb{R}^n\\). A basis of \\(W\\) is a set of vectors that spans \\(W\\) and that is linearly independent. We already have a spanning set \\(\\{\\mathbf{v}_1, ..., \\mathbf{v}_k\\}\\). If this set is linearly independent, it forms a basis. Otherwise, some vectors can be written as linear combinations of others. Either way, the first step is to check if the set is linearly independent. This leads to a homogeneous system that is solved using Gaussain elimination. This process will tell us how to remove the dependent vectors systematically keeping only the vectors corresponding to pivot columns. This process yields a basis that spans the same subspace \\(W\\).\nExample: Find a basis for \\[W=\\text{span}\\left\\{   \n   \\begin{bmatrix}-1\\\\2\\\\1\\\\1\\end{bmatrix},\n   \\begin{bmatrix}2\\\\2\\\\-1\\\\2\\end{bmatrix},\n   \\begin{bmatrix}0\\\\6\\\\1\\\\4\\end{bmatrix},\n   \\begin{bmatrix}1\\\\4\\\\0\\\\3\\end{bmatrix},\n   \\begin{bmatrix}1\\\\-1\\\\1\\\\2\\end{bmatrix}\n\\right\\}\\]\nNotice that this is the same problem as finding a basis for \\(\\text{col}(A)\\), where \\(A\\) is the matrix of Example 1: \\[A = \\begin{bmatrix}\n-1 & 2 & 0 & 1 & 1  \\\\\n2 & 2 & 6 & 4 & -1  \\\\\n1 & -1 & 1 & 0 & 1  \\\\\n1 & 2 & 4 & 3 & 2\n\\end{bmatrix}=\n  \\begin{bmatrix}\n  \\uparrow & \\uparrow & \\uparrow & \\uparrow &\\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\mathbf{c}_3 & \\mathbf{c}_4 & \\mathbf{c}_5 \\\\\n\\downarrow & \\downarrow & \\downarrow & \\downarrow & \\downarrow\n  \\end{bmatrix}.\\]\nThe first step is to check if the set is linearly independent. We look at the equation \\[x_1\\begin{bmatrix}-1\\\\2\\\\1\\\\1\\end{bmatrix}+\n  x_2 \\begin{bmatrix}2\\\\2\\\\-1\\\\2\\end{bmatrix}+\n  x_3 \\begin{bmatrix}0\\\\6\\\\1\\\\4\\end{bmatrix}+\n  x_4 \\begin{bmatrix}1\\\\4\\\\0\\\\3\\end{bmatrix}+\n  x_5 \\begin{bmatrix}1\\\\-1\\\\1\\\\2\\end{bmatrix} =\n  \\begin{bmatrix}0\\\\0\\\\0\\\\0\\end{bmatrix},\n  \\] and we solve it by writing the augmented matrix and finding the row reduced echelon form.\n\\[\\left[\n\\begin{array}{ccccc|c}\n-1 & 2 & 0 & 1 & 1 & 0 \\\\\n2 & 2 & 6 & 4 & -1 & 0 \\\\\n1 & -1 & 1 & 0 & 1 & 0 \\\\\n1 & 2 & 4 & 3 & 2 & 0\n\\end{array}\\right]\n\\xrightarrow{\\text{RREF}}\n\\left[\n\\begin{array}{ccccc|c}\n1 & 0 & 2 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{array}\n\\right].\\]\nWith three pivots and two free columns, the columns are not linearly independent. We use the null space basis from Example 1: \\[S=\\left\\{\\begin{bmatrix}-2\\\\-1\\\\ 1\\\\ 0\\\\0\\end{bmatrix},\n\\begin{bmatrix}-1\\\\-1\\\\ 0\\\\ 1\\\\0\\end{bmatrix}\\right\\}\\]\nThese vectors reveal the dependency of non-pivot columns on pivot columns. Since: \\[A\\begin{bmatrix}-2\\\\-1\\\\ 1\\\\ 0\\\\0\\end{bmatrix}=\\mathbf{0} \\quad\\text{and}\\quad A\\begin{bmatrix}-1\\\\-1\\\\ 0\\\\ 1\\\\0\\end{bmatrix}=\\mathbf{0}\\]\nIt follows from Equation 3.1 that \\(-2\\mathbf{c}_1-\\mathbf{c}_2+\\mathbf{c}_3=\\mathbf{0}\\) and \\(-\\mathbf{c}_1-\\mathbf{c}_2+\\mathbf{c}_4=\\mathbf{0}\\), yielding: \\[\\begin{align}\n\\mathbf{c}_3 &= 2\\mathbf{c}_1+\\mathbf{c}_2\\\\\n\\mathbf{c}_4 &=\\mathbf{c}_1+\\mathbf{c}_2\n\\end{align}\\]\nThese equations show we can express \\(\\mathbf{c}_3\\) and \\(\\mathbf{c}_4\\) using \\(\\{\\mathbf{c}_1,\\mathbf{c}_2,\\mathbf{c}_5\\}\\). Therefore, \\(\\{\\mathbf{c}_1,\\mathbf{c}_2,\\mathbf{c}_5\\}\\) spans the column space of \\(A\\). Since this set is linearly independent, it forms a basis: \\[\\left\\{   \n   \\begin{bmatrix}-1\\\\2\\\\1\\\\1\\end{bmatrix},\n   \\begin{bmatrix}2\\\\2\\\\-1\\\\2\\end{bmatrix},\n   \\begin{bmatrix}1\\\\-1\\\\1\\\\2\\end{bmatrix}\n\\right\\}\\]\n\n\n\n\n\n\nPivot Columns Form a Basis for Col(A)\n\n\n\nThe pivot columns from RREF give us a basis because:\n\nThey are linearly independent (due to the pivot structure in RREF)\nEvery non-pivot column can be written as a combination of pivot columns (using RREF coefficients)\n\nThese two properties are precisely what defines a basis. Since this comes from the systematic row reduction process, it works every time.",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Subspaces of $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/subspaces_Rn.html#the-orthogonal-complement",
    "href": "chapters/subspaces_Rn.html#the-orthogonal-complement",
    "title": "7  Subspaces of \\(\\mathbb{R}^n\\)",
    "section": "7.3 The Orthogonal Complement",
    "text": "7.3 The Orthogonal Complement\nFor a non-empty subset \\(S \\subseteq \\mathbb{R}^n\\), the set \\[S^\\perp = \\{\\mathbf{v} \\in \\mathbb{R}^n : \\mathbf{v} \\cdot \\mathbf{u} = 0 \\text{ for all } \\mathbf{u} \\in S\\}\\] is called the orthogonal complement of \\(S\\).\n\nProposition: \\(S^\\perp\\) is a subspace of \\(\\mathbb{R}^n\\)\n\n\nProof. We need to check the three conditions:\n\nZero Vector Property: For any \\(\\mathbf{u} \\in S\\), \\(\\mathbf{0} \\cdot \\mathbf{u} = 0\\) Therefore, \\(\\mathbf{0} \\in S^\\perp\\)\nClosure under Addition: Let \\(\\mathbf{x}, \\mathbf{y} \\in S^\\perp\\) and \\(\\mathbf{u} \\in S\\) \\((\\mathbf{x} + \\mathbf{y}) \\cdot \\mathbf{u} = \\mathbf{x} \\cdot \\mathbf{u} + \\mathbf{y} \\cdot \\mathbf{u} = 0 + 0 = 0\\) Therefore, \\(\\mathbf{x} + \\mathbf{y} \\in S^\\perp\\)\nClosure under Scalar Multiplication: Let \\(\\mathbf{x} \\in S^\\perp\\), \\(c \\in \\mathbb{R}\\), and \\(\\mathbf{u} \\in S\\) \\((c\\mathbf{x}) \\cdot \\mathbf{u} = c(\\mathbf{x} \\cdot \\mathbf{u}) = c(0) = 0\\) Therefore, \\(c\\mathbf{x} \\in S^\\perp\\) \\(\\square\\)",
    "crumbs": [
      "Subspaces and Bases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Subspaces of $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "worksheets/bases_subspaces.html",
    "href": "worksheets/bases_subspaces.html",
    "title": "Problems",
    "section": "",
    "text": "Bases Coordinates and Subspaces\nFor these problems, you should use SymPy to find row reduced echelon matrices. Once you write a matrix \\(A\\), you can simply write the matrix \\(R\\) indicating that it is the row reduced one. You can use symbols like this: \\(A\\xrightarrow{\\text{RREF}}R\\).",
    "crumbs": [
      "Subspaces and Bases",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/bases_subspaces.html#bases-coordinates-and-subspaces",
    "href": "worksheets/bases_subspaces.html#bases-coordinates-and-subspaces",
    "title": "Problems",
    "section": "",
    "text": "Problem 1\nLet \\(\\mathcal{B} = \\left\\{\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 2 \\end{bmatrix}\\right\\}\\) be a basis for \\(\\mathbb{R}^3\\).\n\nFind the coordinates of \\(\\mathbf{v} = (3,2,4)\\) with respect to \\(\\mathcal{B}\\). That is, find scalars \\(c_1, c_2, c_3\\) such that:\n\n\\[\\mathbf{v} = c_1(1,1,0) + c_2(-1,1,0) + c_3(0,0,2)\\]\n\nFind the coordinates of the following vectors with respect to \\(\\mathcal{B}\\) simultaneously. You can solve three systems of equations, but if you pay attention, you can solve one system that gives you the coordinates of the three vectors at once.\n\n\n\\(\\mathbf{v}_1 = (1,0,1)\\)\n\\(\\mathbf{v}_2 = (2,2,0)\\)\n\\(\\mathbf{v}_3 = (-1,1,3)\\)\n\n\n\nProblem 2\nGiven the basis \\(\\mathcal{B} = \\{(1,1,1), (1,1,-1), (1,-2,0)\\}\\) for \\(\\mathbb{R}^3\\):\n\nFind the coordinates of the standard basis vectors with respect to \\(\\mathcal{B}\\):\n\n\n\\(\\mathbf{e}_1 = (1,0,0)\\)\n\\(\\mathbf{e}_2 = (0,1,0)\\)\n\\(\\mathbf{e}_3 = (0,0,1)\\)\n\n\n\nProblem 3\nLet \\(\\mathcal{B} = \\{(2,0,1), (0,1,1), (1,1,0)\\}\\) be a basis for \\(\\mathbb{R}^3\\).\n\nFind the vector \\(\\mathbf{v}\\) whose coordinates with respect to \\(\\mathcal{B}\\): \\([v]_\\mathcal{B}=(2,-1,3)\\).\nIf a vector has coordinates \\((1,1,1)\\) with respect to \\(\\mathcal{B}\\), what is this vector in standard coordinates?\n\n\n\nProblem 4\nLet \\(S_1 =\n\\left\\{\n\\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix},\n\\begin{bmatrix}0\\\\2\\\\1\\end{bmatrix},\n\\begin{bmatrix}1\\\\1\\\\2\\end{bmatrix}\n\\right\\}\\) and \\(S_2\n=\n\\left\\{\n\\begin{bmatrix}2\\\\1\\\\0\\end{bmatrix},\n\\begin{bmatrix}1\\\\0\\\\2\\end{bmatrix},\n\\begin{bmatrix}3\\\\1\\\\1\\end{bmatrix}\n\\right\\}\\) be bases of \\(\\mathbb{R}^3\\).\n\nIf \\([v]_{S_1} = (2,-1,3)\\), find \\([v]_{S_2}\\).\n\nIf \\([v]_{S_1} = (1,2,0)\\), find \\([v]_{S_2}\\).\n\nIf \\([v]_{S_1} = (0,0,1)\\), find \\([v]_{S_2}\\).\n\n\n\nProblem 5\nLet \\(S_1 =\n\\left\\{\n\\begin{bmatrix}2\\\\1\\\\1\\end{bmatrix},\n\\begin{bmatrix}1\\\\2\\\\0\\end{bmatrix},\n\\begin{bmatrix}0\\\\1\\\\3\\end{bmatrix}\n\\right\\}\\) and \\(S_2 = \\left\\{\n\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix},\n\\begin{bmatrix}2\\\\0\\\\1\\end{bmatrix},\n\\begin{bmatrix}1\\\\2\\\\2\\end{bmatrix}\n\\right\\}\\) be bases of \\(\\mathbb{R}^3\\).\n\nIf \\([w]_{S_1} = (1,-1,2)\\), find \\([w]_{S_2}\\).\n\nIf \\([w]_{S_1} = (3,-0,0)\\), find \\([w]_{S_2}\\).\n\nIf \\([w]_{S_1} = (2,2,1)\\), find \\([w]_{S_2}\\).\n\n\n\nProblem 6\nFor the following matrices\n\n\nFind a basis for \\(\\text{col}(A)\\)\nFind a basis for \\(\\text{nul}(A)\\)\nWrite the columns of \\(A\\) that you didn’t include in the basis in terms of the ones you kept\n\n\n\n\\(A=\\begin{bmatrix}1&4&-1&3\\\\1&5&0&2\\\\0&3&3&1 \\end{bmatrix}\\)\n\\(A=\\begin{bmatrix}\n5 & 10 & 12 & 14 & 11 & 3\\\\\n-4 & -8 & -5 & -2 & -27 & -16\\\\\n2 & 4 & 6 & 8 & 0 & -2\n\\\\1 & 2 & 8 & 14 & -19 & -15\n\\end{bmatrix}\\)\n\n\n\nProblem 7\nFor each of the following sets,\n\n\nwrite the set as the span of some vectors,\nexpress the set as the column space of a matrix \\(A\\),\nfind a basis of the subspace and find the dimension.\n\n\nNotice that it follows from (1) or (2) that the set is a subspace\n\nThe set of all vectors of the form \\[\n\\left\\{ \\begin{bmatrix}\nx + 2y \\\\\ny - z \\\\\n2x - z\n\\end{bmatrix} : x,y,z \\in \\mathbb{R} \\right\\}\n\\]\nThe set of all vectors of the form \\[\n\\left\\{ \\begin{bmatrix}\n-3s + 6t - u + v - 7w \\\\\ns - 2t + 2u + 3v - w \\\\\n2s - 4t + 5u + 8v - 4w\n\\end{bmatrix} : s,t,u,v,w \\in \\mathbb{R} \\right\\}\n\\]\nThe set of all vectors of the form \\[\n\\left\\{ \\begin{bmatrix}\n-2s - 5t + 8u - 17w \\\\\ns + 3t - 5u + v + 5w \\\\\n3s + 11t - 19u + 7v + w \\\\\ns + 7t - 13u + 5v - 3w\n\\end{bmatrix} : s,t,u,v,w \\in \\mathbb{R} \\right\\}\n\\]",
    "crumbs": [
      "Subspaces and Bases",
      "Problems"
    ]
  }
]