[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "Introduction to Linear Algebra\nThese are the companion notes to the Winter 2025 Linear Algebra class. The course is designed to bridge theoretical understanding with practical applications, recognizing two fundamental aspects of linear algebra:\n\nThe geometric/visual interpretation of linear algebra concepts, helping students move beyond purely algorithmic approaches to understand systems as relationships between vectors and transformations that can be visualized.\nThe integration of computational tools to handle routine calculations, allowing focus on interpretation and real-world applications rather than manual computation.\n\nLinear algebra serves as the mathematical foundation for numerous real-world applications that shape our modern world. You’ll encounter it in:\n\nSciences & Engineering: Including quantum computing, computational biology, signal processing, robotics and autonomous systems\nComputing, Data & Statistics: Including machine learning, computer graphics, statistical analysis, and network modeling\nBusiness & Economics: Including market modeling, optimization, and equilibrium analysis\n\nYou’ll learn to view linear algebra through three lenses: theoretical foundations, geometric intuition, and practical applications - preparing you to apply these concepts across mathematics, sciences, engineering, and beyond.",
    "crumbs": [
      "Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "parts/vectors.html",
    "href": "parts/vectors.html",
    "title": "Vectors",
    "section": "",
    "text": "This section covers vectors.\n\nVectors in \\(\\mathbb{R}^n\\)\nAbstract Vector Spaces\nProblems: Vector Operations and Dot Products",
    "crumbs": [
      "Vectors"
    ]
  },
  {
    "objectID": "chapters/vectors.html",
    "href": "chapters/vectors.html",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "1.1 Addition of vectors and scalar product of vectors\nThe following videos from the Essence of Linear Algebra, from 3Blue1Brown, are exceptionally good. Watch them carefully\nLet \\(\\mathbf{v}=(v_1,\\dots,v_n)\\) and \\(\\mathbf{w}=(w_1,\\dots,w_n)\\) be two vectors in \\(\\mathbb{R}^n\\) and let \\(c\\in\\mathbb{R}\\) be a scalar. The sum of \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) is defined by: \\[\\mathbf{v}+\\mathbf{w}=\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots \\\\ v_n\\end{bmatrix}+\\begin{bmatrix}w_1\\\\w_2\\\\\\vdots\\\\ w_n\\end{bmatrix}=\\begin{bmatrix}v_1+w_1\\\\v_2+w_2\\\\\\vdots\\\\ v_n+w_n\\end{bmatrix}.\\]\nThe scalar product of \\(c\\) and \\(\\mathbf{v}\\) is defined by: \\[c\\mathbf{v}=c\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots\\\\ v_n\\end{bmatrix}=\\begin{bmatrix}cv_1\\\\cv_2\\\\\\vdots\\\\ cv_n\\end{bmatrix}.\\]\nIn \\(\\mathbb{R}^n\\), we represent vectors as column vectors. For convenience in inline text, we sometimes write \\(\\mathbf{v}=(v_1,\\dots,v_n)\\), but this notation should be understood to represent a column vector. This is distinct from a row vector, which we explicitly denote as \\(\\mathbf{v}=\\begin{bmatrix}v_1& v_2 &\\cdots &v_n\\end{bmatrix}\\). The distinction is important because row and column vectors behave differently under matrix operations.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vectors.html#addition-of-vectors-and-scalar-product-of-vectors",
    "href": "chapters/vectors.html#addition-of-vectors-and-scalar-product-of-vectors",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "Properties of addition and scalar product\nLet \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{w}\\) be vectors in \\(\\mathbb{R}^n\\), and let \\(c\\) and \\(d\\) be scalars.\n\n\\(\\mathbf{u} + \\mathbf{v} \\in \\mathbb{R}^n\\) (Closed under addition)\n\\(c\\mathbf{u} \\in \\mathbb{R}^n\\) (Closed under scalar multiplication)\n\\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\) (Commutative property of addition)\n\\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\) (Associative property of addition)\n\\(\\exists \\, \\mathbf{0} \\in \\mathbb{R}^n\\) such that \\(\\mathbf{u} + \\mathbf{0} = \\mathbf{u}\\) (Existence of an additive identity)\n\\(\\forall \\, \\mathbf{u} \\in \\mathbb{R}^n, \\, \\exists \\, -\\mathbf{u}\\) such that \\(\\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}\\) (Existence of additive inverses)\n\\(1\\mathbf{u} = \\mathbf{u}\\) (Identity element of scalar multiplication)\n\\((cd)\\mathbf{u} = c(d\\mathbf{u})\\) (Associative property of scalar multiplication)\n\\(c(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}\\) (Distributive property)\n\\((c + d)\\mathbf{u} = c\\mathbf{u} + d\\mathbf{u}\\) (Distributive property)\n\nThese properties characterize sets equipped with addition and scalar multiplication that satisfy the axioms of a vector space. In particular, they define the structure not only of \\(\\mathbb{R}^n\\) but also of more abstract vector spaces, where elements need not be geometric vectors, and scalars may belong to fields other than \\(\\mathbb{R}\\), such as \\(\\mathbb{C}\\) or finite fields.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vectors.html#linear-combinations",
    "href": "chapters/vectors.html#linear-combinations",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "1.2 Linear Combinations",
    "text": "1.2 Linear Combinations\nA linear combination of vectors \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\) in \\(\\mathbb{R}^n\\) is a sum of scalar multiples of these vectors:\n\\[a_1\\mathbf{v}_1 + a_2\\mathbf{v}_2 + \\cdots + a_k\\mathbf{v}_k\\]\nwhere \\(a_1, a_2, \\ldots, a_k\\) are scalars (real numbers).\nLet’s illustrate this with two vectors in \\(\\mathbb{R}^2\\): \\(\\mathbf{v}_1=\\begin{bmatrix}1\\\\1\\end{bmatrix}\\) and \\(\\mathbf{v}_2=\\begin{bmatrix}-1\\\\1\\end{bmatrix}\\). We can create different vectors through linear combinations of \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\):\n\nCombining with positive coefficients: \\[2\\mathbf{v}_1 + \\mathbf{v}_2 = 2\\begin{bmatrix}1\\\\1\\end{bmatrix} + \\begin{bmatrix}-1\\\\1\\end{bmatrix} = \\begin{bmatrix}1\\\\3\\end{bmatrix}\\]\nUsing a negative coefficient: \\[\\mathbf{v}_1 - 3\\mathbf{v}_2 = \\begin{bmatrix}1\\\\1\\end{bmatrix} - 3\\begin{bmatrix}-1\\\\1\\end{bmatrix} = \\begin{bmatrix}4\\\\-2\\end{bmatrix}\\]\nWorking with fractions: \\[\\frac{1}{2}\\mathbf{v}_1 + \\frac{1}{2}\\mathbf{v}_2 = \\frac{1}{2}\\begin{bmatrix}1\\\\1\\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix}-1\\\\1\\end{bmatrix} = \\begin{bmatrix}0\\\\1\\end{bmatrix}\\]\n\nMany questions in linear algebra reduce to solving systems of linear equations. Questions about linear combinations are a prime example, as we’ll see in the following exercise:\n\nExercise: Can we write \\(\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}\\) as a linear combination of the vectors \\(\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}\\), \\(\\begin{bmatrix}4\\\\5\\\\6\\end{bmatrix}\\), \\(\\begin{bmatrix}7\\\\8\\\\9\\end{bmatrix}\\)?\n\n\n\n\n\n\nClick to see the solution\n\n\n\n\n\nWe want scalars \\(c_1\\), \\(c_2\\), \\(c_3\\) such that: \\[c_1\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix} + c_2\\begin{bmatrix}4\\\\5\\\\6\\end{bmatrix} + c_3\\begin{bmatrix}7\\\\8\\\\9\\end{bmatrix} = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}\\] This gives the system of equations: \\[\\begin{align*}\nc_1 + 4c_2 + 7c_3 &= 0\\\\\n2c_1 + 5c_2 + 8c_3 &= 1\\\\\n3c_1 + 6c_2 + 9c_3 &= 0.\n\\end{align*}\\] We can use substitution, or elimination, to show that this system has no solution, so \\(\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}\\) is not a linear combination of the given vectors. We’ll cover solutions to such systems extensively later in the course.\n\n\n\n\nLinear combinations are fundamental in linear algebra and have numerous applications, such as:\n\nExpressing a vector in terms of other vectors\nSolving systems of linear equations\nDescribing lines, planes, and hyperplanes in \\(\\mathbb{R}^n\\)\nAnalyzing linear transformations and matrices",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vectors.html#span",
    "href": "chapters/vectors.html#span",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "1.3 Span",
    "text": "1.3 Span\nThe set of all possible linear combinations of a given set of vectors is known as the span of those vectors, and it has important properties.\nLet’s start with a precise definition. If \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\) are vectors in \\(\\mathbb{R}^n\\), then \\[\\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right) =\\{ c_1\\mathbf{v}_1+\\cdots+c_k\\mathbf{v}_k:c_1,\\dots,c_k\\in\\mathbb{R} \\}\\]\nNotice that \\(\\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\) is a subset of \\(\\mathbb{R}^n\\). When working with sets, we typically focus on two key questions:\n\nHow do we verify if an element belongs to the set?\nWhat properties can we deduce when we know an element belongs to the set?\n\nFor the span of vectors \\(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\):\nVerification: To check if \\(\\mathbf{v}\\) is in the span, we solve a system of equations. Consider:\nIf \\(\\mathbf{v}_1 = \\begin{bmatrix}1\\\\2\\\\1\\end{bmatrix}\\), \\(\\mathbf{v}_2 = \\begin{bmatrix}0\\\\1\\\\1\\end{bmatrix}\\), and \\(\\mathbf{v} = \\begin{bmatrix}2\\\\5\\\\3\\end{bmatrix}\\)\nTo check if \\(\\mathbf{v}\\) is in span\\((\\{\\mathbf{v}_1,\\mathbf{v}_2\\})\\), we ask: do there exist \\(c_1,c_2\\) such that \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 = \\mathbf{v}\\)?\nThis gives us: \\[\\begin{align*}\nc_1(1) + c_2(0) &= 2\\\\\nc_1(2) + c_2(1) &= 5\\\\\nc_1(1) + c_2(1) &= 3\n\\end{align*}\\]\nIf we find values for \\(c_1,c_2\\) satisfying all equations, then \\(\\mathbf{v}\\) is in the span. If no such values exist, \\(\\mathbf{v}\\) is not in the span.\nProperties: If a vector \\(\\mathbf{w}\\) is in \\(\\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\), we know that there exist constants \\(c_1,\\dots,c_n\\in\\mathbb{R}\\) such that \\(\\mathbf{w}=c_1\\mathbf{v}_1+\\cdots+c_k\\mathbf{v}_k\\).\nWe use these properties to deduce important results:\n\nTheorem 1.1 Suppose that \\(\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\) are vectors in \\(\\mathbb{R}^n\\). Then\n\nIf \\(\\mathbf{w}_1\\) and \\(\\mathbf{w}_2\\) are in \\(\\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\), then \\(\\mathbf{w}_1+\\mathbf{w}_2\\) is also in \\(\\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\).\nIf \\(\\mathbf{w}\\in \\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\), and \\(c\\in\\mathbb{R}\\), then \\(c\\mathbf{w}\\in \\text{span}\\left(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_k\\}\\right)\\).\n\n\n\nProof. We only check (1). The proof of (2) is similar. Since \\(\\mathbf{w}_1\\) is in the span, there exist \\(a_1,\\dots,a_k\\) with \\(\\mathbf{w}_1 = a_1\\mathbf{v}_1+\\cdots+a_k\\mathbf{v}_k\\). Similarly, there exist \\(b_1,\\dots,b_k\\) with \\(\\mathbf{w}_2 = b_1\\mathbf{v}_1+\\cdots+b_k\\mathbf{v}_k\\). Then: \\[\\mathbf{w}_1 + \\mathbf{w}_2 = (a_1+b_1)\\mathbf{v}_1+\\cdots+(a_k+b_k)\\mathbf{v}_k\\] showing \\(\\mathbf{w}_1 + \\mathbf{w}_2\\) is in the span. \\(\\square\\)\n\nVisualizing Vector Spans in \\(\\mathbb{R}^3\\)\n\nSpan of a Single Vector Given \\(\\mathbf{v} \\in \\mathbb{R}^3\\), the \\(\\text{span}({\\mathbf{v}})\\) is:\n\n\nA point at origin if \\(\\mathbf{v} = \\mathbf{0}\\)\nA line through the origin if \\(\\mathbf{v} \\neq \\mathbf{0}\\), containing all scalar multiples of \\(\\mathbf{v}\\)\n\n\nSpan of Two Vectors For nonzero vectors \\(\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^3\\), the \\(\\text{span}({\\mathbf{v}, \\mathbf{w}})\\) is:\n\n\nA line through origin if the vectors are parallel (one is a scalar multiple of the other)\nA plane through origin otherwise, containing all linear combinations \\(s\\mathbf{v} + t\\mathbf{w}\\) where \\(s,t \\in \\mathbb{R}\\)\n\n\nSpan of Multiple Vectors Consider the set of vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3, \\mathbf{v}_4\\}\\): \\[\n\\mathbf{v}_1 = \\begin{bmatrix}1\\\\1\\\\3\\end{bmatrix}, \\;\n\\mathbf{v}_2 = \\begin{bmatrix}-2\\\\-2\\\\-6\\end{bmatrix}, \\;\n\\mathbf{v}_3 = \\begin{bmatrix}1\\\\-2\\\\5\\end{bmatrix}, \\;\n\\mathbf{v}_4 = \\begin{bmatrix}0\\\\3\\\\-2\\end{bmatrix}\n\\]\n\nThe span of these vectors is the set of all possible linear combinations: \\[\\text{span}(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3, \\mathbf{v}_4\\}) = \\{t_1\\mathbf{v}_1 + t_2\\mathbf{v}_2 + t_3\\mathbf{v}_3 + t_4\\mathbf{v}_4 : t_1,t_2,t_3,t_4 \\in \\mathbb{R}\\}.\\]\nTheir span is visualized by the following graph and we see that all the vectors are in one plane.\n\n\n                                                \n\n\nThe span of a set of vectors in \\(\\mathbb{R}^3\\) must be one of exactly four geometric objects:\n\nA single point (specifically, the origin \\((0,0,0)\\))\nA line passing through the origin\nA plane containing the origin\nAll of \\(\\mathbb{R}^3\\) (the entire three-dimensional space)",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vectors.html#dot-product-in-mathbbrn",
    "href": "chapters/vectors.html#dot-product-in-mathbbrn",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "1.4 Dot Product in \\(\\mathbb{R}^n\\)",
    "text": "1.4 Dot Product in \\(\\mathbb{R}^n\\)\nLet \\(\\mathbf{v}=(v_1,\\dots,v_n)\\) and \\(\\mathbf{w}=(w_1,\\dots,w_n)\\) be two vectors in \\(\\mathbb{R}^n\\). The dot product of \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) is defined by: \\[\\mathbf{v}\\cdot\\mathbf{w}=\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots\\\\ v_n\\end{bmatrix}\\cdot \\begin{bmatrix}w_1\\\\w_2\\\\\\vdots\\\\ w_n\\end{bmatrix}= \\sum_{i=1}^nv_iw_i.\\]\n\n1.4.1 Properties of the Dot Product in \\(\\mathbb{R}^n\\)\nThe dot product has three fundamental properties that can be verified directly from its definition. These properties form the foundation for many calculations and proofs in linear algebra.\nLet \\(\\mathbf{u}\\), \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) be vectors in \\(\\mathbb{R}^n\\), and let \\(a\\in\\mathbb{R}\\) and \\(b\\in\\mathbb{R}\\) be scalars.\n\n\\(\\mathbf{v} \\cdot \\mathbf{v} \\geq 0\\) and \\(\\mathbf{v} \\cdot \\mathbf{v} = 0\\) if and only if \\(\\mathbf{v} = \\mathbf{0}\\) (Positive Definite)\n\\(\\mathbf{v} \\cdot \\mathbf{w} = \\mathbf{w} \\cdot \\mathbf{v}\\) (Symmetric)\n\\(\\mathbf{u}\\cdot(a\\mathbf{v} + b\\mathbf{w}) = a(\\mathbf{u} \\cdot \\mathbf{v}) + b(\\mathbf{u} \\cdot \\mathbf{w})\\) and \\((a\\mathbf{u} + b\\mathbf{v}) \\cdot \\mathbf{w} = a(\\mathbf{u} \\cdot \\mathbf{w}) + b(\\mathbf{v} \\cdot \\mathbf{w})\\) (Linear in each Variable)\n\nExercise: Suppose that \\(\\mathbf{v}_1,\\mathbf{v}_2,\\mathbf{w}_1,\\mathbf{w}_2\\in\\mathbb{R}^n\\) and that we know the values of \\(\\mathbf{v}_i\\cdot\\mathbf{w}_j\\). Find \\[(2\\mathbf{v}_1+3\\mathbf{v}_2)\\cdot(\\mathbf{w}_1-2\\mathbf{w}_2)\\] in terms of the \\(\\mathbf{v}_i\\cdot\\mathbf{w}_j\\)’s.\n\n\n\n\n\n\nClick to see the answer\n\n\n\n\n\n\nFirst, recall the key linearity properties:\n\nLinear in first variable: \\((c_1\\mathbf{a}_1 + c_2\\mathbf{a}_2)\\cdot\\mathbf{b} = c_1(\\mathbf{a}_1\\cdot\\mathbf{b}) + c_2(\\mathbf{a}_2\\cdot\\mathbf{b})\\)\nLinear in second variable: \\(\\mathbf{a}\\cdot(c_1\\mathbf{b}_1 + c_2\\mathbf{b}_2) = c_1(\\mathbf{a}\\cdot\\mathbf{b}_1) + c_2(\\mathbf{a}\\cdot\\mathbf{b}_2)\\)\n\nLet’s start with the first variable using linearity:\n\n\\((2\\mathbf{v}_1+3\\mathbf{v}_2)\\cdot(\\mathbf{w}_1-2\\mathbf{w}_2)\\)\n\\(= 2\\mathbf{v}_1\\cdot(\\mathbf{w}_1-2\\mathbf{w}_2) + 3\\mathbf{v}_2\\cdot(\\mathbf{w}_1-2\\mathbf{w}_2)\\)\n\nNow apply linearity in the second variable for each term:\n\n\\(= 2(\\mathbf{v}_1\\cdot\\mathbf{w}_1 - 2\\mathbf{v}_1\\cdot\\mathbf{w}_2) + 3(\\mathbf{v}_2\\cdot\\mathbf{w}_1 - 2\\mathbf{v}_2\\cdot\\mathbf{w}_2)\\)\n\nExpand this:\n\n\\(= 2\\mathbf{v}_1\\cdot\\mathbf{w}_1 - 4\\mathbf{v}_1\\cdot\\mathbf{w}_2 + 3\\mathbf{v}_2\\cdot\\mathbf{w}_1 - 6\\mathbf{v}_2\\cdot\\mathbf{w}_2\\)\n\n\nNow we can use the given values of \\(\\mathbf{v}_i\\cdot\\mathbf{w}_j\\) to compute the final result by substituting those values into this expression.\nThe final formula in terms of the known dot products is: \\[2(\\mathbf{v}_1\\cdot\\mathbf{w}_1) - 4(\\mathbf{v}_1\\cdot\\mathbf{w}_2) + 3(\\mathbf{v}_2\\cdot\\mathbf{w}_1) - 6(\\mathbf{v}_2\\cdot\\mathbf{w}_2).\\]\n\n\n\n\n\n1.4.2 Norm\nThe dot product induces a norm on \\(\\mathbb{R}^n\\). The norm of a vector \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\in\\mathbb{R}^n\\) is given by:\n\\[\\|\\mathbf{v}\\| = \\sqrt{\\mathbf{v}\\cdot\\mathbf{v}} = \\sqrt{\\sum_{i=1}^n |v_i|^2}\\]\nThe norm is also known as the magnitude or length of a vector. When we compute the norm or when we check properties, we often look at \\(\\|\\mathbf{v}\\|^2=\\mathbf{v}\\cdot\\mathbf{v}\\) to avoid the square root.\nThe norm satisfies the following properties:\n\nNon-negativity: \\(\\|\\mathbf{v}\\| \\geq 0\\) for all \\(\\mathbf{v} \\in \\mathbb{R}^n\\)\nDefiniteness: \\(\\|\\mathbf{v}\\| = 0\\) if and only if \\(\\mathbf{v} = \\mathbf{0}\\)\nHomogeneity: \\(\\|c\\mathbf{v}\\| = |c| \\|\\mathbf{v}\\|\\) for all \\(c \\in \\mathbb{R}\\) and \\(\\mathbf{v} \\in \\mathbb{R}^n\\)\nTriangle inequality: \\(\\|\\mathbf{v} + \\mathbf{w}\\| \\leq \\|\\mathbf{v}\\| + \\|\\mathbf{w}\\|\\) for all \\(\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^n\\)\n\nThe first three properties are easy to verify. The Triangle Inequality can be proved using Cauchy-Schwarz inequality that says that \\(|\\mathbf{v} \\cdot \\mathbf{w}| \\leq \\|\\mathbf{v}\\| \\|\\mathbf{w}\\|\\)\n\nExercise: Use Cauchy-Schwarz Inequality to prove the Triangle Inequality of the norm.\n\n\n\n\n\n\nClick to see the proof\n\n\n\n\n\nWe start with the squared norm of \\(\\mathbf{v} + \\mathbf{w}\\):\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 = (\\mathbf{v} + \\mathbf{w}) \\cdot (\\mathbf{v} + \\mathbf{w})\\]\nExpanding the right-hand side using the properties of the dot product, we get:\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 = \\mathbf{v} \\cdot \\mathbf{v} + 2(\\mathbf{v} \\cdot \\mathbf{w}) + \\mathbf{w} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\|^2 + 2(\\mathbf{v} \\cdot \\mathbf{w}) + \\|\\mathbf{w}\\|^2\\]\nNow, we apply the Cauchy-Schwarz Inequality to the term \\(2(\\mathbf{v} \\cdot \\mathbf{w})\\):\n\\[2(\\mathbf{v}\\cdot\\mathbf{w})\\leq2|\\mathbf{v} \\cdot \\mathbf{w}| \\leq 2\\|\\mathbf{v}\\| \\|\\mathbf{w}\\|\\]\nSubstituting this into the previous equation, we obtain:\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 \\leq \\|\\mathbf{v}\\|^2 + 2\\|\\mathbf{v}\\| \\|\\mathbf{w}\\| + \\|\\mathbf{w}\\|^2 = (\\|\\mathbf{v}\\| + \\|\\mathbf{w}\\|)^2\\]\nTaking the square root of both sides (which is valid since both sides are non-negative) yields:\n\\[\\|\\mathbf{v} + \\mathbf{w}\\| \\leq \\|\\mathbf{v}\\| + \\|\\mathbf{w}\\|\\]\nwhich is the Triangle Inequality for the norm in \\(\\mathbb{R}^n\\). \\(\\square\\)\n\n\n\n\n\n\n1.4.3 Orthogonality and Cauchy-Schwarz Inequality\nTwo vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) in \\(\\mathbb{R}^n\\) are orthogonal if \\(\\mathbf{v}\\cdot\\mathbf{w}=0\\). Orthogonality is a central topic in linear algebra and has numerous applications in various fields, such as:\n\nCoordinate systems and basis vectors\nLeast squares approximation and regression analysis\nFourier series and signal processing\nQuantum mechanics and Hilbert spaces\n\n\nTheorem 1.2 (Pythagorean Theorem) If \\(\\mathbf{v}, \\mathbf{w}\\in \\mathbb{R}^n\\) are orthogonal vectors, then \\(\\|\\mathbf{v} + \\mathbf{w}\\|^2= \\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2\\)\n\n\nProof. Let \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\) and \\(\\mathbf{w} = (w_1, w_2, \\ldots, w_n)\\) be orthogonal vectors in \\(\\mathbb{R}^n\\). We start with the squared norm of \\(\\mathbf{v} + \\mathbf{w}\\):\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 = (\\mathbf{v} + \\mathbf{w}) \\cdot (\\mathbf{v} + \\mathbf{w})\\]\nExpanding the right-hand side using the properties of the dot product, we get:\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 = \\mathbf{v} \\cdot \\mathbf{v} + 2(\\mathbf{v} \\cdot \\mathbf{w}) + \\mathbf{w} \\cdot \\mathbf{w}\\]\nSince \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) are orthogonal, \\(\\mathbf{v} \\cdot \\mathbf{w} = 0\\). Substituting this into the equation above, we obtain:\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 = \\mathbf{v} \\cdot \\mathbf{v} + \\mathbf{w} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2\\]\nwhich is the Pythagorean Theorem for orthogonal vectors in \\(\\mathbb{R}^n\\). \\(\\square\\)\n\nA similar result is the Parallelogram Law, that says that for any two vectors \\(\\mathbf{v}, \\mathbf{w}\\in \\mathbb{R}^n\\):\n\\[\\|\\mathbf{v} + \\mathbf{w}\\|^2 + \\|\\mathbf{v} - \\mathbf{w}\\|^2 = 2(\\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2)\\]\nWhen \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) are orthogonal, \\(\\|\\mathbf{v} + \\mathbf{w}\\|^2 = \\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2\\) and \\(\\|\\mathbf{v} - \\mathbf{w}\\|^2 = \\|\\mathbf{v}\\|^2 + \\|\\mathbf{w}\\|^2\\). Then the Parallelogram Law reduces to the Pythagorean Theorem.\n\nExercise: Prove the Parallelogram Law\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nExpand both squared norms using the dot product definition: \\(|\\mathbf{u}|^2 = \\mathbf{u} \\cdot \\mathbf{u}\\)\nFor the left side, you’ll get terms with \\(\\mathbf{v} \\cdot \\mathbf{v}\\), \\(\\mathbf{w} \\cdot \\mathbf{w}\\), and \\(\\mathbf{v} \\cdot \\mathbf{w}\\)\nPay attention to the signs of the cross terms \\(\\mathbf{v} \\cdot \\mathbf{w}\\) in both expansions\n\n\n\n\n\n\nTheorem 1.3 (Theorem: Cauchy-Schwarz Inequality) Let \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\) and \\(\\mathbf{w} = (w_1, w_2, \\ldots, w_n)\\) be two vectors in \\(\\mathbb{R}^n\\). Then \\(|\\mathbf{v} \\cdot \\mathbf{w}| \\leq \\|\\mathbf{v}\\| \\|\\mathbf{w}\\|\\)\n\n\nProof. Let \\(t \\in \\mathbb{R}\\) be a scalar. Consider the non-negative quantity \\(\\|\\mathbf{v} - t\\mathbf{w}\\|^2\\):\n\\[\\|\\mathbf{v} - t\\mathbf{w}\\|^2 \\geq 0\\]\nExpanding the left-hand side using the properties of the dot product, we get:\n\\[(\\mathbf{v} - t\\mathbf{w}) \\cdot (\\mathbf{v} - t\\mathbf{w}) = \\mathbf{v} \\cdot \\mathbf{v} - 2t(\\mathbf{v} \\cdot \\mathbf{w}) + t^2(\\mathbf{w} \\cdot \\mathbf{w}) \\geq 0\\]\nThis inequality holds for all values of \\(t\\). Let’s choose \\(t\\) to be the value that minimizes the left-hand side:\n\\[t = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{\\|\\mathbf{w}\\|^2}\\]\nSubstituting this value of \\(t\\) into the inequality, we obtain:\n\\[\\|\\mathbf{v}\\|^2 - 2\\frac{(\\mathbf{v} \\cdot \\mathbf{w})^2}{\\|\\mathbf{w}\\|^2} + \\frac{(\\mathbf{v} \\cdot \\mathbf{w})^2}{\\|\\mathbf{w}\\|^2} \\geq 0\\]\nSimplifying the left-hand side:\n\\[\\|\\mathbf{v}\\|^2 - \\frac{(\\mathbf{v} \\cdot \\mathbf{w})^2}{\\|\\mathbf{w}\\|^2} \\geq 0\\]\nMultiplying both sides by \\(\\|\\mathbf{w}\\|^2\\) yields:\n\\[\\|\\mathbf{v}\\|^2\\|\\mathbf{w}\\|^2 \\geq (\\mathbf{v} \\cdot \\mathbf{w})^2\\]\nTaking the square root of both sides (which is valid since both sides are non-negative) gives:\n\\[\\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\geq |\\mathbf{v} \\cdot \\mathbf{w}|\\]\nwhich is the Cauchy-Schwarz Inequality. \\(\\square\\)\n\n\n\n\n\n\n\nClick to see an alternative proof of Cauchy-Schawrz\n\n\n\n\n\nSuppose that neither \\(\\mathbf{v}\\) nor \\(\\mathbf{w}\\) are zero and that one is not a multiple of the other.\nFor any scalar \\(t\\in\\mathbb{R}\\), we can write \\(\\mathbf{w}\\) as the sum of two vectors: \\(\\mathbf{w} = t\\mathbf{v}+(\\mathbf{w}-t\\mathbf{v})\\). Our goal is to find \\(t\\in\\mathbb{R}\\) such that \\(t\\mathbf{v}\\) and \\(\\mathbf{w}-t\\mathbf{v}\\) are orthogonal. For such a \\(t\\), \\[\\|\\mathbf{w}\\|^2 = t^2\\|\\mathbf{v}\\|^2+\\|\\mathbf{w}-t\\mathbf{v}\\|^2.\\] In particular, \\[t^2\\|\\mathbf{v}\\|^2\\leq\\|\\mathbf{w}\\|^2.\\] From the equation \\(\\mathbf{v}\\cdot (\\mathbf{w}-t\\mathbf{v})=0\\), we find that \\(t=\\frac{\\mathbf{v}\\cdot \\mathbf{w}}{\\mathbf{v}\\cdot \\mathbf{v}}\\). When we substitute this into \\[t^2\\|\\mathbf{v}\\|^2\\leq\\|\\mathbf{w}\\|^2,\\] and simplify we get the Cauchy-Schwarz inequality.\n\n\n\n\n\n1.4.4 Geometric Interpretation of the Dot Product\nThe dot product of two vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) in \\(\\mathbb{R}^n\\) can be expressed in terms of their norms and the angle between them: \\[\\mathbf{v} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\cos(\\theta)\\] where \\(\\theta\\) is the angle between \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\). This relationship highlights the geometric interpretation of the dot product. When \\(\\theta = 0°\\), the vectors are parallel, and the dot product equals the product of their norms. When \\(\\theta = 90°\\), the vectors are orthogonal, and the dot product is zero. The Cauchy-Schwarz Inequality follows directly from this relationship, as \\(|\\cos(\\theta)| \\leq 1\\).\nWe can verify this easily in \\(\\mathbb{R}^2\\). Consider two vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) in \\(\\mathbb{R}^2\\). Let the angle that vector \\(\\mathbf{v}\\) makes with the positive x-axis be \\(\\alpha\\), and the angle that vector \\(\\mathbf{w}\\) makes with the positive x-axis be \\(\\alpha + \\beta\\), where \\(\\beta\\) is the angle between vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\).\nThe vectors can be expressed in terms of their magnitudes and angles: \\(\\mathbf{v} = (\\|\\mathbf{v}\\| \\cos(\\alpha), \\|\\mathbf{v}\\| \\sin(\\alpha))\\) and \\(\\mathbf{w} = (\\|\\mathbf{w}\\| \\cos(\\alpha + \\beta), \\|\\mathbf{w}\\| \\sin(\\alpha + \\beta))\\). The dot product of these vectors is:\n\\[\\mathbf{v} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| (\\cos(\\alpha) \\cos(\\alpha + \\beta) + \\sin(\\alpha) \\sin(\\alpha + \\beta))\\]\nUsing the angle addition formulas:\n\\[\\cos(\\alpha + \\beta) = \\cos(\\alpha) \\cos(\\beta) - \\sin(\\alpha) \\sin(\\beta)\\] \\[\\sin(\\alpha + \\beta) = \\sin(\\alpha) \\cos(\\beta) + \\cos(\\alpha) \\sin(\\beta)\\]\nWe show that \\(\\cos(\\alpha) \\cos(\\alpha + \\beta) + \\sin(\\alpha) \\sin(\\alpha + \\beta) = \\cos(\\beta)\\) and we conclude that \\[\\mathbf{v} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\cos(\\beta).\\]\n\nExercise: Prove that \\(\\cos(\\alpha) \\cos(\\alpha + \\beta) + \\sin(\\alpha) \\sin(\\alpha + \\beta) = \\cos(\\beta)\\)\n\n\n\n\n\n\nClick to see the proof\n\n\n\n\n\n\\[\\begin{aligned}\n\\cos(\\alpha) \\cos(\\alpha + \\beta) + \\sin(\\alpha) \\sin(\\alpha + \\beta) &=\n\\cos(\\alpha) (\\cos(\\alpha) \\cos(\\beta) - \\sin(\\alpha) \\sin(\\beta)) \\\\\n&\\quad + \\sin(\\alpha) (\\sin(\\alpha) \\cos(\\beta) + \\cos(\\alpha) \\sin(\\beta))) \\\\\n&= (\\cos^2(\\alpha) \\cos(\\beta) - \\cos(\\alpha) \\sin(\\alpha) \\sin(\\beta) \\\\\n&\\quad + \\sin^2(\\alpha) \\cos(\\beta) + \\cos(\\alpha) \\sin(\\alpha) \\sin(\\beta)) \\\\\n&= (\\cos^2(\\alpha) \\cos(\\beta) + \\sin^2(\\alpha) \\cos(\\beta)) \\\\\n&= (\\cos^2(\\alpha) + \\sin^2(\\alpha)) \\cos(\\beta) \\\\\n&= \\cos(\\beta)\n\\end{aligned}\\]\n\n\n\n\n\n\n1.4.5 Distance\nThe norm induces a distance (or metric) on \\(\\mathbb{R}^n\\), the distance between two vectors \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\) and \\(\\mathbf{w} = (w_1, w_2, \\ldots, w_n)\\) is given by:\n\\[d(\\mathbf{v}, \\mathbf{w}) =\\|\\mathbf{v}-\\mathbf{w}\\| = \\sqrt{\\sum_{i=1}^n |v_i - w_i|^2}\\]\nThis distance is known as the Euclidean distance. It satisfies the following properties:\n\nNon-negativity: \\(d(\\mathbf{v}, \\mathbf{w}) \\geq 0\\) for all \\(\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^n\\)\nDefiniteness: \\(d(\\mathbf{v}, \\mathbf{w}) = 0\\) if and only if \\(\\mathbf{v} = \\mathbf{w}\\)\nSymmetry: \\(d(\\mathbf{v}, \\mathbf{w}) = d(\\mathbf{w}, \\mathbf{v})\\) for all \\(\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^n\\)\nTriangle inequality: \\(d(\\mathbf{v}, \\mathbf{z}) \\leq d(\\mathbf{v}, \\mathbf{w}) + d(\\mathbf{w}, \\mathbf{z})\\) for all \\(\\mathbf{v}, \\mathbf{w}, \\mathbf{z} \\in \\mathbb{R}^n\\)\n\nThe induced distance has numerous applications in various fields, such as:\n\nClustering and classification in machine learning\nMeasuring similarity or dissimilarity between objects or data points\nOptimization problems in operations research\nError analysis and approximation theory in numerical analysis\n\nUnderstanding the relationships between dot product, norm, and distance is crucial in applications in mathematics, physics, computer science, and engineering.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vectors.html#vector-computation-in-python",
    "href": "chapters/vectors.html#vector-computation-in-python",
    "title": "1  Vectors in \\(\\mathbb{R}^n\\)",
    "section": "1.5 Vector Computation in Python",
    "text": "1.5 Vector Computation in Python\nPython provides various ways to work with vectors and mathematical computations. Here’s an overview of the main approaches:\n\nBuilt-in Python Lists:\n\nBasic vector operations require explicit formulas or list comprehensions\nUseful for understanding the underlying computations\nNot optimized for large-scale numerical calculations\n\nNumPy (Numerical Python):\n\nIndustry-standard library for numerical computing\nProvides efficient array operations and mathematical functions\nOptimized for performance with vectorized operations\nEssential for scientific computing and data analysis\n\nSymPy (Symbolic Python):\n\nComputer algebra system for symbolic mathematics\nHandles mathematical expressions with variables and symbols\nPerfect for mathematical proofs and algebraic manipulations\nUseful for verifying theoretical results\n\n\nIn this class, we will primarily use NumPy and Sympy. For solving linear systems, NumPy uses numerically stable methods like QR decomposition rather than Gaussian elimination. While Gaussian elimination is a foundational algorithm taught in linear algebra courses for its theoretical importance and intuitive approach, it can be numerically unstable in practice. SymPy provides a direct implementation of Gaussian elimination, making it useful for understanding the algorithm and verifying theoretical results.\n\n1.5.1 Representing Vectors\nIn Python, we can represent vectors using lists, in NumPy we use arrays, and in Sympy we use vectors, that are implemented with the function Matrix. Notice that Sympy shows vectors as column vectors.\n\nimport numpy as np\nfrom sympy import Matrix\n\n# Using Python lists\nv = [1, 2, 3]\nw = [4, 5, 6]\nprint(v)  # Output: [51,2,3]\n\n# Using NumPy arrays\nv_np = np.array([1, 2, 3])\nw_np = np.array([4, 5, 6])\nprint(v_np)  # Output: [1 2 3]\n\n# Using Sympy vectors\nv_sp = Matrix([1,2,3])\nw_sp = Matrix([4,5,6])\n\n[1, 2, 3]\n[1 2 3]\n\n\n\n# Showing v_sp\nv_sp\n\n\\(\\displaystyle \\left[\\begin{matrix}1\\\\2\\\\3\\end{matrix}\\right]\\)\n\n\n\n\n1.5.2 Vector Addition\nThe + is already implemented in Numpy and Sympy.\n\n# Using Python lists\nresult = [v[i] + w[i] for i in range(len(v))]\nprint(result)  # Output: [5, 7, 9]\n\n# Using NumPy arrays\nresult_np = v_np + w_np\nprint(result_np)  # Output: [5 7 9]\n\n[5, 7, 9]\n[5 7 9]\n\n\n\n# Using Sympy vectors\nv_sp + w_sp\n\n\\(\\displaystyle \\left[\\begin{matrix}5\\\\7\\\\9\\end{matrix}\\right]\\)\n\n\n\n\n1.5.3 Scalar Multiplication\nThe scalar multiplication is already implemented in Numpy and Sympy.\n\nscalar = 2\n\n# Using Python lists\nresult = [scalar * x for x in v]\nprint(result)  # Output: [2, 4, 6]\n\n# Using NumPy arrays\nresult_np = scalar * v_np\nprint(result_np)  # Output: [2 4 6]\n\n[2, 4, 6]\n[2 4 6]\n\n\n\n# Using Sympy vectors\nscalar * v_sp\n\n\\(\\displaystyle \\left[\\begin{matrix}2\\\\4\\\\6\\end{matrix}\\right]\\)\n\n\n\n\n1.5.4 Dot Product\nThe dot product is already implemented in Numpy and Sympy.\n\n# Using Python lists\ndot_product = sum([v[i] * w[i] for i in range(len(v))])\nprint(dot_product)  # Output: 32\n\n# Using NumPy arrays\ndot_product_np = np.dot(v_np, w_np)\nprint(dot_product_np)  # Output: 32\n\n# Using Sympy vectors\ndot_product_sp = v_sp.dot(w_sp)\nprint(dot_product_sp)  # Output: 32\n\n32\n32\n32\n\n\n\n\n1.5.5 Vector Norms\nThe norm is already implemented in Numpy and Sympy, but it is inside the linalg library of Numpy.\n\nimport math\n\n# Using Python lists\nnorm = math.sqrt(sum([x**2 for x in v]))\nprint(norm)  # Output: 3.7416573867739413\n\n# Using NumPy arrays\nnorm_np = np.linalg.norm(v_np)\nprint(norm_np)  # Output: 3.7416573867739413\n\n# Using Symoy vectors\nnorm_sp = v_sp.norm()\nprint(norm_sp)  # Output: sqrt(14) which is 3.7416573867739413\n\n3.7416573867739413\n3.7416573867739413\nsqrt(14)",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Vectors in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/vector_spaces.html",
    "href": "chapters/vector_spaces.html",
    "title": "2  Abstract Vector Spaces",
    "section": "",
    "text": "2.1 Definition\nThe step from \\(\\mathbb{R}^n\\) to abstract vector spaces reflects a fundamental principle in mathematics: identifying common patterns to unify seemingly different objects. While \\(\\mathbb{R}^n\\) provides a concrete and visualizable model, the abstract framework reveals that spaces of functions, polynomials, and solutions to some differential equations share the exact same algebraic structure. This abstraction is not just elegant—it’s immensely practical. When we prove a theorem about vector spaces in general, it automatically applies to all these examples at once.\nA vector space \\(V\\) over \\(\\mathbb{R}\\) is a set equipped with two operations: vector addition (\\(+\\)) and scalar multiplication (\\(\\cdot\\)). For \\(\\mathbf{u},\\mathbf{v},\\mathbf{w}\\in V\\) and scalars \\(a,b\\in\\mathbb{R}\\), these operations must satisfy:\nVector Addition Properties:\nScalar Multiplication Properties:\nExamples: Vector spaces appear naturally throughout mathematics and its applications. While we’ll encounter many examples throughout this course, here are a few fundamental ones to illustrate how diverse they can be.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abstract Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/vector_spaces.html#definition",
    "href": "chapters/vector_spaces.html#definition",
    "title": "2  Abstract Vector Spaces",
    "section": "",
    "text": "Commutativity: \\(\\mathbf{u}+\\mathbf{v}=\\mathbf{v}+\\mathbf{u}\\)\nAssociativity: \\((\\mathbf{u}+\\mathbf{v})+\\mathbf{w}=\\mathbf{u}+(\\mathbf{v}+\\mathbf{w})\\)\nZero vector: There exists \\(\\mathbf{0}\\in V\\) such that \\(\\mathbf{v}+\\mathbf{0}=\\mathbf{v}\\) for all \\(\\mathbf{v}\\in V\\)\nAdditive inverse: For each \\(\\mathbf{v}\\in V\\), there exists \\(-\\mathbf{v}\\in V\\) such that \\(\\mathbf{v}+(-\\mathbf{v})=\\mathbf{0}\\)\n\n\n\nDistributivity over vector addition: \\(a(\\mathbf{u}+\\mathbf{v})=a\\mathbf{u}+a\\mathbf{v}\\)\nDistributivity over scalar addition: \\((a+b)\\mathbf{v}=a\\mathbf{v}+b\\mathbf{v}\\)\nAssociativity: \\(a(b\\mathbf{v})=(ab)\\mathbf{v}\\)\nIdentity: \\(1\\mathbf{v}=\\mathbf{v}\\)\n\n\n\n\\(\\mathbb{R}^n\\): Our familiar space of n-tuples of real numbers with the standard addition and scalar multiplication.\nFunction spaces:\n\n\\(C[a,b]\\): Continuous functions on \\([a,b]\\)\n\nSum: \\((f + g)(x) = f(x) + g(x)\\)\nScalar multiplication: \\((cf)(x) = c\\cdot f(x)\\)\n\n\\(C^\\infty(\\mathbb{R})\\): Infinitely differentiable functions\n\nSum: \\((f + g)(x) = f(x) + g(x)\\)\nScalar multiplication: \\((cf)(x) = c\\cdot f(x)\\)\n\n\nPolynomial spaces:\n\n\\(\\mathbb{P}_n\\): Polynomials of degree \\(\\leq n\\)\n\nSum: \\((p + q)(x) = p(x) + q(x)\\)\nScalar multiplication: \\((cp)(x) = c\\cdot p(x)\\)\n\n\\(\\mathbb{P}\\): All polynomials\n\nSum: \\((p + q)(x) = p(x) + q(x)\\)\nScalar multiplication: \\((cp)(x) = c\\cdot p(x)\\)",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abstract Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/vector_spaces.html#subspaces-of-vector-spaces",
    "href": "chapters/vector_spaces.html#subspaces-of-vector-spaces",
    "title": "2  Abstract Vector Spaces",
    "section": "2.2 Subspaces of Vector Spaces",
    "text": "2.2 Subspaces of Vector Spaces\nWithin any vector space, certain subsets naturally inherit the vector space structure. These special subsets, called subspaces, play a fundamental role in linear algebra.\n\nDefinition: A subset \\(W\\) of the vector space \\(V\\) is called a subspace if it satisfies three conditions:\n\nThe zero vector \\(\\mathbf{0}\\) is in \\(W\\)\nFor all \\(\\mathbf{u},\\mathbf{v}\\in W\\), their sum \\(\\mathbf{u}+\\mathbf{v}\\) is also in \\(W\\) (closed under addition)\nFor all \\(\\mathbf{v}\\in W\\) and all scalars \\(c\\in\\mathbb{R}\\), the vector \\(c\\mathbf{v}\\) is in \\(W\\) (closed under scalar multiplication)\n\n\nThese conditions ensure that \\(W\\) inherits the vector space structure from \\(V\\), making it a vector space in its own right and providing us with a rich source of new examples.\n\nTheorem: Every subspace of a vector space \\(V\\) is itself a vector space.\n\n\nProof. Let \\(W\\) be a subspace of the vector space \\(V\\). We must verify all eight vector space properties.\nVector Addition Properties:\n\nCommutativity: Let \\(\\mathbf{u},\\mathbf{v}\\in W\\). Since \\(W\\subseteq V\\), we know \\(\\mathbf{u}+\\mathbf{v}=\\mathbf{v}+\\mathbf{u}\\), as this holds in \\(V\\).\nAssociativity: Let \\(\\mathbf{u},\\mathbf{v},\\mathbf{w}\\in W\\). Since \\(W\\subseteq V\\), we know that \\((\\mathbf{u}+\\mathbf{v})+\\mathbf{w}=\\mathbf{u}+(\\mathbf{v}+\\mathbf{w})\\), as this holds in \\(V\\).\nZero vector: This is given directly by subspace property 1.\nAdditive inverse: Let \\(\\mathbf{v}\\in W\\). By subspace property 3, \\((-1)\\mathbf{v}\\in W\\). This is the additive inverse of \\(\\mathbf{v}\\) since \\(\\mathbf{v}+(-1)\\mathbf{v}=1\\mathbf{v}+(-1)\\mathbf{v}=(1-1)\\mathbf{v}=0\\mathbf{v}=\\mathbf{0}\\).\n\nScalar Multiplication Properties:\n\nDistributivity over vector addition: Let \\(a\\in\\mathbb{R}\\) and \\(\\mathbf{u},\\mathbf{v}\\in W\\). We know that \\(a(\\mathbf{u}+\\mathbf{v})=a\\mathbf{u}+a\\mathbf{v}\\), as this holds in \\(V\\).\nDistributivity over scalar addition: Let \\(a,b\\in\\mathbb{R}\\) and \\(\\mathbf{v}\\in W\\). We know that \\((a+b)\\mathbf{v}=a\\mathbf{v}+b\\mathbf{v}\\), as this holds in \\(V\\).\nAssociativity of scalar multiplication: Let \\(a,b\\in\\mathbb{R}\\) and \\(\\mathbf{v}\\in W\\). We know that \\(a(b\\mathbf{v})=(ab)\\mathbf{v}\\), as this holds in \\(V\\).\nIdentity scalar multiplication: Let \\(\\mathbf{v}\\in W\\). We know that \\(1\\mathbf{v}=\\mathbf{v}\\), as this holds in \\(V\\), and clearly \\(\\mathbf{v}\\in W\\) by assumption.\n\nTherefore, since all eight vector space properties are satisfied, \\(W\\) is indeed a vector space.\n\nNote that this proof relies heavily on two key facts:\n\nThe vector space operations in \\(W\\) are inherited from \\(V\\)\nThe subspace properties ensure that these operations are well-defined on \\(W\\) (i.e., their outputs remain in \\(W\\))\n\n\n\n\n\n\n\nSubspaces of \\(\\mathbb{R}^n\\)\n\n\n\nSubspaces of \\(\\mathbb{R}^n\\) are fundamental structures that arise naturally in many applications of linear algebra. They have intuitive geometric interpretations that help us visualize abstract concepts:\n\nIn \\(\\mathbb{R}^2\\):\n\nA line through the origin\nThe entire plane \\(\\mathbb{R}^2\\) itself\nThe zero vector \\({\\vec{0}}\\)\n\nIn \\(\\mathbb{R}^3\\):\n\nA line through the origin\nA plane through the origin\nThe entire space \\(\\mathbb{R}^3\\)\nThe zero vector \\({\\mathbf{0}}\\)\n\n\nIn Theorem 1.1, we proved that the span of a set of vectors \\({\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_k}\\) in \\(\\mathbb{R}^n\\) is a subspace.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abstract Vector Spaces</span>"
    ]
  },
  {
    "objectID": "chapters/vector_spaces.html#inner-products",
    "href": "chapters/vector_spaces.html#inner-products",
    "title": "2  Abstract Vector Spaces",
    "section": "2.3 Inner Products:",
    "text": "2.3 Inner Products:\nAn inner product on a vector space \\(V\\) is a function \\(\\langle\\cdot,\\cdot\\rangle:V\\times V\\to\\mathbb{R}\\) satisfying:\n\nSymmetry: \\(\\langle\\mathbf{u},\\mathbf{v}\\rangle=\\langle\\mathbf{v},\\mathbf{u}\\rangle\\)\nLinearity: \\(\\langle a\\mathbf{u}+b\\mathbf{v},\\mathbf{w}\\rangle=a\\langle\\mathbf{u},\\mathbf{w}\\rangle+b\\langle\\mathbf{v},\\mathbf{w}\\rangle\\)\nPositive definiteness: \\(\\langle\\mathbf{v},\\mathbf{v}\\rangle\\geq 0\\) with equality if and only if \\(\\mathbf{v}=\\mathbf{0}\\)\n\nThe inner product generalizes the familiar dot product of \\(\\mathbb{R}^n\\). Different fields use different notations:\n\nIn \\(\\mathbb{R}^n\\): \\(\\mathbf{u}\\cdot\\mathbf{v}\\) (dot product notation)\nIn mathematics: \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle\\) (angle bracket notation)\nIn physics: \\(\\langle \\mathbf{u} | \\mathbf{v} \\rangle\\) (Dirac or bra-ket notation)\n\nExamples of Inner Products:\n\nStandard dot product in \\(\\mathbb{R}^n\\): \\(\\langle\\mathbf{x},\\mathbf{y}\\rangle=\\sum_{i=1}^n x_iy_i\\)\nOn \\(C[a,b]\\): \\(\\langle f,g\\rangle=\\int_a^b f(x)g(x)\\,dx\\)\nOn \\(\\mathbb{P}_n\\): \\(\\langle p,q\\rangle=\\int_{-1}^1 p(x)q(x)\\,dx\\)\n\nJust as in \\(\\mathbb{R}^n\\), these inner products satisfy fundamental properties that make them powerful tools. Every inner product generates a norm through \\(\\|\\mathbf{v}\\|=\\sqrt{\\langle\\mathbf{v},\\mathbf{v}\\rangle}\\), which in turn induces a distance function \\(d(\\mathbf{u},\\mathbf{v})=\\|\\mathbf{u}-\\mathbf{v}\\|\\). The Cauchy-Schwarz inequality holds in any inner product space: \\(|\\langle\\mathbf{u},\\mathbf{v}\\rangle|\\leq\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|\\). And the norm satisfies all the properties we know from \\(\\mathbb{R}^n\\): positivity, homogeneity, and the triangle inequality. Thus, every inner product space inherits the geometric structure that makes \\(\\mathbb{R}^n\\) so useful.",
    "crumbs": [
      "Vectors",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abstract Vector Spaces</span>"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html",
    "href": "worksheets/computing_dot_products.html",
    "title": "Problems",
    "section": "",
    "text": "Vector Operations and Dot Products",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-1",
    "href": "worksheets/computing_dot_products.html#problem-1",
    "title": "Problems",
    "section": "Problem 1",
    "text": "Problem 1\nConsider the following vectors:\n\n\n\n\n\n\n\n\n\n\nFind the coordinates of \\(\\mathbf{u}\\), \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\)\nFind \\(\\mathbf{u} + \\mathbf{v} + \\mathbf{w}\\) (does the answer make geometric sense?)\nFind \\(\\mathbf{u} - \\mathbf{v} - \\mathbf{w}\\) (does the answer make geometric sense?)\nNormalize the vectors \\(\\mathbf{u}\\), \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\). That is, find a vector in the same direction with norm equal to one.\nCheck that \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) satisfy Cauchy-Schwartz inequality.",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-2",
    "href": "worksheets/computing_dot_products.html#problem-2",
    "title": "Problems",
    "section": "Problem 2",
    "text": "Problem 2\nLet \\(\\mathbf{v}\\), \\(\\mathbf{w}\\), \\(\\mathbf{z} \\in \\mathbb{R}^n\\). Suppose that \\(\\mathbf{v} \\cdot \\mathbf{w} = 2\\), \\(\\mathbf{v} \\cdot \\mathbf{z} = -1\\), \\(\\mathbf{z} \\cdot \\mathbf{w} = 1\\), \\(\\|\\mathbf{v}\\| = \\sqrt{3}\\), \\(\\|\\mathbf{w}\\| = 2\\), and \\(\\|\\mathbf{z}\\| = \\sqrt{5}\\). Find the following:\n\n\\((2\\mathbf{v} + 3\\mathbf{w}) \\cdot \\mathbf{z}\\)\n\\(\\mathbf{v} \\cdot (\\mathbf{v} - 2\\mathbf{w} + 3\\mathbf{z})\\)\n\\((3\\mathbf{v} - 4\\mathbf{z}) \\cdot (\\mathbf{w} + 5\\mathbf{z})\\)\n\\(\\|\\mathbf{v} + \\mathbf{w}\\|\\)\n\\(\\|\\mathbf{v} - \\mathbf{w}\\|\\)\n\\(\\|2\\mathbf{v} - 6\\mathbf{z}\\|\\)\nFind the angle between \\(\\mathbf{v}\\) and \\(\\mathbf{z}\\)\nFind \\(c \\in \\mathbb{R}\\) such that \\(\\mathbf{v}\\) is orthogonal to \\(\\mathbf{w} + c\\mathbf{z}\\)\nFind \\(c \\in \\mathbb{R}\\) such that \\(\\mathbf{w} - c\\mathbf{v}\\) is orthogonal to \\(\\mathbf{v}\\)",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-3",
    "href": "worksheets/computing_dot_products.html#problem-3",
    "title": "Problems",
    "section": "Problem 3",
    "text": "Problem 3\nLet \\(\\mathbf{v}\\), \\(\\mathbf{w} \\in \\mathbb{R}^n\\). Suppose that \\(\\|\\mathbf{v}\\| = 2\\), \\(\\|\\mathbf{v} + \\mathbf{w}\\| = \\sqrt{3}\\) and \\(\\|\\mathbf{w}\\| = \\sqrt{2}\\). Find \\(\\mathbf{v} \\cdot \\mathbf{w}\\).",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-4",
    "href": "worksheets/computing_dot_products.html#problem-4",
    "title": "Problems",
    "section": "Problem 4",
    "text": "Problem 4\nLet \\(\\mathbf{v}\\), \\(\\mathbf{w} \\in \\mathbb{R}^n\\). Suppose that \\(\\|\\mathbf{v}\\| = \\sqrt{2}\\), \\(\\|\\mathbf{v} - \\mathbf{w}\\| = \\sqrt{3}\\) and \\(\\mathbf{v} \\cdot \\mathbf{w} = \\frac{1}{2}\\). Find \\(\\|\\mathbf{w}\\|\\).",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-5",
    "href": "worksheets/computing_dot_products.html#problem-5",
    "title": "Problems",
    "section": "Problem 5",
    "text": "Problem 5\nSuppose that \\(\\mathbf{u} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\), \\(\\mathbf{v} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\\) and \\(\\mathbf{w} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix}\\)\n\nFind a constant \\(c \\in \\mathbb{R}\\) such that \\(\\mathbf{v} - c\\mathbf{u}\\) is orthogonal to \\(\\mathbf{u}\\). Name this vector \\(\\mathbf{f_2} = \\mathbf{v} - c\\mathbf{u}\\).\nFind constants \\(c, d \\in \\mathbb{R}\\) such that \\(\\mathbf{w} - c\\mathbf{u} - d\\mathbf{f_2}\\) is orthogonal to \\(\\mathbf{u}\\) and orthogonal to \\(\\mathbf{f_2}\\). Name this vector \\(\\mathbf{f_3} = \\mathbf{w} - c\\mathbf{u} - d\\mathbf{f_2}\\).\nFind constants \\(c, d, e \\in \\mathbb{R}\\) such that the vectors \\(c\\mathbf{u}\\), \\(d\\mathbf{f_2}\\), and \\(e\\mathbf{f_3}\\) have norm one. Rename these vectors \\(\\mathbf{e_1} = c\\mathbf{u_1}\\), \\(\\mathbf{e_2} = d\\mathbf{f_2}\\), and \\(\\mathbf{e_3} = e\\mathbf{f_3}\\) and write them explicitly.",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/computing_dot_products.html#problem-6",
    "href": "worksheets/computing_dot_products.html#problem-6",
    "title": "Problems",
    "section": "Problem 6",
    "text": "Problem 6\nLet \\(\\mathbf{v_1}\\), \\(\\mathbf{v_2}\\), \\(\\mathbf{v_3}\\), \\(\\mathbf{v_4}\\) be vectors satisfying:\n\\(\\mathbf{v_i} \\cdot \\mathbf{v_j} = \\begin{cases} 1 & \\text{if } i = j \\\\ 0 & \\text{otherwise} \\end{cases}\\)\n\nFind \\((2\\mathbf{v_1} - 3\\mathbf{v_2}) \\cdot (2\\mathbf{v_3} + 4\\mathbf{v_4})\\)\nFind \\((\\mathbf{v_1} + \\mathbf{v_2}) \\cdot (\\mathbf{v_1} - \\mathbf{v_2})\\)\nFind \\(\\|\\mathbf{v_4}\\|\\)\nFind \\(\\|4\\mathbf{v_1} - 3\\mathbf{v_2}\\|\\)\nFind \\(\\|2\\mathbf{v_1} - 3\\mathbf{v_2} + 4\\mathbf{v_1} - 5\\mathbf{v_2}\\|\\)",
    "crumbs": [
      "Vectors",
      "Problems"
    ]
  },
  {
    "objectID": "parts/matrices.html",
    "href": "parts/matrices.html",
    "title": "Matrices",
    "section": "",
    "text": "This section covers matrices.\n\nMatrices\nProblems: Matrix Multiplication\nApplication: Image Transformation with Matrices",
    "crumbs": [
      "Matrices"
    ]
  },
  {
    "objectID": "chapters/matrices.html",
    "href": "chapters/matrices.html",
    "title": "3  Matrices",
    "section": "",
    "text": "3.1 Matrices: Definition and Different Perspectives\nThe following video from the Essence of Linear Algebra, from 3Blue1Brown, is exceptionally good. Watch it carefully\nA matrix is a rectangular array of numbers arranged in rows and columns. Formally, a matrix \\(A\\) with \\(m\\) rows and \\(n\\) columns is written as:\n\\[\nA = \\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m,1} & a_{m,2} & \\cdots & a_{m,n}\n\\end{bmatrix}.\n\\] The matrix \\(A\\) is said to have dimension \\(m\\times n\\). The entry \\(a_{i,j}\\) represents the element at the \\(i\\)-th row and \\(j\\)-th column. This entry is also sometimes denoted as \\(A_{i,j}\\) or \\((A)_{i,j}\\).\nFor example, consider the following matrix:\n\\[M=\\begin{bmatrix}8&6&0&6\\\\-4&-8&2&-7\\\\-8&4&-5&3\\end{bmatrix}\\]\nThis matrix \\(M\\) has 3 rows and 4 columns. We can interpret and view this matrix in several ways:",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrices-definition-and-different-perspectives",
    "href": "chapters/matrices.html#matrices-definition-and-different-perspectives",
    "title": "3  Matrices",
    "section": "",
    "text": "As an array of numbers: We can see \\(M\\) as a collection of numbers arranged in a rectangular grid with 3 rows and 4 columns. Each entry in the matrix is identified by its row and column index. For example, \\(M_{2,3}\\) or \\((M)_{2,3}\\), the entry in the second row and third column is 2.\nAs a collection of column vectors: We can view \\(M\\) as having 4 column vectors, each with 3 elements. The columns of \\(M\\) are:\n\\[\\left [ \\begin{bmatrix}8\\\\-4\\\\-8\\end{bmatrix}, \\begin{bmatrix}6\\\\-8\\\\4\\end{bmatrix}, \\begin{bmatrix}0\\\\2\\\\-5\\end{bmatrix}, \\begin{bmatrix}6\\\\-7\\\\3\\end{bmatrix}\\right ]\\]\nEach column vector can be treated as a separate entity, and matrix operations can be performed on these columns.\nAs a collection of row vectors: We can view \\(M\\) as having 3 row vectors, each with 4 elements. The rows of \\(M\\) are:\n\\[\\biggl [ \\begin{bmatrix}8&6&0&6\\end{bmatrix}, \\begin{bmatrix}-4&-8&2&-7\\end{bmatrix}, \\begin{bmatrix}-8&4&-5&3\\end{bmatrix}\\biggr ]\\]\nEach row vector can be treated as a separate entity, and matrix operations can be performed on these rows.",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#addition-and-scalar-multiplication-of-matrices",
    "href": "chapters/matrices.html#addition-and-scalar-multiplication-of-matrices",
    "title": "3  Matrices",
    "section": "3.2 Addition and Scalar Multiplication of Matrices",
    "text": "3.2 Addition and Scalar Multiplication of Matrices\nGiven two matrices \\(A\\) and \\(B\\) of the same size \\(m \\times n\\), the sum of \\(A\\) and \\(B\\), denoted as \\(A + B\\), is a new matrix \\(C\\) of size \\(m \\times n\\) where each element \\(c_{i,j}\\) is the sum of the corresponding elements \\(a_{i,j}\\) and \\(b_{i,j}\\) from matrices \\(A\\) and \\(B\\), respectively. In other words: \\[c_{i,j} = a_{i,j} + b_{i,j}\\] for all \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\). For example, if \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\\), then: \\[A + B = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 1+5 & 2+6 \\\\ 3+7 & 4+8 \\end{bmatrix} = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix}\\] The scalar multiplication of a matrix \\(A\\) by a scalar \\(k\\), denoted as \\(kA\\), is a new matrix \\(B\\) of the same size as \\(A\\), where each element \\(b_{i,j}\\) is the product of the scalar \\(k\\) and the corresponding element \\(a_{i,j}\\) from matrix \\(A\\). In other words: \\[b_{i,j} = k \\cdot a_{i,j}\\] for all \\(1 \\leq i \\leq m\\) and \\(1 \\leq j \\leq n\\). For example, if \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) and \\(k = 2\\), then: \\[2A = 2 \\cdot \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 2 \\cdot 1 & 2 \\cdot 2 \\\\ 2 \\cdot 3 & 2 \\cdot 4 \\end{bmatrix} = \\begin{bmatrix} 2 & 4 \\\\ 6 & 8 \\end{bmatrix}\\]\n\n3.2.1 Properties of addition and scalar product\nLet \\(A\\), \\(B\\), and \\(C\\) be \\(m\\times n\\) matrices and let \\(c\\) and \\(d\\) be scalars. Then we can esily check that\n\nCommutativity of addition: \\(A + B = B + A\\)\nAssociativity of addition: \\((A + B) + C = A + (B + C)\\)\nExistence of zero matrix: There exists a matrix \\(O\\) such that \\(A + O = A\\) for all matrices \\(A\\)\nExistence of additive inverse: For every matrix \\(A\\), there exists a matrix \\(-A\\) such that \\(A + (-A) = O\\)\nDistributivity of scalar multiplication over matrix addition: \\(k(A + B) = kA + kB\\)\nDistributivity of scalar multiplication over field addition: \\((k + l)A = kA + lA\\)\nAssociativity of scalar multiplication: \\((kl)A = k(lA)\\)\nExistence of multiplicative identity: \\(1A = A\\) for all matrices \\(A\\)\n\nSince we have already shown that if \\(A\\) and \\(B\\) are \\(m\\times n\\) matrices and \\(k\\) is a scalar, then \\(A+B\\) and \\(kA\\) are also \\(m\\times n\\) matrices, we can conclude that the set of all \\(m\\times n\\) matrices forms a vector space. This set is commonly denoted using various notations, including: \\(M_{m\\times n}\\), \\(\\mathbb{M}_{m\\times n}\\), and \\(\\mathbb{R}^{m\\times n}\\).",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#the-transpose-of-a-matrix",
    "href": "chapters/matrices.html#the-transpose-of-a-matrix",
    "title": "3  Matrices",
    "section": "3.3 The Transpose of a Matrix",
    "text": "3.3 The Transpose of a Matrix\nGiven a matrix \\(A\\) with \\(m\\) rows and \\(n\\) columns, denoted as: \\[\nA = \\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m,1} & a_{m,2} & \\cdots & a_{m,n}\n\\end{bmatrix},\n\\] the transpose of matrix \\(A\\), denoted as \\(A^T\\) or \\(A'\\), is obtained by interchanging the rows and columns of \\(A\\). In other words, the first row of \\(A\\) becomes the first column of \\(A^T\\), the second row of \\(A\\) becomes the second column of \\(A^T\\), and so on. The resulting matrix \\(A^T\\) has \\(n\\) rows and \\(m\\) columns:\n\\[\nA^T = \\begin{bmatrix}\na_{1,1} & a_{2,1} & \\cdots & a_{m,1}\\\\\na_{1,2} & a_{2,2} & \\cdots & a_{m,2}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\na_{1,n} & a_{2,n} & \\cdots & a_{m,n}.\n\\end{bmatrix}\n\\]\nWhen \\(A\\) is represented by columns or by rows, we can easily determine the form of the transpose. \\[\n\\text{If}\\quad A = \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix},\\text{ then }\\quad\nA^T=\\begin{bmatrix}\n\\leftarrow & \\mathbf{c}_1^T &\\rightarrow \\\\\n\\leftarrow & \\mathbf{c}_2^T &\\rightarrow\\\\\n\\vdots & \\vdots &\\vdots\\\\\n\\leftarrow & \\mathbf{c}_n^T &\\rightarrow\n\\end{bmatrix},\n\\] and \\[\n\\text{if}\\quad A =\n\\begin{bmatrix}\n\\leftarrow & \\mathbf{r}_1 &\\rightarrow \\\\\n\\leftarrow & \\mathbf{r}_2 &\\rightarrow\\\\\n\\vdots & \\vdots &\\vdots\\\\\n\\leftarrow & \\mathbf{r}_m &\\rightarrow\n\\end{bmatrix}, \\text{ then }\\quad\nA^T = \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{r}_1^T & \\mathbf{r}_2^T & \\cdots & \\mathbf{r}_n^T \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}.\n\\]\nNotice that if we take the transpose twice returns the original matrix. In other words, \\[(A^T)^T=A.\\] To check properties of the transpose we use the definition \\((A^T)_{i,j}=A_{j,i}\\).\n\nExercise: Suppose that \\(A\\) and \\(B\\) are \\(m\\times n\\) matrices and that \\(c\\in\\mathbb{R}\\). Prove that \\((A+B)^T=A^T+B^T\\) and that \\((cA)^T=cA^T\\).\n\n\n\n\n\n\nClick to see a sketch of the proof\n\n\n\n\n\n\\[((A+B)^T)_{i,j}=(A+B)_{j,i}=A_{j,i}+B_{j,i}=(A^T)_{i,j}+(B^T)_{i,j}=(A^T+B^T)_{i,j}.\\] The other one is similar",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrix-vector-multiplication",
    "href": "chapters/matrices.html#matrix-vector-multiplication",
    "title": "3  Matrices",
    "section": "3.4 Matrix-Vector Multiplication",
    "text": "3.4 Matrix-Vector Multiplication\nWe now cover one of the most important operations in linear algebra: multiplying a matrix with a vector. Let \\(A\\) be a an \\(m\\times n\\) matrix and \\(\\mathbf{x}\\in\\mathbb{R}^n\\). The product \\(A\\mathbf{x}\\) will be a vector in \\(\\mathbb{R}^m\\). This operation is crucial because it allows us to:\n\nTransform vectors in space (like rotations, reflections, or scaling)\nSolve systems of linear equations in a compact way\nApply linear transformations in computer graphics, data science, and physics\n\nWe look at the \\(m\\times n\\) matrix \\(A\\) in three ways: \\[A = \\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m,1} & a_{m,2} & \\cdots & a_{m,n}\n\\end{bmatrix}\n=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\n=\\begin{bmatrix}\n\\leftarrow & \\mathbf{r}_1 &\\rightarrow \\\\\n\\leftarrow & \\mathbf{r}_2 &\\rightarrow \\\\\n\\vdots & \\vdots &\\vdots\\\\\n\\leftarrow & \\mathbf{r}_m &\\rightarrow\n\\end{bmatrix}.\\]\n\n3.4.1 A Linear Combination of Columns\nWe define the product \\(A\\mathbf{x}\\) as a linear combination of the columns of \\(A\\): \\[\nA\\mathbf{x} =\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\n\\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_n \\end{bmatrix}\n= x_1\\mathbf{c}_1+x_2\\mathbf{c_2}+\\cdots+x_n\\mathbf{c}_n.\n\\tag{3.1}\\]\nExample: Let \\(A=\\begin{bmatrix} 1 & 2 & 0 \\\\ -1 & 3 & 4 \\end{bmatrix}\\) be a \\(2\\times 3\\) matrix and \\(\\mathbf{x}=\\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix}\\) be a vector in \\(\\mathbb{R}^3\\). Then \\[\n\\begin{aligned}\nA\\mathbf{x} &=\n\\begin{bmatrix} 1 & 2 & 0 \\\\ -1 & 3 & 4 \\end{bmatrix}\n\\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix} \\\\\n&=2\\begin{bmatrix}1\\\\-1 \\end{bmatrix}\n+ (-1)\\begin{bmatrix}2\\\\3 \\end{bmatrix}\n+3\\begin{bmatrix}0\\\\4 \\end{bmatrix}\n=\\begin{bmatrix}0\\\\6 \\end{bmatrix}\n\\end{aligned}\n\\]\nEquation (Equation 3.1) is one of the most useful formulas.\n\nIt allows us to write matrix multiplications as linear combinations,\nIt allows us write linear combinations as matrix multiplication.\n\n\n\n3.4.2 The Component-wise Formula\nFrom the definition given by (Equation 3.1), we can write \\(A\\mathbf{x}\\) in terms of the \\(a_{i,j}\\)’s: \\[\\begin{aligned}\nA\\mathbf{x} &=\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\na_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m,1} & a_{m,2} & \\cdots & a_{m,n}\n\\end{bmatrix}\n\\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix} \\\\\n&= x_1\\begin{bmatrix} a_{1,1}\\\\ a_{2,1}\\\\ \\vdots\\\\ a_{m,1} \\end{bmatrix}\n+ x_2\\begin{bmatrix} a_{1,2}\\\\ a_{2,2}\\\\ \\vdots\\\\ a_{m,2} \\end{bmatrix} +\\cdots\n+ x_n\\begin{bmatrix} a_{1,n}\\\\ a_{2,n}\\\\ \\vdots\\\\ a_{m,n} \\end{bmatrix} \\\\\n% &= \\begin{bmatrix} a_{1,1}x_1\\\\ a_{2,1}x_1\\\\ \\vdots\\\\ a_{m,1}x_1 \\end{bmatrix}\n% + \\begin{bmatrix} a_{1,2}x_2\\\\ a_{2,2}x_2\\\\ \\vdots\\\\ a_{m,2}x_2 \\end{bmatrix} +\\cdots\n% + \\begin{bmatrix} a_{1,n}x_n\\\\ a_{2,n}x_n\\\\ \\vdots\\\\ a_{m,n}x_n \\end{bmatrix} \\\\\nA\\mathbf{x} &= \\begin{bmatrix} a_{1,1}x_1+a_{1,2}x_2+\\cdots+a_{1,n}x_n\\\\ a_{2,1}x_1+a_{2,2}x_2+\\cdots+a_{2,n}x_n\\\\ \\vdots\\\\ a_{m,1}x_1+a_{m,2}x_2+\\cdots+a_{m,n}x_n\\\\ \\end{bmatrix}.\n\\end{aligned}\n\\]\nTherefore, the \\(i\\)-th component of \\(A\\mathbf{x}\\) is: \\[(A\\mathbf{x})_i=a_{i,1}x_1+a_{i,2}x_2+\\cdots+a_{i,n}x_n \\tag{3.2}\\]\n\n\n3.4.3 The Row Dot Product Formula\nFrom (Equation 3.2) we see that the \\(i\\)-th term of \\(A\\mathbf{x}\\) can be written as a dot product: \\[\n(A\\mathbf{x})_i=a_{i,1}x_1+a_{i,2}x_2+\\cdots+a_{i,n}x_n\n=\\begin{bmatrix} a_{i,1}\\\\ a_{i,2}\\\\\\vdots \\\\ a_{i,n}\\end{bmatrix}\\cdot\n\\begin{bmatrix} x_1\\\\ x_2\\\\\\vdots \\\\ x_n\\end{bmatrix}\n=\\mathbf{r}_i^T\\cdot\\mathbf{x}.\\] Recall that vectors in \\(\\mathbb{R}^n\\) are represented by column vectors, and that the first vector is the transpose of the \\(i\\)-th row of \\(A\\). Putting all the compunents togther, we get: \\[\nA\\mathbf{x}=\\begin{bmatrix}\n\\leftarrow & \\mathbf{r}_1 &\\rightarrow \\\\\n\\leftarrow & \\mathbf{r}_2 &\\rightarrow \\\\\n\\vdots & \\vdots &\\vdots\\\\\n\\leftarrow & \\mathbf{r}_m &\\rightarrow \\end{bmatrix}\n\\begin{bmatrix} x_1\\\\ x_2\\\\\\vdots \\\\ x_n\\end{bmatrix}\n=\\begin{bmatrix}\n\\mathbf{r}_1^T\\cdot\\mathbf{x} \\\\\n\\mathbf{r}_2^T\\cdot\\mathbf{x}\\\\ \\vdots \\\\\n\\mathbf{r}_n^T\\cdot\\mathbf{x}\\end{bmatrix}.\n\\]",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrix-matrix-multiplication",
    "href": "chapters/matrices.html#matrix-matrix-multiplication",
    "title": "3  Matrices",
    "section": "3.5 Matrix-Matrix Multiplication",
    "text": "3.5 Matrix-Matrix Multiplication\nMatrix-matrix multiplication, like matrix-vector multiplication, requires compatibility between the dimensions of the matrices involved. For the product \\(AB\\) to be defined, the number of columns in \\(A\\) must equal the number of rows in \\(B\\). When this condition is met, the matrices are said to be compatible for multiplication. Specifically, if \\(A\\) is an \\(m \\times n\\) matrix and \\(B\\) is an \\(n \\times p\\) matrix, their product \\(AB\\) will be an \\(m \\times p\\) matrix.\n\n3.5.1 The Product \\(AB\\): \\(A\\) Acts on the Columns of \\(B\\)\nLet \\(A\\) be an \\(m \\times n\\) matrix and \\(B\\) an \\(n \\times p\\) matrix. Since the number of columns in \\(A\\) matches the number of rows in \\(B\\), the matrices are compatible for multiplication. To define the product \\(AB\\), we express \\(B\\) in terms of its column vectors and let \\(A\\) act on each column individually. Specifically,\n\\[\nAB = A \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{b}_1 & \\mathbf{b}_2 & \\cdots & \\mathbf{b}_p \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\nA\\mathbf{b}_1 & A\\mathbf{b}_2 & \\cdots & A\\mathbf{b}_p \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}.\n\\]\nNotice that \\(AB\\) consists of \\(p\\) columns, where each column \\(A\\mathbf{c}_i \\in \\mathbb{R}^m\\). Therefore, \\(AB\\) is an \\(m \\times p\\) matrix.\n\n\n3.5.2 The Component Formula\nWe use the previous formula to compute the individual entries of \\(AB\\). Consider \\((AB)_{i,j}\\). This is the element of \\(AB\\) in the \\(i\\)-th row and \\(j\\)-th column. Since the \\(j\\)-th column of \\(AB\\) is \\(A\\mathbf{b}_j\\), it follows from (Equation 3.2) that \\((AB)_{i,j}=(A\\mathbf{b}_j)_i= \\sum_{k=1}^na_{i,k}b_{k,j}.\\) Then we have \\[(AB)_{i,j}=\\sum_{k=1}^na_{i,k}b_{k,j}. \\tag{3.3}\\]\n\n\n3.5.3 The Row-Column Dot Product Formula\nFrom (Equation 3.3), it follows that \\((AB)_{i,j}\\) is the dot product of the \\(i\\)-th row of \\(A\\) and the \\(j\\)-th column of \\(B\\). Writing \\(A\\) in terms of its rows and \\(B\\) in terms of its columns, we have:\n\\[\nAB =\n\\begin{bmatrix}\n\\leftarrow & \\mathbf{r}_1 & \\rightarrow \\\\\n\\leftarrow & \\mathbf{r}_2 & \\rightarrow \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\leftarrow & \\mathbf{r}_m & \\rightarrow\n\\end{bmatrix}\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{b}_1 & \\mathbf{b}_2 & \\cdots & \\mathbf{b}_p \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\mathbf{r}_1^T \\cdot \\mathbf{b}_1 & \\mathbf{r}_1^T \\cdot \\mathbf{b}_2 & \\cdots & \\mathbf{r}_1^T \\cdot \\mathbf{b}_p \\\\  \n\\mathbf{r}_2^T \\cdot \\mathbf{b}_1 & \\mathbf{r}_2^T \\cdot \\mathbf{b}_2 & \\cdots & \\mathbf{r}_2^T \\cdot \\mathbf{b}_p \\\\  \n\\vdots & \\vdots & \\ddots & \\vdots \\\\  \n\\mathbf{r}_m^T \\cdot \\mathbf{b}_1 & \\mathbf{r}_m^T \\cdot \\mathbf{b}_2 & \\cdots & \\mathbf{r}_m^T \\cdot \\mathbf{b}_p  \n\\end{bmatrix}.\n\\]\nHere, \\(\\mathbf{r}_i\\) represents the \\(i\\)-th row of \\(A\\), and \\(\\mathbf{b}_j\\) represents the \\(j\\)-th column of \\(B\\). Each entry of \\(AB\\), denoted \\((AB)_{i,j}\\), is the dot product \\(\\mathbf{r}_i^T \\cdot \\mathbf{b}_j\\).",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#the-transpose-of-the-product",
    "href": "chapters/matrices.html#the-transpose-of-the-product",
    "title": "3  Matrices",
    "section": "3.6 The Transpose of the Product",
    "text": "3.6 The Transpose of the Product\nAn important property of matrix multiplication is that the transpose of a product is the product of the transposes in reverse order. This relation is fundamental in many areas of linear algebra, from proving theoretical results about linear transformations to solving practical problems in optimization and data analysis.\n\n3.6.0.1 Theorem\nLet \\(A\\) be an \\(m \\times n\\) matrix and \\(B\\) an \\(n \\times p\\) matrix. Then:\n\\[(AB)^T = B^T A^T.\\]\n\n\nProof. The product \\(AB\\) is an \\(m \\times p\\) matrix, so its transpose \\((AB)^T\\) is a \\(p \\times m\\) matrix. Similarly, \\(B^T\\) is a \\(p \\times n\\) matrix, and \\(A^T\\) is an \\(n \\times m\\) matrix. Thus, the product \\(B^T A^T\\) also has dimensions \\(p \\times m\\), matching those of \\((AB)^T\\).\nTo prove the equality, we verify that the entries of \\((AB)^T\\) and \\(B^T A^T\\) are identical. Consider the \\((i, j)\\)-th entry of \\((AB)^T\\):\n\\[( (AB)^T )_{i,j} = (AB)_{j,i}.\\]\nUsing the definition of matrix multiplication, we expand \\((AB)_{j,i}\\):\n\\[(AB)_{j,i} = \\sum_{k=1}^n A_{j,k} B_{k,i}.\\]\nNext, observe that \\((B^T)_{i,k} = B_{k,i}\\) and \\((A^T)_{k,j} = A_{j,k}\\). Substituting these into the sum, we get:\n\\[(AB)_{j,i} = \\sum_{k=1}^n (B^T)_{i,k} (A^T)_{k,j}.\\]\nTherefore,\n\\[((AB)^T)_{i,j} = (B^T A^T)_{i,j}.\\]\nSince the \\((i, j)\\)-th entries of \\((AB)^T\\) and \\(B^T A^T\\) are equal for all \\(i\\) and \\(j\\), we conclude that:\n\\[(AB)^T = B^T A^T.\\]\n\n\n\n\n\n\n\nClick to see a proof that uses the row column dot product formula\n\n\n\n\n\nWrite \\(A\\) and \\(B\\) in terms of their rows and columns \\[A=\\begin{bmatrix}\n\\leftarrow & \\mathbf{r}_1 & \\rightarrow \\\\\n\\leftarrow & \\mathbf{r}_2 & \\rightarrow \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\leftarrow & \\mathbf{r}_m & \\rightarrow\n\\end{bmatrix} \\quad\\quad\nB=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{b}_1 & \\mathbf{b}_2 & \\cdots & \\mathbf{b}_p \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\\] Then \\[B^T = \\begin{bmatrix} \\leftarrow &\\mathbf{b}_1^T&\\rightarrow \\\\ \\leftarrow& \\mathbf{b}_2^T&\\rightarrow \\\\ \\vdots &\\vdots&\\vdots \\\\\\leftarrow & \\mathbf{b}_p^T &\\rightarrow \\end{bmatrix}\\quad\\quad\nA^T =\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{r}_1^T & \\mathbf{r}_2^T & \\cdots & \\mathbf{r}_m^T \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}.\\] Therefore it follows from the row column dot product formula that \\[(B^TA^T)_{i,j} = (\\mathbf{b}_i^T)^T\\cdot\\mathbf{r}_j^T=\\mathbf{b}_i\\cdot\\mathbf{r}_j^T\n=\\mathbf{r}_j^T\\cdot\\mathbf{b}_i=(AB)_{j,i}=((AB)^T)_{i,j}.\\]",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrix-types-and-the-inverse",
    "href": "chapters/matrices.html#matrix-types-and-the-inverse",
    "title": "3  Matrices",
    "section": "3.7 Matrix Types and the Inverse",
    "text": "3.7 Matrix Types and the Inverse\nMatrices come in various types, each with unique properties that make them fundamental to linear algebra and its applications. Among these, the following play a central role.\n\nSquare Matrices\nA square matrix is a matrix that has an equal number of rows and columns. The collection of all \\(n \\times n\\) square matrices is denoted by \\(M_n\\). This set is closed under several operations: if \\(A, B \\in M_n\\), their product \\(AB\\) also belongs to \\(M_n\\). Similarly, any power of \\(A\\), such as \\(A^k\\) for a positive integer \\(k\\), remains in \\(M_n\\), as does the transpose of \\(A\\).\n\n\nDiagonal Matrices\nA diagonal matrix is a square matrix in which all off-diagonal entries are zero. Formally, a matrix \\(D \\in M_n\\) is diagonal if \\((D)_{i,j} = 0\\) for all \\(i \\neq j\\). The only potentially nonzero entries are located along the main diagonal, from the top-left to the bottom-right. Diagonal matrices are significant because they are easy to work with: addition, multiplication, and finding powers are straightforward operations when the matrices are diagonal.\n\n\nUpper and Lower Triangular Matrices\nAn upper triangular matrix is a square matrix in which all entries below the main diagonal are zero, meaning \\((U)_{i,j} = 0\\) for all \\(i &gt; j\\). Similarly, a lower triangular matrix has all entries above the main diagonal equal to zero, i.e., \\((L)_{i,j} = 0\\) for all \\(i &lt; j\\). These matrices are commonly used in matrix factorizations, and solving systems of linear equations efficiently. Both types are particularly important in numerical methods, as their structure reduces computational complexity in many algorithms.\n\n\nSymmetric Matrices\nA square matrix is symmetric if it is equal to its transpose, meaning \\(A^T=T\\) or, equivalently, \\(A_{i,j}=A_{j,i}\\) for all \\(i,j\\). Symmetric matrices play a crucial role in various fields due to their significant orthogonal properties and frequent appearance in science and engineering applications.\n\n\nIdentity Matrices\nAn identity matrix is a special type of diagonal matrix where all the diagonal entries are 1, and all off-diagonal entries are 0. It is denoted as \\(I_n\\) for an \\(n \\times n\\) matrix. Formally, \\((I_n)_{i,j} = 1\\) if \\(i = j\\) and \\((I_n)_{i,j} = 0\\) if \\(i \\neq j\\).\nThe identity matrix serves as the multiplicative identity in matrix multiplication. Specifically, if \\(A\\) is an \\(n \\times p\\) matrix, then \\(I_n A = A\\). Similarly, if \\(B\\) is an \\(m \\times n\\) matrix, then \\(B I_n = B\\).\n\n\nInverse of a Matrix\nThe inverse of a matrix is a concept that applies to square matrices. A square matrix \\(A\\) is said to be invertible (or nonsingular) if there exists another matrix \\(A^{-1}\\) such that:\n\\[A A^{-1} = A^{-1} A = I_n,\\]\nwhere \\(I_n\\) is the identity matrix. The matrix \\(A^{-1}\\) is called the inverse of \\(A\\).\nNot all square matrices have an inverse. In practical applications, matrix inverses are used to solve systems of linear equations, analyze transformations, and compute solutions in various scientific and engineering contexts. However, for large matrices, explicit inversion is computationally expensive, and alternative methods, such as iterative techniques, are often preferred.",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#elementary-matrices-row-operations",
    "href": "chapters/matrices.html#elementary-matrices-row-operations",
    "title": "3  Matrices",
    "section": "3.8 Elementary Matrices: Row Operations",
    "text": "3.8 Elementary Matrices: Row Operations\nElementary matrices are special square matrices that perform row operations through matrix multiplication. They play an important role in linear algebra, particularly in solving systems of linear equations, characterizing invertible matrices, and understanding and computing determinants. There are three types:\n\nType 1: Switching two rows\nType 2: Multiplying a row by a non-zero constant\nType 3: Adding a multiple of a row to another row\n\nLet’s illustrate each type with 3×3 elementary matrices acting on a generic 3×5 matrix:\nLet A be a 3×5 matrix: \\(A = \\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix}\\)\n\n3.8.1 Example 1: Interchanging Rows 1 and 2\n\\(\\begin{aligned}\nE_1A &=\n\\begin{bmatrix}\n0 & 1 & 0\\\\\n1 & 0 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix} \\\\\n&=\\begin{bmatrix}\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix}\n\\end{aligned}\\)\n\n\n3.8.2 Example 2: Multiplying Row 3 by 2\n\\(\\begin{aligned}\nE_2A &=\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 2\n\\end{bmatrix}\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix} \\\\\n&=\\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\n2a_{3,1} & 2a_{3,2} & 2a_{3,3} & 2a_{3,4} & 2a_{3,5}\n\\end{bmatrix}\n\\end{aligned}\\)\n\n\n3.8.3 Example 3: Adding 3 Times Row 1 to Row 2\n\\(\\begin{aligned}\nE_3A &=\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n3 & 1 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} & a_{2,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix} \\\\\n&=\\begin{bmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} & a_{1,5}\\\\\n3a_{1,1}+a_{2,1} & 3a_{1,2}+a_{2,2} & 3a_{1,3}+a_{2,3} & 3a_{1,4}+a_{2,4} & 3a_{1,5}+a_{2,5}\\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} & a_{3,5}\n\\end{bmatrix}\n\\end{aligned}\\)\nNote that each elementary matrix is invertible, and its inverse performs the opposite operation:\n\nFor \\(E_1\\): its own inverse (swapping the same rows again)\nFor \\(E_2\\): multiply the third row by 1/2\nFor \\(E_3\\): subtract 3 times row 1 from row 2\n\nWe saw in (Equation 3.1) that when we multiply a matrix \\(A\\) by a vector \\(\\mathbf{x}\\), the product \\(A\\mathbf{x}\\) is a linear combination of the columns of \\(A\\). Similarly, when we multiply by a row vector \\(\\mathbf{z}\\) from the left, the product \\(\\mathbf{z}A\\) is a linear combination of the rows of \\(A\\). This fundamental principle helps us understand elementary matrices: when we multiply a matrix \\(A\\) by an elementary matrix \\(E\\) on the left, each row of the product \\(EA\\) is a linear combination of the rows of \\(A\\), precisely implementing our desired row operation.",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrices-in-numpy",
    "href": "chapters/matrices.html#matrices-in-numpy",
    "title": "3  Matrices",
    "section": "3.9 Matrices in Numpy",
    "text": "3.9 Matrices in Numpy\nThis section covers fundamental matrix operations using NumPy’s ndarray class. We’ll explore creation, indexing, and basic mathematical operations.\n\n3.9.1 Setup\nFirst, let’s import NumPy:\n\nimport numpy as np\n\n\n\n3.9.2 Creating Matrices\nNumPy provides several ways to create matrices using ndarrays:\n\n# From a list of lists\nA = np.array([[1, 2, 3],\n              [4, 5, 6]])\nprint(\"Matrix A:\")\nprint(A)\n\n# Using special functions\nzeros = np.zeros((2, 3))    # 2x3 matrix of zeros\nones = np.ones((3, 3))      # 3x3 matrix of ones\neye = np.eye(3)             # 3x3 identity matrix\n\nprint(\"\\nZeros matrix:\")\nprint(zeros)\nprint(\"\\nIdentity matrix:\")\nprint(eye)\n\nMatrix A:\n[[1 2 3]\n [4 5 6]]\n\nZeros matrix:\n[[0. 0. 0.]\n [0. 0. 0.]]\n\nIdentity matrix:\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\n\n\n3.9.3 Matrix Properties and Shape\nThe shape attribute tells us the dimensions of the matrix:\n\nA = np.array([[1, 2, 3],\n              [4, 5, 6]])\nprint(f\"Shape: {A.shape}\")\nprint(f\"Number of dimensions: {A.ndim}\")\nprint(f\"Size: {A.size}\")\n\nShape: (2, 3)\nNumber of dimensions: 2\nSize: 6\n\n\n\n\n3.9.4 Indexing and Slicing\nNumPy provides powerful ways to access matrix elements:\n\nA = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\n# Individual elements\nprint(\"First element:\", A[0, 0])\nprint(\"Second row, third column:\", A[1, 2])\n\n# Extracting rows\nprint(\"\\nFirst row:\", A[0, :])\nprint(\"Second row:\", A[1])  # : is implicit\n\n# Extracting columns\nprint(\"\\nFirst column:\", A[:, 0])\nprint(\"Second column:\", A[:, 1])\n\n# Slicing\nprint(\"\\nSubmatrix (first two rows, second and third columns):\")\nprint(A[0:2, 1:3])\n\nFirst element: 1\nSecond row, third column: 6\n\nFirst row: [1 2 3]\nSecond row: [4 5 6]\n\nFirst column: [1 4 7]\nSecond column: [2 5 8]\n\nSubmatrix (first two rows, second and third columns):\n[[2 3]\n [5 6]]\n\n\n\n\n3.9.5 Basic Operations\n\n3.9.5.1 Addition and Subtraction\nMatrix addition and subtraction work element-wise:\n\nA = np.array([[1, 2],\n              [3, 4]])\nB = np.array([[5, 6],\n              [7, 8]])\n\nprint(\"Matrix A:\")\nprint(A)\nprint(\"\\nMatrix B:\")\nprint(B)\nprint(\"\\nA + B:\")\nprint(A + B)\nprint(\"\\nA - B:\")\nprint(A - B)\n\nMatrix A:\n[[1 2]\n [3 4]]\n\nMatrix B:\n[[5 6]\n [7 8]]\n\nA + B:\n[[ 6  8]\n [10 12]]\n\nA - B:\n[[-4 -4]\n [-4 -4]]\n\n\n\n\n3.9.5.2 Scalar Operations\nMultiply or divide a matrix by a scalar:\n\nA = np.array([[1, 2],\n              [3, 4]])\n\nprint(\"Original matrix:\")\nprint(A)\nprint(\"\\nMultiply by 2:\")\nprint(2 * A)\nprint(\"\\nDivide by 2:\")\nprint(A / 2)\n\nOriginal matrix:\n[[1 2]\n [3 4]]\n\nMultiply by 2:\n[[2 4]\n [6 8]]\n\nDivide by 2:\n[[0.5 1. ]\n [1.5 2. ]]\n\n\n\n\n3.9.5.3 Matrix Multiplication\nNumPy provides several ways to perform matrix multiplication:\n\nA = np.array([[1, 2],\n              [3, 4]])\nB = np.array([[5, 6],\n              [7, 8]])\n\nprint(\"Matrix multiplication (A @ B):\")\nprint(A @ B)          # Preferred method (Python 3.5+)\n\nprint(\"\\nElement-wise multiplication (A * B):\")\nprint(A * B)          # Hadamard product\n\nMatrix multiplication (A @ B):\n[[19 22]\n [43 50]]\n\nElement-wise multiplication (A * B):\n[[ 5 12]\n [21 32]]\n\n\n\n\n\n3.9.6 Common Matrix Operations\nHere are some frequently used matrix operations:\n\nA = np.array([[1, 2],\n              [3, 4]])\n\nprint(\"Original matrix:\")\nprint(A)\n\nprint(\"\\nTranspose:\")\nprint(A.T)\n\nprint(\"\\nMatrix trace:\")\nprint(np.trace(A))\n\nprint(\"\\nMatrix determinant:\")\nprint(np.linalg.det(A))\n\nprint(\"\\nMatrix inverse:\")\nprint(np.linalg.inv(A))\n\nOriginal matrix:\n[[1 2]\n [3 4]]\n\nTranspose:\n[[1 3]\n [2 4]]\n\nMatrix trace:\n5\n\nMatrix determinant:\n-2.0000000000000004\n\nMatrix inverse:\n[[-2.   1. ]\n [ 1.5 -0.5]]\n\n\n\n\n3.9.7 Important Notes\n\nAlways check matrix dimensions when performing operations\nUse the appropriate multiplication operator:\n\n@ or np.matmul() for matrix multiplication\n* for element-wise multiplication\n\nRemember that indexing starts at 0, not 1\nWhen extracting rows or columns:\n\nA single row: A[i] or A[i, :]\nA single column: A[:, j]",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/matrices.html#matrices-in-sympy",
    "href": "chapters/matrices.html#matrices-in-sympy",
    "title": "3  Matrices",
    "section": "3.10 Matrices in Sympy",
    "text": "3.10 Matrices in Sympy\nThis section covers fundamental matrix operations using SymPy’s Matrix class. We’ll explore creation, indexing, and both numeric and symbolic operations.\n\n3.10.1 Setup\nFirst, let’s import SymPy and set up symbolic variables:\n\nfrom sympy import Matrix, Symbol, init_printing, pprint\nimport sympy as sp\n\n# Setup pretty printing\ninit_printing()\n\n# Define some symbolic variables\nx = Symbol('x')\ny = Symbol('y')\n\n\n\n3.10.2 Creating Matrices\nSymPy provides several ways to create matrices:\n\n# From a list of lists\nA = Matrix([[1, 2, 3],\n            [4, 5, 6]])\nprint(\"Matrix A:\")\npprint(A)\n\n# Using special constructors\nzeros = Matrix.zeros(2, 3)    # 2x3 matrix of zeros\nones = Matrix.ones(3, 3)      # 3x3 matrix of ones\neye = Matrix.eye(3)           # 3x3 identity matrix\n\nprint(\"\\nZeros matrix:\")\npprint(zeros)\nprint(\"\\nIdentity matrix:\")\npprint(eye)\n\n# Symbolic matrix\nsymbolic = Matrix([[x, y],\n                  [y, x]])\nprint(\"\\nSymbolic matrix:\")\npprint(symbolic)\n\nMatrix A:\n⎡1  2  3⎤\n⎢       ⎥\n⎣4  5  6⎦\n\nZeros matrix:\n⎡0  0  0⎤\n⎢       ⎥\n⎣0  0  0⎦\n\nIdentity matrix:\n⎡1  0  0⎤\n⎢       ⎥\n⎢0  1  0⎥\n⎢       ⎥\n⎣0  0  1⎦\n\nSymbolic matrix:\n⎡x  y⎤\n⎢    ⎥\n⎣y  x⎦\n\n\n\n\n3.10.3 Matrix Properties and Shape\nSymPy matrices have several useful properties:\n\nA = Matrix([[1, 2, 3],\n            [4, 5, 6]])\nprint(f\"Shape: {A.shape}\")\nprint(f\"Number of rows: {A.rows}\")\nprint(f\"Number of columns: {A.cols}\")\n\nShape: (2, 3)\nNumber of rows: 2\nNumber of columns: 3\n\n\n\n\n3.10.4 Indexing and Slicing\nSymPy uses different indexing methods than NumPy:\n\nA = Matrix([[1, 2, 3],\n            [4, 5, 6],\n            [7, 8, 9]])\n\n# Individual elements (zero-based indexing)\nprint(\"First element:\", A[0, 0])\nprint(\"Second row, third column:\", A[1, 2])\n\n# Extracting rows\nprint(\"\\nFirst row:\")\npprint(A.row(0))\nprint(\"\\nSecond row:\")\npprint(A.row(1))\n\n# Extracting columns\nprint(\"\\nFirst column:\")\npprint(A.col(0))\nprint(\"\\nSecond column:\")\npprint(A.col(1))\n\n# Extracting submatrices\nprint(\"\\nSubmatrix:\")\npprint(A[0:2, 1:3])\n\nFirst element: 1\nSecond row, third column: 6\n\nFirst row:\n[1  2  3]\n\nSecond row:\n[4  5  6]\n\nFirst column:\n⎡1⎤\n⎢ ⎥\n⎢4⎥\n⎢ ⎥\n⎣7⎦\n\nSecond column:\n⎡2⎤\n⎢ ⎥\n⎢5⎥\n⎢ ⎥\n⎣8⎦\n\nSubmatrix:\n⎡2  3⎤\n⎢    ⎥\n⎣5  6⎦\n\n\n\n\n3.10.5 Basic Operations\n\n3.10.5.1 Addition and Subtraction\nMatrix addition and subtraction work both with numeric and symbolic matrices:\n\nA = Matrix([[1, 2],\n            [3, 4]])\nB = Matrix([[5, 6],\n            [7, 8]])\n\nprint(\"Matrix A:\")\npprint(A)\nprint(\"\\nMatrix B:\")\npprint(B)\nprint(\"\\nA + B:\")\npprint(A + B)\nprint(\"\\nA - B:\")\npprint(A - B)\n\n# Symbolic example\nC = Matrix([[x, y],\n            [y, x]])\nprint(\"\\nSymbolic addition A + C:\")\npprint(A + C)\n\nMatrix A:\n⎡1  2⎤\n⎢    ⎥\n⎣3  4⎦\n\nMatrix B:\n⎡5  6⎤\n⎢    ⎥\n⎣7  8⎦\n\nA + B:\n⎡6   8 ⎤\n⎢      ⎥\n⎣10  12⎦\n\nA - B:\n⎡-4  -4⎤\n⎢      ⎥\n⎣-4  -4⎦\n\nSymbolic addition A + C:\n⎡x + 1  y + 2⎤\n⎢            ⎥\n⎣y + 3  x + 4⎦\n\n\n\n\n3.10.5.2 Scalar Operations\nMultiply or divide a matrix by a scalar (numeric or symbolic):\n\nA = Matrix([[1, 2],\n            [3, 4]])\n\nprint(\"Original matrix:\")\npprint(A)\nprint(\"\\nMultiply by 2:\")\npprint(2 * A)\nprint(\"\\nMultiply by symbolic x:\")\npprint(x * A)\n\nOriginal matrix:\n⎡1  2⎤\n⎢    ⎥\n⎣3  4⎦\n\nMultiply by 2:\n⎡2  4⎤\n⎢    ⎥\n⎣6  8⎦\n\nMultiply by symbolic x:\n⎡ x   2⋅x⎤\n⎢        ⎥\n⎣3⋅x  4⋅x⎦\n\n\n\n\n3.10.5.3 Matrix Multiplication\nSymPy matrix multiplication works with both numeric and symbolic matrices:\n\nA = Matrix([[1, 2],\n            [3, 4]])\nB = Matrix([[5, 6],\n            [7, 8]])\n\nprint(\"Matrix multiplication (A * B):\")\npprint(A * B)\n\nMatrix multiplication (A * B):\n⎡19  22⎤\n⎢      ⎥\n⎣43  50⎦\n\n\n\n\n\n3.10.6 Other Matrix Operations\nSymPy provides powerful symbolic matrix operations:\n\nA = Matrix([[1, 2],\n            [3, 4]])\n\nprint(\"Original matrix:\")\npprint(A)\n\nprint(\"\\nTranspose:\")\npprint(A.transpose())\n\nprint(\"\\nMatrix trace:\")\npprint(A.trace())\n\nprint(\"\\nDeterminant:\")\npprint(A.det())\n\nprint(\"\\nMatrix inverse:\")\npprint(A.inv())\n\n# Symbolic example\nS = Matrix([[x, y],\n            [y, x]])\nprint(\"\\n5th power of S:\")\nS**5\n\nOriginal matrix:\n⎡1  2⎤\n⎢    ⎥\n⎣3  4⎦\n\nTranspose:\n⎡1  3⎤\n⎢    ⎥\n⎣2  4⎦\n\nMatrix trace:\n5\n\nDeterminant:\n-2\n\nMatrix inverse:\n⎡-2    1  ⎤\n⎢         ⎥\n⎣3/2  -1/2⎦\n\n5th power of S:\n\n\n\\(\\displaystyle \\left[\\begin{matrix}x^{5} + 10 x^{3} y^{2} + 5 x y^{4} & 5 x^{4} y + 10 x^{2} y^{3} + y^{5}\\\\5 x^{4} y + 10 x^{2} y^{3} + y^{5} & x^{5} + 10 x^{3} y^{2} + 5 x y^{4}\\end{matrix}\\right]\\)\n\n\n\n\n3.10.7 Important Notes\n\nSymPy matrices use * for matrix multiplication (unlike NumPy’s @)\nIndexing is zero-based, similar to NumPy\nSymPy matrices are immutable - operations return new matrices\nRow and column extraction methods return Matrix objects\nSymPy can handle:\n\nSymbolic computations\nExact fractions\nAlgebraic expressions",
    "crumbs": [
      "Matrices",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html",
    "href": "worksheets/matrix_multiplication.html",
    "title": "Problems",
    "section": "",
    "text": "Problem 1\nFor this problem let\n\\[A = \\begin{bmatrix}\n3 & 1 & -2 \\\\\n5 & -4 & 3\n\\end{bmatrix},\nB = \\begin{bmatrix}\n8 & 2\\\\\n-6 & -3\\\\\n2 & -4\n\\end{bmatrix}\\]\n\\[C = \\begin{bmatrix}\n2 & 3 \\\\\n-3 & 2\n\\end{bmatrix},\nD = \\begin{bmatrix}\n4 & 6 \\\\\n-2 & 5\n\\end{bmatrix},\nE = \\begin{bmatrix}\n-6 \\\\\n4\n\\end{bmatrix}\\]\nCompute the following. If the operation is not possible explain why\nFor the following problems we have:\n\\(A = \\begin{bmatrix}\\uparrow&\\uparrow&\\uparrow\\\\\n\\mathbf{c}_1&\\mathbf{c}_2 &\\mathbf{c}_3\\\\\n\\downarrow&\\downarrow&\\downarrow\\end{bmatrix} =\n\\begin{bmatrix} \\leftarrow \\mathbf{r}_1 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_2 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_3 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_4 \\rightarrow \\end{bmatrix}\\)\n\\(E_1 = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\),   \\(E_2 = \\begin{bmatrix} 1 & 0 & -2 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\),\n\\(E_3 = \\begin{bmatrix} -1 & 0 & 0 & 0 \\\\ 1 & -1 & 0 & 0 \\\\ 0 & 1 & -1 & 0 \\\\ 0 & 0 & 1 & -1 \\end{bmatrix}\\),   \\(E_4 = \\begin{bmatrix} 1 & 0 & 1 & -10 \\\\ 2 & 2 & 1 & 0 \\\\ 0 & -2 & -1 & 0 \\\\ 4 & 0 & 0 & -1 \\end{bmatrix}\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-1",
    "href": "worksheets/matrix_multiplication.html#problem-1",
    "title": "Problems",
    "section": "",
    "text": "\\(-3A+B\\)\n\\(B - 3A^T\\)\n\\(I_3-AB\\)\n\\(AB-2I_2\\)\n\\(AC\\)\n\\(DA\\)\n\\(E^TCE\\)\n\\(BD+A^T\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-2",
    "href": "worksheets/matrix_multiplication.html#problem-2",
    "title": "Problems",
    "section": "Problem 2",
    "text": "Problem 2\nFor each of the following problems, indicate if the matrices can be multiplied. If they can, express the answer in terms of the rows of \\(A\\) or the columns of \\(A\\). If they cannot be multiplied, explain why.\n\n\\(AE_1\\)\n\\(E_1A\\)\n\\(AE_2\\)\n\\(E_2A\\)\n\\(AE_3\\)\n\\(E_3A\\)\n\\(AE_4\\)\n\\(E_4A\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-3",
    "href": "worksheets/matrix_multiplication.html#problem-3",
    "title": "Problems",
    "section": "Problem 3",
    "text": "Problem 3\nSuppose that \\(AB_1 = \\begin{bmatrix} \\uparrow & \\uparrow & \\uparrow & \\uparrow \\\\ (\\mathbf{c}_1-3\\mathbf{c}_2) & \\mathbf{c}_3 & \\mathbf{c}_2 & (8\\mathbf{c}_2-\\mathbf{c}_3) \\\\ \\downarrow & \\downarrow & \\downarrow & \\downarrow \\end{bmatrix}\\). Find \\(B_1\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-4",
    "href": "worksheets/matrix_multiplication.html#problem-4",
    "title": "Problems",
    "section": "Problem 4",
    "text": "Problem 4\nSuppose that \\(B_2A = \\begin{bmatrix} \\leftarrow \\mathbf{r}_1 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_2 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_3 \\rightarrow \\\\ \\leftarrow \\mathbf{r}_4 \\rightarrow \\end{bmatrix}\\). Find \\(B_2\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-5",
    "href": "worksheets/matrix_multiplication.html#problem-5",
    "title": "Problems",
    "section": "Problem 5",
    "text": "Problem 5\nSuppose that \\(B_3A = \\begin{bmatrix} \\leftarrow (\\mathbf{r}_1+\\mathbf{r}_2-\\mathbf{r}_3) \\rightarrow \\\\ \\leftarrow (\\mathbf{r}_2-\\mathbf{r}_3+\\mathbf{r}_4) \\rightarrow \\end{bmatrix}\\). Find \\(B_2\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-6",
    "href": "worksheets/matrix_multiplication.html#problem-6",
    "title": "Problems",
    "section": "Problem 6",
    "text": "Problem 6\nSuppose that \\(B_4E_1 = A\\). Find \\(B_4\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "worksheets/matrix_multiplication.html#problem-7",
    "href": "worksheets/matrix_multiplication.html#problem-7",
    "title": "Problems",
    "section": "Problem 7",
    "text": "Problem 7\nSuppose that \\(B_5\\begin{bmatrix} \\leftarrow (\\mathbf{r}_1+\\mathbf{r}_2+\\mathbf{r}_3+\\mathbf{r}_4) \\rightarrow \\\\ \\leftarrow (\\mathbf{r}_2+\\mathbf{r}_3+\\mathbf{r}_4) \\rightarrow \\\\ \\leftarrow (\\mathbf{r}_3+\\mathbf{r}_4) \\rightarrow \\\\ \\leftarrow \\mathbf{r}_4 \\rightarrow \\end{bmatrix} = A\\). Find \\(B_5\\)",
    "crumbs": [
      "Matrices",
      "Problems"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html",
    "href": "applications/image_transforms_matrix.html",
    "title": "Real World Application",
    "section": "",
    "text": "Image Transformation with Matrices\nThis section explores how images are represented as matrices and demonstrates various transformations using Python. We’ll cover both grayscale and color images, showing how matrix operations can be used to create different visual effects.",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#image-transformation-with-matrices",
    "href": "applications/image_transforms_matrix.html#image-transformation-with-matrices",
    "title": "Real World Application",
    "section": "",
    "text": "Required Libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#black-and-white-images",
    "href": "applications/image_transforms_matrix.html#black-and-white-images",
    "title": "Real World Application",
    "section": "1. Black and White Images",
    "text": "1. Black and White Images\n\nUnderstanding Grayscale Representation\nGrayscale images are typically represented as 2D arrays (matrices) where each element represents the intensity of a pixel. When working with uint8 (integers) data type, the values range from 0 (black) to 255 (white). When working with float data type, the values should be normalized to the range 0 to 1.\n\n# Create a simple 3x4 grayscale image\ngrayscale_example = np.array([\n    [0, 85, 170, 255],    # Different shades of gray\n    [255, 170, 85, 0],    # Reversed pattern\n    [128, 128, 128, 128]  # Medium gray\n])\n\nplt.figure(figsize=(6, 4))\nplt.imshow(grayscale_example, cmap='gray')\nplt.colorbar()\nplt.title('3x4 Grayscale Example')\nplt.show()",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#color-images-rgb",
    "href": "applications/image_transforms_matrix.html#color-images-rgb",
    "title": "Real World Application",
    "section": "2. Color Images (RGB)",
    "text": "2. Color Images (RGB)\nColors can be represented in various digital formats - from HSV (Hue, Saturation, Value) to CMYK (Cyan, Magenta, Yellow, Key/Black). Here,we’ll work with the RGB (Red, Green, Blue) color model, where each pixel’s color is created by combining different intensities of these three primary colors.\n\nRGB Color Model\nColor images use three channels: Red, Green, and Blue. Each pixel is represented by three values, creating a 3D array with shape (height, width, 3). As with grayscale images, the values can be either in the range 0-255 (uint8) or 0-1 (float).\n\nCommon Colors:\n\nBlack: (0, 0, 0)\nWhite: (255, 255, 255)\nPure Red: (255, 0, 0)\nPure Green: (0, 255, 0)\nPure Blue: (0, 0, 255)\nYellow: (255, 255, 0) [Red + Green]\nMagenta: (255, 0, 255) [Red + Blue]\nCyan: (0, 255, 255) [Green + Blue]\n\n\n\n\nSimple RGB image with common colors\n\nsimple_rgb = np.array([\n    [[255,0,0], [0,255,0], [0,0,255], [255,255,255]],   # top row\n    [[255,255,0], [0,255,255],[0,0,0], [100,100,100]]   # bottom row\n])\n\nplt.figure(figsize=(4, 4))\nplt.imshow(simple_rgb)\nplt.title('2x4 RGB Image')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNavigating Color Images in NumPy\nWhen working with color images in NumPy, image[a,b,c] lets us access specific pixel values. The first two numbers (a,b) are the pixel coordinates - a selects the row (moving down), b selects the column (moving right). The last number c picks the color channel: 0 for red, 1 for green, or 2 for blue. So image[1,2,1] gets the green value at row 2, column 3.\nTo access entire color channels, you can use : to select all rows and columns. For example, image[:,:,0] gives you the complete red channel, image[:,:,1] the green channel, and image[:,:,2] the blue channel. Each channel is a 2D array of intensities from 0 to 255, which we visualize in grayscale in the folling function - bright pixels show where that color is strong, dark pixels where it’s absent.\n\ndef display_rgb_channels(image):\n    \"\"\"Display an image and its RGB channels separately\"\"\"\n    \n    # Create a figure with 2x2 subplots\n    fig, axes = plt.subplots(2, 2, figsize=(8,8))\n    \n    # Original image\n    axes[0,0].imshow(image)\n    axes[0,0].set_title('Original')\n    \n    # Red channel showed as gray\n    axes[0,1].imshow(image[:,:,0], cmap=\"gray\")\n    axes[0,1].set_title('Red Channel')\n    \n    # Green channel showed as gray\n    axes[1,0].imshow(image[:,:,1], cmap=\"gray\")\n    axes[1,0].set_title('Green Channel')\n    \n    # Blue channel showed as gray\n    axes[1,1].imshow(image[:,:,2], cmap=\"gray\")\n    axes[1,1].set_title('Blue Channel')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Load an image\ncircles = plt.imread('rgb_colors.png')\ndisplay_rgb_channels(circles)\nbutterfly = plt.imread('butterfly.jpg')\ndisplay_rgb_channels(butterfly)",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#converting-between-images-and-matrices",
    "href": "applications/image_transforms_matrix.html#converting-between-images-and-matrices",
    "title": "Real World Application",
    "section": "3. Converting Between Images and Matrices",
    "text": "3. Converting Between Images and Matrices\n\nUnderstanding Image Arrays and Reshaping\nA grayscale image is stored as a 2D array with shape (height, width), while a color image uses a 3D array with shape (height, width, 3). For example, a 100x100 color image has shape (100, 100, 3), where the third dimension holds RGB values.\nMatrix operations require 2D arrays, so we need to reorganize our 3D color images. We transform from height × width × 3 to a matrix of (height × width) rows by 3 columns, flattening the spatial dimensions while keeping color information.\nThe reshape(-1,3) method transforms our 3D color image into a 2D matrix. The -1 tells NumPy to automatically calculate the number of rows needed, while 3 specifies we want 3 columns. For example, an image of shape (100,100,3) becomes a matrix of shape (10000,3), where each row represents one pixel’s values. The columns have a specific meaning: the first column contains all red values, the second green, and the third blue.\n\n\nExample\nLet’s illustrate this is a simple example:\n\nimg_2by2 = np.array([\n    [[1,2,3],[4,5,6]],      # top row\n    [[7,8,9],[10,11,12]]    # bottom row\n])\nprint(\"The original image:\")\nprint(img_2by2)\nprint(\"Shape:\", img_2by2.shape)\n\nprint(\"\\n\" + \"-\"*40 + \"\\n\")  # adds a dividing line\n\nM_2by2 = img_2by2.reshape(-1,3)\nprint(\"The reshaped matrix:\")\nprint(M_2by2)\nprint(\"Shape:\", M_2by2.shape)\n\nThe original image:\n[[[ 1  2  3]\n  [ 4  5  6]]\n\n [[ 7  8  9]\n  [10 11 12]]]\nShape: (2, 2, 3)\n\n----------------------------------------\n\nThe reshaped matrix:\n[[ 1  2  3]\n [ 4  5  6]\n [ 7  8  9]\n [10 11 12]]\nShape: (4, 3)\n\n\nNotice that the columns represent red, green, and blue values respectively. This process is reversible. If we write M_2by2.reshape(img_2by2.shape) we get img_2by2 back - no need to remember the original dimensions since they’re stored in the shape attribute. However, if you want, you can also write M_2by2.reshape(2,2).\n\nM_2by2.reshape(img_2by2.shape)\n\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]])\n\n\n\n\nHelper Functions\nWe’ll use these helper functions to convert between image and matrix formats throughout our examples:\n\ndef image_to_matrix(image):\n    \"\"\"Convert image to matrix format (n_pixels × 3)\"\"\"\n    return image.reshape(-1, 3)\n\ndef matrix_to_image(matrix, original_shape):\n    \"\"\"Convert matrix back to image format\"\"\"\n    return matrix.reshape(original_shape)",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#color-transformations-using-permutation-matrices",
    "href": "applications/image_transforms_matrix.html#color-transformations-using-permutation-matrices",
    "title": "Real World Application",
    "section": "4. Color Transformations Using Permutation Matrices",
    "text": "4. Color Transformations Using Permutation Matrices\n\nSwapping Color Channels\nWe can use permutation matrices to swap color channels:\n\ndef swap_colors(image, permutation_matrix):\n    \"\"\"Apply color permutation to image\"\"\"\n    matrix = image_to_matrix(image)\n    transformed = matrix @ permutation_matrix\n    return matrix_to_image(transformed, image.shape)\n\n# Example permutation matrices\nRGB_to_BGR = np.array([\n    [0, 0, 1],\n    [0, 1, 0],\n    [1, 0, 0]\n])\n\n\n\n\n\n\n\nExercise: Find all six 3×3 permutation matrices.\n\n\n\n\n\n\nIdentity:\n\\(\\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&1\\end{bmatrix}\\)\nSingle swap (R-G):\n\\(\\begin{bmatrix}0&1&0\\\\1&0&0\\\\0&0&1\\end{bmatrix}\\)\nSingle swap (R-B):\n\\(\\begin{bmatrix}0&0&1\\\\0&1&0\\\\1&0&0\\end{bmatrix}\\)\nSingle swap (G-B):\n\\(\\begin{bmatrix}1&0&0\\\\0&0&1\\\\0&1&0\\end{bmatrix}\\)\nCyclic (R→G→B→R):\n\\(\\begin{bmatrix}0&0&1\\\\1&0&0\\\\0&1&0\\end{bmatrix}\\)\nCyclic Cyclic (R→B→G→R):\n\\(\\begin{bmatrix}0&1&0\\\\0&0&1\\\\1&0&0\\end{bmatrix}\\)",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#grayscale-conversion-and-negative",
    "href": "applications/image_transforms_matrix.html#grayscale-conversion-and-negative",
    "title": "Real World Application",
    "section": "5. Grayscale Conversion and Negative",
    "text": "5. Grayscale Conversion and Negative\nWhen converting a color image to grayscale, we don’t simply average the RGB values. Our eyes have different sensitivities to different colors, with green light being perceived as brightest and blue as darkest. To create natural-looking grayscale images, we use weighted averages that match human perception: 29.9% for red, 58.7% for green, and 11.4% for blue.\nThe negative of an image can be obtained by subtracting each pixel value from the maximum possible value (255 for 8-bit images). Thanks to NumPy’s broadcasting capabilities, we can simply write 255 - image and this operation will be applied to every pixel value automatically, whether it’s a grayscale or color image. Here’s a simple function to create image negatives:\n\ndef to_grayscale(image):\n    \"\"\"Convert RGB image to grayscale using weighted sum\"\"\"\n    weights = np.array([0.299, 0.587, 0.114])\n    matrix = image_to_matrix(image)\n    grayscale_values = matrix @ weights\n    return grayscale_values.reshape(image.shape[:2])\n\ndef create_negative(image):\n    \"\"\"Create negative of an image\"\"\"\n    return 255 - image\n\n# Upload black and white image of a dog\ndog = np.array(Image.open('grayscale.png').convert('L'))\n# Show image and negative\nfig, axes = plt.subplots(2, 1, figsize=(8, 10))\naxes[0].set_title('Original Grayscale Image')\naxes[0].imshow(dog, cmap='gray')\naxes[1].set_title('Negative Image')\naxes[1].imshow(255-dog,cmap = 'gray')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#color-filters",
    "href": "applications/image_transforms_matrix.html#color-filters",
    "title": "Real World Application",
    "section": "6. Color Filters",
    "text": "6. Color Filters\n\nImplementing a Sepia Filter\nA sepia filter transforms a regular color image into one with a warm, brownish tone reminiscent of vintage photographs. To create this effect, we need to adjust each color channel using specific weights. For each pixel, the new RGB values are calculated as a combination of the original values:\nThe red channel is amplified with warm tones The green channel is moderately reduced The blue channel is significantly reduced\nThis creates the characteristic reddish-brown tint that gives sepia images their antique appearance.\n\ndef apply_sepia(image):\n    \"\"\"Apply sepia filter to image\"\"\"\n    sepia_matrix = np.array([\n        [0.393, 0.349, 0.272],\n        [0.769, 0.686, 0.534],\n        [0.189, 0.168, 0.131]\n    ])\n    \n    matrix = image_to_matrix(image)\n    sepia = matrix @ sepia_matrix\n    \n    # Clip values to valid range\n    sepia = np.clip(sepia, 0, 1)\n    return matrix_to_image(sepia, image.shape)\n\n\n\nImplementing a Color Intensification Filter\nA color intensification filter makes images more vibrant by amplifying the primary colors while reducing color bleeding between channels. To create this effect, each color channel is multiplied by 1.5 (intensifying its own color) while subtracting a quarter of the other colors’ intensities. This process:\n\nBoosts each channel’s own color\nReduces the influence of other colors\nIncreases contrast between different colored areas\n\nThis creates a more vivid appearance with enhanced color separation and impact.\n\ndef intensify_colors(image):\n    \"\"\"Apply color intensification filter to image\"\"\"\n    intensity_matrix = np.array([\n        [1.5, -0.25, -0.25],\n        [-0.25, 1.5, -0.25],\n        [-0.25, -0.25, 1.5]\n    ])\n    \n    matrix = image_to_matrix(image)\n    intensified = matrix @ intensity_matrix\n    \n    # Clip values to valid range\n    intensified = np.clip(intensified, 0, 1)\n    return matrix_to_image(intensified, image.shape)",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#example-usage",
    "href": "applications/image_transforms_matrix.html#example-usage",
    "title": "Real World Application",
    "section": "Example Usage",
    "text": "Example Usage\nHere’s how to use these transformations on an actual image:\n\n# Load an image: Open it in Image, and convert it to a numpy array as a float\nimage = np.array(Image.open('city_river.jpg')).astype(float)/255\n\n# Display original and transformed versions\nfig, axes = plt.subplots(5, 1, figsize=(10,25))\n\naxes[0].imshow(image)\naxes[0].set_title('Original')\n\naxes[1].imshow(swap_colors(image, RGB_to_BGR))\naxes[1].set_title('RGB to BGR')\n\naxes[2].imshow(to_grayscale(image), cmap='gray')\naxes[2].set_title('Grayscale')\n\naxes[3].imshow(apply_sepia(image))\naxes[3].set_title('Sepia')\n\naxes[4].imshow(intensify_colors(image))\naxes[4].set_title('Intensification')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "applications/image_transforms_matrix.html#conclusion",
    "href": "applications/image_transforms_matrix.html#conclusion",
    "title": "Real World Application",
    "section": "Conclusion",
    "text": "Conclusion\nThis section demonstrates how matrices naturally represent and transform digital images. When an RGB image is reshaped, it becomes a matrix with three columns representing the color channels: \\[\n\\begin{bmatrix}\n\\color{red}\\uparrow & \\color{green}\\uparrow & \\color{blue}\\uparrow\\\\\n\\color{red}\\mathbf{c}_1 & \\color{green}\\mathbf{c}_2 & \\color{blue}\\mathbf{c}_3\\\\\n\\color{red}\\downarrow & \\color{green}\\downarrow & \\color{blue}\\downarrow\n\\end{bmatrix}.\n\\]\nThe power of matrix multiplication (Equation 3.1) becomes evident in image processing. When we multiply this matrix by another matrix on the right, we take linear combinations of these color channels, enabling various transformations:\n\nColor channel permutations (by rearranging columns)\nGrayscale conversion (by weighted averaging of channels)\nSepia filter effects (through specific linear combinations)\n\nThis direct connection between abstract matrix operations and visual transformations provides a concrete example of linear combinations in practice. Understanding how matrices act on these color channels helps explain why matrix multiplication works the way it does and illustrates its practical applications.",
    "crumbs": [
      "Matrices",
      "Real World Application"
    ]
  },
  {
    "objectID": "parts/linear_maps.html",
    "href": "parts/linear_maps.html",
    "title": "Linear Transformation",
    "section": "",
    "text": "This section covers Linear Transformations.\n\nLinear Transformations\nProblems",
    "crumbs": [
      "Linear Transformation"
    ]
  },
  {
    "objectID": "chapters/linear_maps.html",
    "href": "chapters/linear_maps.html",
    "title": "4  Linear Transformations in \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "4.1 Motivating Example\nIf \\(A\\) is an \\(m\\times n\\) matrix, Equation 3.1 tells us that \\(A\\) induces a map \\(A:\\mathbb{R}^n\\to\\mathbb{R}^m\\) defined by the Matrix-Vector formula \\(A\\mathbf{x}\\). This map has two important properties:\nLet’s check 1: Let \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) and \\(\\mathbf{y}=(y_1,\\dots,y_n)\\) be two arbtrary elements of \\(\\mathbb{R}^n\\). Then writing the product in terms of the columns of \\(A\\) and using standard operations of vectors we get: \\[\\begin{aligned}\nA(\\mathbf{x}+\\mathbf{y}) &=\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\\left(\n\\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_n \\end{bmatrix}  +    \n\\begin{bmatrix}y_1\\\\y_2\\\\\\vdots\\\\y_n \\end{bmatrix}     \n\\right) \\\\\n&=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\n\\begin{bmatrix}x_1+y_1\\\\x_2+y_2\\\\\\vdots\\\\x_n+y_n \\end{bmatrix} \\\\\n&= (x_1+y_1)\\mathbf{c}_1+\\cdots +(x_n+y_n)\\mathbf{c}_n\\\\\n&=(x_1\\mathbf{c}_1+\\cdots +x_n\\mathbf{c}_n)+(y_1\\mathbf{c}_1+\\cdots + y_n\\mathbf{c}_n)\\\\\n&=A\\mathbf{x}+A\\mathbf{y}.\n\\end{aligned}\n\\]\nExercise: Check property 2. That is, for every \\(\\mathbf{x}\\in\\mathbb{R}^n\\) and every \\(c\\in\\mathbb{R}\\), \\(A(c\\mathbf{x})=cA\\mathbf{x}\\).",
    "crumbs": [
      "Linear Transformation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Transformations in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/linear_maps.html#motivating-example",
    "href": "chapters/linear_maps.html#motivating-example",
    "title": "4  Linear Transformations in \\(\\mathbb{R}^n\\)",
    "section": "",
    "text": "For every \\(\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^n, A(\\mathbf{x}+\\mathbf{y})=A\\mathbf{x}+A\\mathbf{y}\\), and\nFor every \\(\\mathbf{x}\\in\\mathbb{R}^n\\) and every \\(c\\in\\mathbb{R}\\), \\(A(c\\mathbf{x})=cA\\mathbf{x}\\)\n\n\n\n\n\n\n\n\n\nClick to see the proof\n\n\n\n\n\nLet \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) be an arbitrary element in \\(\\mathbb{R}^n\\) and let \\(c\\in\\mathbb{R}\\) be a scalar. Writing the product in terms of the columns of \\(A\\) and using standard operations on vectors we get: \\[\n\\begin{aligned}\nA(c\\mathbf{x}) &=\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\n\\left(\nc\\begin{bmatrix}x_1\\\\ x_2\\\\\\vdots\\\\ x_n \\end{bmatrix}  \n\\right) \\\\\n&= \\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\n\\mathbf{c}_1 & \\mathbf{c}_2 & \\cdots & \\mathbf{c}_n \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\\left(\n\\begin{bmatrix}cx_1\\\\ cx_2\\\\ \\vdots\\\\ cx_n \\end{bmatrix}  \n\\right) \\\\\n&= (cx_1)\\mathbf{c}_1+\\cdots +(cx_n)\\mathbf{c}_n\\\\\n&= c(x_1\\mathbf{c}_1+\\cdots +x_n\\mathbf{c}_n)\\\\\n&= c(A\\mathbf{x})\n\\end{aligned}\n\\]",
    "crumbs": [
      "Linear Transformation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Transformations in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/linear_maps.html#linear-transformations",
    "href": "chapters/linear_maps.html#linear-transformations",
    "title": "4  Linear Transformations in \\(\\mathbb{R}^n\\)",
    "section": "4.2 Linear Transformations",
    "text": "4.2 Linear Transformations\n\nDefinition: A function \\(T:\\mathbb{R}^n\\to\\mathbb{R}^m\\) is a linear transformation if it satisfies two properties:\n\nAdditivity: For every \\(\\mathbf{x}_1,\\mathbf{x}_2\\in\\mathbb{R}^n, T(\\mathbf{x}_1+\\mathbf{x}_2)=T(\\mathbf{x}_1)+T(\\mathbf{x}_2)\\), and\nScalar Multiplication: For every \\(\\mathbf{x}\\in\\mathbb{R}^n\\) and every \\(c\\in\\mathbb{R}\\), \\(T(c\\mathbf{x})=cT(\\mathbf{x})\\)\n\n\nThese properties naturally extend to any finite collection of vectors. For vectors \\(\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_k\\in\\mathbb{R}^n\\) and scalars \\(c_1,c_2,\\ldots,c_k\\in\\mathbb{R}\\), we have \\[T(c_1\\mathbf{x}_1+\\cdots+c_k\\mathbf{x}_k)=c_1T(\\mathbf{x}_1)+\\cdots+c_kT(\\mathbf{x}_k) \\tag{4.1}\\] This is an important formula that we will use many times.\nNotice that we just establihed that an \\(m\\times n\\) matrix \\(A\\) induces a linear transformation \\(A:\\mathbb{R}^n\\to\\mathbb{R}^m\\) defined by \\(A\\mathbf{x}\\). In this section we will demonstrate the converse: that any linear transformation from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\) can be expressed as matrix multiplication. The following simple example will illustrate this fundamental property.\n\nExample: Let \\(T:\\mathbb{R}^2\\to\\mathbb{R}^3\\) be defined by \\(T\\left( \\begin{bmatrix}a\\\\ b\\end{bmatrix} \\right)=\\begin{bmatrix}a+b\\\\ b-2a\\\\ a\\end{bmatrix}\\)\n\nThe first step is to show that the function is a linear transformation. Try to do it using the definition, but feel free to click to see the detailed proof.\n\nExercise: Prove that \\(T:\\mathbb{R}^2\\to\\mathbb{R}^3\\) is a linear transformation.\n\n\n\n\n\n\nClick to see the proof\n\n\n\n\n\n\nProof. To prove that \\(T\\) is a linear transformation, we must verify both properties from the definition:\n\nAdditivity: \\(T(\\mathbf{x}+\\mathbf{y})=T(\\mathbf{x})+T(\\mathbf{y})\\) for all \\(\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^2\\)\nScalar multiplication: \\(T(c\\mathbf{x})=cT(\\mathbf{x})\\) for all \\(\\mathbf{x}\\in\\mathbb{R}^2\\) and \\(c\\in\\mathbb{R}\\)\n\nProperty 1 (Additivity): Let \\(\\mathbf{x}=\\begin{bmatrix}x_1\\\\ x_2\\end{bmatrix}\\) and \\(\\mathbf{y}=\\begin{bmatrix}y_1\\\\ y_2\\end{bmatrix}\\) be arbitrary vectors in \\(\\mathbb{R}^2\\).\nFirst, let’s compute \\(T(\\mathbf{x}+\\mathbf{y})\\): \\[\\begin{align*}\nT(\\mathbf{x}+\\mathbf{y}) &= T\\left(\\begin{bmatrix}x_1+y_1\\\\ x_2+y_2\\end{bmatrix}\\right)\\\\\n&= \\begin{bmatrix}(x_1+y_1)+(x_2+y_2)\\\\ (x_2+y_2)-2(x_1+y_1)\\\\ x_1+y_1\\end{bmatrix}\\\\\n&= \\begin{bmatrix}x_1+x_2+y_1+y_2\\\\ x_2+y_2-2x_1-2y_1\\\\ x_1+y_1\\end{bmatrix}\n\\end{align*}\\]\nNow, let’s compute \\(T(\\mathbf{x})+T(\\mathbf{y})\\): \\[\\begin{align*}\nT(\\mathbf{x})+T(\\mathbf{y}) &= \\begin{bmatrix}x_1+x_2\\\\ x_2-2x_1\\\\ x_1\\end{bmatrix} + \\begin{bmatrix}y_1+y_2\\\\ y_2-2y_1\\\\ y_1\\end{bmatrix}\\\\\n&= \\begin{bmatrix}x_1+x_2+y_1+y_2\\\\ x_2-2x_1+y_2-2y_1\\\\ x_1+y_1\\end{bmatrix}\n\\end{align*}\\]\nSince these are equal for any choice of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), Property 1 is verified.\nProperty 2 (Scalar Multiplication): Let \\(\\mathbf{x}=\\begin{bmatrix}x_1\\\\ x_2\\end{bmatrix}\\) be an arbitrary vector in \\(\\mathbb{R}^2\\) and \\(c\\in\\mathbb{R}\\) be an arbitrary scalar.\nFirst, let’s compute \\(T(c\\mathbf{x})\\): \\[\\begin{align*}\nT(c\\mathbf{x}) &= T\\left(\\begin{bmatrix}cx_1\\\\ cx_2\\end{bmatrix}\\right)\\\\\n&= \\begin{bmatrix}cx_1+cx_2\\\\ cx_2-2(cx_1)\\\\ cx_1\\end{bmatrix}\\\\\n&= \\begin{bmatrix}c(x_1+x_2)\\\\ c(x_2-2x_1)\\\\ cx_1\\end{bmatrix}\n\\end{align*}\\]\nNow, let’s compute \\(cT(\\mathbf{x})\\): \\[\\begin{align*}\ncT(\\mathbf{x}) &= c\\begin{bmatrix}x_1+x_2\\\\ x_2-2x_1\\\\ x_1\\end{bmatrix}\\\\\n&= \\begin{bmatrix}c(x_1+x_2)\\\\ c(x_2-2x_1)\\\\ cx_1\\end{bmatrix}\n\\end{align*}\\]\nSince these are equal for any choice of \\(\\mathbf{x}\\) and \\(c\\), Property 2 is verified.\nTherefore, since both properties hold, \\(T\\) is indeed a linear transformation.\n\n\n\n\n\nNow that we know that \\(T:\\mathbb{R}^2\\to\\mathbb{R}^3\\) is linear we use elementary vector operations, Equation 4.1, and Equation 3.1 to obtain: \\[\\begin{aligned}\nT\\left( \\begin{bmatrix}a\\\\ b\\end{bmatrix} \\right)\n&=T\\left( a\\begin{bmatrix}1\\\\ 0\\end{bmatrix} + b\\begin{bmatrix}0\\\\ 1\\end{bmatrix}\\right) \\\\\n&=aT\\left( \\begin{bmatrix}1\\\\ 0\\end{bmatrix}\\right) + bT\\left(\\begin{bmatrix}0\\\\ 1\\end{bmatrix}\\right) \\\\\n&=a\\begin{bmatrix}1\\\\ -1\\\\ 1\\end{bmatrix} + b\\begin{bmatrix}1\\\\ 1\\\\0\\end{bmatrix}\n= \\begin{bmatrix}1 & 1\\\\ -1 &1\\\\ 1&0\\end{bmatrix}\\begin{bmatrix}a\\\\ b\\end{bmatrix}.\n\\end{aligned}\\] This means that for every \\(\\mathbf{x}\\in\\mathbb{R}^2\\), \\(T(\\mathbf{x})=A\\mathbf{x}\\) for \\(A= \\begin{bmatrix}1 & 1\\\\ -1 &1\\\\ 1&0\\end{bmatrix}\\)\n\nTheorem: Let \\(T:\\mathbb{R}^n\\to\\mathbb{R}^m\\) be a linear transformation. Then there exists an \\(m\\times n\\) matrix \\(A\\) such that for every \\(\\mathbf{x}\\in\\mathbb{R}^n\\), \\(T(\\mathbf{x})=A\\mathbf{x}\\).\n\n\nProof. The proof consists of three main steps:\n\nFirst, let’s identify the canonical basis vectors of \\(\\mathbb{R}^n\\). Let \\(\\mathbf{e}_i\\) denote the \\(i\\)-th canonical basis vector: \\[\\mathbf{e}_1=\\begin{bmatrix}1\\\\0\\\\ \\vdots\\\\ 0\\end{bmatrix},\\quad \\mathbf{e}_2=\\begin{bmatrix}0\\\\1\\\\ \\vdots\\\\ 0\\end{bmatrix},\\quad \\ldots, \\quad \\mathbf{e}_n=\\begin{bmatrix}0\\\\0\\\\ \\vdots\\\\ 1\\end{bmatrix}\\]\nAny vector \\(\\mathbf{x}\\in\\mathbb{R}^n\\) can be written uniquely as a linear combination of these basis vectors: \\[\\mathbf{x}=\\begin{bmatrix}x_1\\\\ x_2\\\\ \\vdots\\\\ x_n\\end{bmatrix}=x_1\\mathbf{e}_1 + x_2\\mathbf{e}_2 + \\cdots + x_n\\mathbf{e}_n\\]\nNow, using the linearity of \\(T\\) (see Equation 4.1), we have: \\[\\begin{aligned}\nT(\\mathbf{x})&=T(x_1\\mathbf{e}_1 + x_2\\mathbf{e}_2 + \\cdots + x_n\\mathbf{e}_n)\\\\\n&=x_1T(\\mathbf{e}_1) + x_2T(\\mathbf{e}_2) + \\cdots + x_nT(\\mathbf{e}_n)\n\\end{aligned}\n\\]\ndefine the matrix \\(A\\) by using the transformed basis vectors as its columns: \\[A=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\nT(\\mathbf{e}_1) & T(\\mathbf{e}_2) & \\cdots & T(\\mathbf{e}_n) \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow \\\\\n\\end{bmatrix}\\]\nThen by the definition of matrix multiplication (see Equation 3.1): \\[\\begin{aligned}\nA\\mathbf{x}&=\n\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\nT(\\mathbf{e}_1) & T(\\mathbf{e}_2) & \\cdots & T(\\mathbf{e}_n) \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow \\\\\n\\end{bmatrix}\n\\begin{bmatrix}x_1\\\\ x_2\\\\ \\vdots\\\\ x_n\\end{bmatrix} \\\\\n&=x_1T(\\mathbf{e}_1) + x_2T(\\mathbf{e}_2) + \\cdots + x_nT(\\mathbf{e}_n)=T(\\mathbf{x})\n\\end{aligned}.\n\\]\n\nTherefore, we have constructed a matrix \\(A\\) such that \\(T(\\mathbf{x})=A\\mathbf{x}\\) for all \\(\\mathbf{x}\\in\\mathbb{R}^n\\). Note that \\(A\\) is \\(m\\times n\\) since it has \\(n\\) columns, and each column \\(T(\\mathbf{e}_i)\\in\\mathbb{R}^m\\).\n\n\n\n\n\n\n\n\nKey Algorithm\n\n\n\nThe definition of the matrix \\(A\\) provides an algorithm for finding the matrix representation of any linear transformation \\(T:\\mathbb{R}^n\\to\\mathbb{R}^m\\). Simply compute \\(T(\\mathbf{e}_i)\\) for each canonical basis vector and use these vectors as the columns of \\(A\\): \\[A=\\begin{bmatrix}\n\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\\nT(\\mathbf{e}_1) & T(\\mathbf{e}_2) & \\cdots & T(\\mathbf{e}_n) \\\\\n\\downarrow & \\downarrow & \\cdots & \\downarrow\n\\end{bmatrix}\\] In other words, the \\(j\\)-th column of \\(A\\) is the output of the transformation \\(T\\) when applied to the \\(j\\)-th canonical basis vector \\(\\mathbf{e}_j\\).",
    "crumbs": [
      "Linear Transformation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Transformations in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "chapters/linear_maps.html#composition-of-linear-transformations",
    "href": "chapters/linear_maps.html#composition-of-linear-transformations",
    "title": "4  Linear Transformations in \\(\\mathbb{R}^n\\)",
    "section": "4.3 Composition of Linear Transformations",
    "text": "4.3 Composition of Linear Transformations\nblah blah",
    "crumbs": [
      "Linear Transformation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Linear Transformations in $\\mathbb{R}^n$</span>"
    ]
  },
  {
    "objectID": "parts/system_equations.html",
    "href": "parts/system_equations.html",
    "title": "System of Linear Equations",
    "section": "",
    "text": "This section covers system of linear equations.\n\nSystem of Linear Equations",
    "crumbs": [
      "System of Linear Equations"
    ]
  },
  {
    "objectID": "chapters/system_equations.html",
    "href": "chapters/system_equations.html",
    "title": "5  System of Linear Equations",
    "section": "",
    "text": "To be completed …",
    "crumbs": [
      "System of Linear Equations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>System of Linear Equations</span>"
    ]
  }
]